 {
\abnormalparskip{0pt}
\chapter{Implementation}
\label{cha:implementation}
}

This chapter touches on parts of the implementation which are done differently
from Smiths implementation.

The new implementation is written in Googles, still relatively new, language
Golang~\cite{GolangHomepage}

\section{Overview}
\label{sec:overview-1}

Overview of the source code. What has been done where. About the language etc.

\section{Building topologies}
\label{sec:building-topologies}

Naive approach when building and pruning significantly increases run
time. Explain Smiths approach with a stack sorted on the best seen/most
promising results.

\section{Analytical solution to the Fermat-Torricelli problem}
\label{sec:analyt-solut-ferm}

An analytical solution for finding the Fermat-Torricelli point of three
prespecified points is presented in~\cite{Uteshev2012}, which is the same as
finding the Steiner point of those three points.  The article presents a
solution for two dimensions which will be described.  However as we are dealing
with  higher dimensions, a generalization of the solution, not presented
in~\cite{Uteshev2012}, will also be described.

\TODO[inline]{Why is this interesting/how has it been used in the code?}

\subsection{2D}
\label{sec:2d}

The solution for two dimensions is presented in~\cite{Uteshev2012}.  The solution
presented is for the generalized Fermat-Torricelli problem, which given three
noncolinear points $P_1 = (x_1, y_1)$, $P_2 = (x_2, y_2)$ and $P_3 = (x_3, y_3)$
in the plane, finds the point $P_\ast = (x_\ast, y_\ast)$ which gives a solution
to the optimization problem
%
\begin{equation}
  \min_{(x,y)} F(x,y) \quad \text{for} \quad F(x,y) = \sum_{j=1}^3 m_j
  \sqrt{{(x-x_j)}^2 + {(y-y_j)}^2}
\end{equation}
%
Where the weight of point $j$th point, $m_j$, is a real positive number.  The
instance where $m_1 = m_2 = m_3 = 1$ is exactly the classical Fermat-Torricelli
problem, which has a unique solution either coinciding with one of the points
$P_1$, $P_2$, $P_3$ or with the Fermat-Torricelli or Steiner point of the
triangle $P_1 P_2 P_3$.  The instance with unequal weights is known as the
generalized Fermat-Torricelli point.

Existence and uniqueness of the solution is guaranteed by
\cref{thm:fermat-torricelli}
%
\begin{theorem}
\label{thm:fermat-torricelli}
  Denote the corner angles of the triangle $P_1 P_2 P_3$ by $\alpha_1, \alpha_2,
  \alpha_3$. Then if the conditions
  \begin{equation}
    \label{eq:12}
    \left\{
      \begin{array}{c}
        m_1^2 < m_2^2 + m_3^2 + 2 m_2 m_3 \cos \alpha_1 , \\
        m_2^2 < m_1^2 + m_3^2 + 2 m_1 m_3 \cos \alpha_2 , \\
        m_3^2 < m_1^2 + m_2^2 + 2 m_1 m_2 \cos \alpha_3 , \\
      \end{array}
    \right.
  \end{equation}
  \TODO[inline]{Describe these equations and from where they come.}
  are fulfilled then there exists a unique solution $P_\ast = (x_\ast, y_\ast)
  \in \mathbb{R}^2$ for the generalized Fermat-Torricelli problem lying inside
  the triangle $P_1 P_2 P_3$.  This point is a stationary point for the function
  $F(x,y)$, i.e.\ a real solution of the system
  \begin{equation}
    \label{eq:13}
    \sum_{j=1}^3 \frac{m_j(x-x_j)}{\sqrt{{(x-x_j)}^2 + {(y - y_j)}^2}} = 0, \quad
    \sum_{j=1}^3 \frac{m_j(y-y_j)}{\sqrt{{(x-x_j)}^2 + {(y - y_j)}^2}} = 0.
  \end{equation}
  \TODO[inline]{Describe these equations and from where they come.}
  If any of the conditions in \cref{eq:12} is violated then $F(x,y)$ attains its
  minimum value at the corresponding vertex of triangle.
\end{theorem}
%
Calculating the coordinates of the point $(x_\ast, y_\ast)$, if the conditions
in \cref{eq:12} are fulfilled, is done as follows
\begin{gather}
  \label{eq:15}
  x_\ast = \frac{K_1 K_2 K_3}{4 S \sigma d} \left( \frac{x_1}{K_1} +
  \frac{x_2}{K_2} + \frac{x_3}{K_3} \right), \quad
  y_\ast = \frac{K_1 K_2 K_3}{4 S \sigma d} \left( \frac{y_1}{K_1} +
  \frac{y_2}{K_2} + \frac{y_3}{K_3} \right)
  \intertext{with}
  \label{eq:11}
  F(x_\ast, y_\ast) = \min_{(x,y)} F(x,y) = \sqrt{d}
  \intertext{Here}
  \label{eq:20}
  S = | x_1 y_2 + x_2 y_3 + x_3 y_1 - x_1 y_3 -x_3 y_2 - x_2 y_1 | \\
  \sigma = \frac{1}{2} \sqrt{-m_1^4 - m_2^4 - m_3^4
    + 2 m_1^2 m_2^2 + 2 m_1^2 m_3^2 + 2 m_2^2 m_3^2} \\
  r_{jl} = \sqrt{{(x_j - x_l)}^2 + {(y_j - y_l)}^2} = |P_j P_l| \label{eq:1}
  \quad \{j,l\} \in \{ 1, 2, 3 \} \\
  \label{eq:3}
  \left\{
    \begin{array}{c}
      K_1 = (r_{12}^2 + r_{13}^2 - r_{23}^2) \sigma + (m_2^2 + m_3^2 - m_1^2) S
      \\
      K_2 = (r_{12}^2 + r_{23}^2 - r_{13}^2) \sigma + (m_1^2 + m_3^2 - m_2^2) S
      \\
      K_3 = (r_{23}^2 + r_{13}^2 - r_{12}^2) \sigma + (m_1^2 + m_2^2 - m_3^2) S
    \end{array}
  \right. \\
  d = \frac{1}{2 \sigma} (m_1^2 K_1 + m_2^2 K_2 + m_3^2 + K_3) \\
\end{gather}

To prove \cref{eq:15} the first step is to prove the two following
equations\footnote{Note that all the following calculations can also be found
  in~\cite{Uteshev2012}, however as these are a bit convoluted they are also
  presented with a few clarifying intermediate calculations}
%
\begin{align}
  \label{eq:16}
  K_1 K_2 + K_1 K_3 + K_2 K_3 &= 4 \sigma S d \\
  \label{eq:2}
  r_{23}^2 K_1 + r_{13}^2 K_2 + r_{12}^2 K_3 &= 2 S d
\end{align}
%
\NOTE[inline]{Possibly actually do this calculations.}
Both can relatively easily be proven by simply substituting the
right hand sides with the definition of each symbol.  Using \cref{eq:16} we can
rewrite \cref{eq:15} as
%
\begin{gather}
  \label{eq:17}
  x_\ast = \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}} \left( \frac{x_1}{K_1} +
    \frac{x_2}{K_2} + \frac{x_3}{K_3} \right), \quad
  y_\ast = \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}} \left( \frac{y_1}{K_1} +
    \frac{y_2}{K_2} + \frac{y_3}{K_3} \right)
  \intertext{Which will be useful in later calculations.  The second part is to
    prove that}
  \label{eq:18}
  \sqrt{{(x_\ast - x_j)}^2 + {(y_\ast - y_j)}^2} = \frac{m_j K_j}{2 \sigma
    \sqrt{d}}, \quad j \in \{ 1, 2, 3 \}
\end{gather}
%
As the calculations are similar for all $j$, only the one for $j = 1$ is shown
here. Thus
%
\begin{align}
  {(x_\ast - x_1)}^2
  &+ {(y_\ast - y_1)}^2 \\
  &= {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \left( \frac{x_1}{K_1} +
    \frac{x_2}{K_2} + \frac{x_3}{K_3} \right) - x_1 \right)}^2 +
    {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \left( \frac{y_1}{K_1} +
    \frac{y_2}{K_2} + \frac{y_3}{K_3} \right) - y_1 \right)}^2 \\
  &= {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right)}^2 \left[{\left(
    \frac{x_1}{K_1} + \frac{x_2}{K_2} + \frac{x_3}{K_3} -
    \frac{x_1 4 S \sigma d}{K_1 K_2 K_3} \right)}^2 + {\left(
    \frac{y_1}{K_1} + \frac{y_2}{K_2} + \frac{y_3}{K_3} -
    \frac{y_1 4 S \sigma d}{K_1 K_2 K_3} \right)}^2 \right] \\
  &\stackrel{\mathclap{(\ref{eq:16})}}{=}
    {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right)}^2
    \left[ {\left( \frac{x_2}{K_2} + \frac{x_3}{K_3} - \frac{x_1}{K_2} -
    \frac{x_1}{K_3} \right)}^2 + {\left( \frac{y_2}{K_2} + \frac{y_3}{K_3} -
    \frac{y_1}{K_2} - \frac{y_1}{K_3} \right)}^2 \right] \\
  &= {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right)}^2
    \left[ \frac{{(x_2 - x_1)}^2 + {(y_2 - y_1)}^2}{K_2^2} +
    \frac{{(x_3 - x_1)}^2 + {(y_3 - y_1)}^2}{K_3^2} \right. \\
  &\hspace{7em} \left. + \; 2 \frac{(x_2 - x_1)(x_3 - x_1) +
    (y_2 - y_1)(y_3 - y_1)}{K_2 K_3} \right] \\
  &\stackrel{\mathclap{(\ref{eq:1})}}{=}
    {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right)}^2
    \left[ \frac{r_{12}^2}{K_2^2} + \frac{r_{13}^2}{K_3^2} +
    \frac{r_{12}^2 + r_{13}^2 - r_{23}^2}{K_2 K_3} \right] \\
  &= \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ r_{12}^2 K_3^2 + r_{13}^2 K_2^2 +
    (r_{12}^2 + r_{13}^2 - r_{23}^2) K_2 K_3 \right] \\
  &= \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ (r_{12}^2 K_3 + r_{13}^2 K_2) (K_2 + K_3) -
    r_{23}^2 K_2 K_3 \right] \\
  &\stackrel{\mathclap{(\ref{eq:2})}}{=}
    \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ (2 S d - r_{23}^2 K_1) (K_2 + K_3) - r_{23}^2 K_2 K_3 \right] \\
  &= \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ 2 S d (K_2 + K_3) - r_{23}^2 (K_1 K_2 + K_1 K_3 + K_2 K_3) \right] \\
  &\stackrel{\mathclap{(\ref{eq:16})}}{=}
    \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ 2 S d (K_2 + K_3) - 4 r_{23}^2 \sigma S d \right] \\
  &= \frac{2 S d K_1^2}{{(4 S \sigma d)}^2}
    \left[ K_2 + K_3 - 2 r_{23}^2 \sigma \right] \\
  &\stackrel{\mathclap{(\ref{eq:3})}}{=}
    \frac{K_1^2}{8 S d \sigma^2}
    \left[ 2 m_1^2 S \right] \\
  &= \frac{m_1^2 K_1^2}{4 \sigma^2 d}
\end{align}
%
To finish the proof one would also have to show that $K_1, K_2, K_3$ are
nonnegative.  The proof of this can be found~\cite[p.~5-6]{Uteshev2012}.

We can now prove \cref{eq:15}, by substituting it into \cref{eq:13}. As the
equations are similar for $x$ and $y$, only $x$ is shown here
%
\begin{align}
  0
  &= \frac{m_1 (x_\ast - x_1)}{\sqrt{{(x_\ast - x_1)}^2 + {(y_\ast - y_1)}^2}} +
    \frac{m_2 (x_\ast - x_2)}{\sqrt{{(x_\ast - x_2)}^2 + {(y_\ast - y_2)}^2}} +
    \frac{m_3 (x_\ast - x_3)}{\sqrt{{(x_\ast - x_3)}^2 + {(y_\ast - y_3)}^2}} \\
  &\stackrel{\mathclap{(\ref{eq:18})}}=
    \frac{m_1 (x_\ast - x_1)}{\frac{m_1 K_1}{2 \sigma \sqrt{d}}} +
    \frac{m_2 (x_\ast - x_2)}{\frac{m_2 K_2}{2 \sigma \sqrt{d}}} +
    \frac{m_3 (x_\ast - x_3)}{\frac{m_3 K_3}{2 \sigma \sqrt{d}}} \\
  &= 2 \sigma \sqrt{d} \left[
    \frac{x_\ast - x_1}{K_1} +
    \frac{x_\ast - x_2}{K_2} +
    \frac{x_\ast - x_3}{K_3} \right] \\
  &= 2 \sigma \sqrt{d} \left[
    x_\ast \left( \frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3} \right) -
    \left( \frac{x_1}{K_1} + \frac{x_2}{K_2} + \frac{x_3}{K_3} \right)
    \right] \\
  &\stackrel{\mathclap{(\ref{eq:17})}}=
    2 \sigma \sqrt{d} \left[
    \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}} \left( \frac{x_1}{K_1} +
    \frac{x_2}{K_2} + \frac{x_3}{K_3} \right)
    \left( \frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3} \right) -
    \left( \frac{x_1}{K_1} + \frac{x_2}{K_2} + \frac{x_3}{K_3} \right)
    \right] \\
  &= 2 \sigma \sqrt{d} \left[
    \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}}
    \left( \frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3} \right) -
    1 \right] \\
  &= 2 \sigma \sqrt{d} \left[ 1 - 1 \right] = 0
\end{align}
%
As can be seen \cref{eq:15} is a solution to \cref{eq:13}.  Secondly we can also
prove \cref{eq:11}
%
\begin{align}
  F(x_\ast,y_\ast)
  &= \sum_{j=1}^3 m_j \sqrt{{(x_\ast - x_j)}^2 + {(y_\ast - y_j)}^2} \\
  &\stackrel{\mathclap{(\ref{eq:18})}}{=}
    \sum_{j=1}^3 \frac{m_j^2 K_j}{2 \sigma \sqrt{d}} \\
  &\stackrel{\mathclap{(\ref{eq:20})}}{=}
    \frac{2 \sigma d}{2 \sigma \sqrt{d}} = \sqrt{d}
\end{align}

\subsection{Generalization to higher dimensions}
\label{sec:gener-high-dimens}

Generalizing the solution described above to higher dimensions is in general
pretty straight forward. The first change is that we no longer try to minimize a
function of just two parameters, but instead we intend to minimize a function of
$d$ parameters, where our points are contained in $R^d$, i.e.\ we have $d$
dimensions.  As we normally represent this using vectors, we would not actually
have $d$ parameters, but instead one parameter being a $d$-vector. Thus we wish
to find a point $P_\ast = (x_{\ast,1}, x_{\ast,2}, \ldots, x_{\ast,d}) \in R^d$
which minimizes the following function for the three points
$P_1 = (x_{1,1}, x_{1,2}, \ldots, x_{1,d}), P_2 = (x_{2,1}, x_{2,2}, \ldots,
x_{2,d}), P_3 = (x_{3,1}, x_{3,2}, \ldots, x_{3,d}) \in R^d$.
%
\begin{gather}
  \min_{(x_1, x_2, \ldots, x_d)} F(x_1, x_2, \ldots, x_d) \quad \text{for} \quad
  F(x_1, x_2, \ldots, x_d) = \sum_{j=1}^3 m_j \sqrt{\sum_{i=1}^d {(x_i -
    x_{j,i})}^2 }
  \\ \Updownarrow \\
  \label{eq:4}
  \min_{(P)} F(P) \quad \text{for} \quad
  F(P) = \sum_{j=1}^3 m_j | P P_j |, \quad P \in R^d
\end{gather}
%
Where $|P P_j|$ is the Euclidean distance between $P$ and $P_j$.
The point $P_\ast$ now no longer has to be a solution to the
equation system in \cref{eq:13}, but to a similar system containing $d$
equations, as follows
%
\begin{equation}
\label{eq:7}
  \left\{
    \begin{array}{c}
    \sum_{j=1}^3 \frac{m_j(x_1-x_{j,1})}{| P P_j|} = 0
    \\
    \sum_{j=1}^3 \frac{m_j(x_2-x_{j,2})}{| P P_j|} = 0
    \\
    \vdots
    \\
    \sum_{j=1}^3 \frac{m_j(x_d-x_{j,d})}{| P P_j|} = 0
  \end{array}
  \right.
\end{equation}
%
Then under the conditions in  presented in \cref{eq:12} the solution to the
\cref{eq:7} can be found as follows



\section{Sorting points}
\label{sec:sorting-points}

longest path in fully connected graph is NP-complete [ref]. Greedy approach
shows promising results.

\chapterbreak{}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
