 {
\abnormalparskip{0pt}
\chapter{Implementation}
\label{cha:implementation}
}

The new implementation has been written in the, still relatively new but at this
point stable, language Go, which quoting the homepage: ``Go is an open source
programming language that makes it easy to build simple, reliable, and efficient
software.''~\cite{golanghomepage}. Go is a general-purpose, strongly typed and
garbage-collected language. The syntax bears resemblance to C\footnote{But
  with a lot less braces, and no pointer arithmetic.}. Furthermore Go has
explicit support for concurrent programming. The language furthermore sports
extremely fast compiling of source code and tries to have performance
comparable to C.

The reasoning behind choosing Go for the new implementation were firstly that I
already had experience with the language making it easy to get to work on the
new implementation without also having to learn a new programming
language. Secondly the language has, in my opinion, a very clean syntax which
makes it easily readable for anyone familiar with any C-syntax styled
language. Finally the easy use of concurrency in Go\footnote{Starting a new
  goroutine, which is the languages type of lightweight threads, is as simple as
  writing: \texttt{go~SomeFunction~(args\ldots)}.} made it interesting as
continued work with the new implementation could include a potential,
significant, speedup by making the branching of the algorithm concurrent. The
same could be be true for the Gauss elimination, concurrent\footnote{Not
  concurrency has actually been implemented in the new implementation. However
  the possibilities of this is discussed in \cref{sec:concurrency}.}.

In general the implementation tries to follow guidelines given in
\textcite{effectivego}. These are guidelines as to how one codes effective
Go. Here effective means both in terms of speed, memory usage, stability and
readability.

Source code for the new implementation, fixes for the old implementation and
experiments can be found at \url{https://github.com/jsfr/SteinerExact}. The
source code for just the new implementation is also included in
\cref{cha:source-code}.

This chapter will describe the architecture of the new implementation, i.e.\ how
the new implementation has been structured. Furthermore the chapter will
describe some more notable elements of the new implementation, namely the way in
which the new implementation builds topologies, and the new iteration using the
analytical method for solving the Fermat-Torricelli problem, as described by
\textcite{uteshev2014}. This also includes a description of said analytical
method and a generalization to $\mathbb{R}^{d}, d \ge 3$. Finally, will a way
of sorting the terminals---also implemented---be described.

\section{Architecture}
\label{sec:architecture-1}

\subsection{Main file}
\label{sec:main-file}

\subsection{\ac{smt} Package}
\label{sec:smt-package}

\subsubsection{Data Structures}
\label{sec:data-structures}

\subsubsection{Methods}
\label{sec:methods}

\TODO[inline]{Describe the source code architecture}

\section{Building topologies}
\label{sec:building-topologies}

In \textcite{smith1992} implementation of the algorithm when generating
topologies every new topology is rebuilt completely from the bottom. E.g.\ say
that we have just optimized the topology corresponding to the topology vector
$A_1 = [1, 2]$. If we now wish to optimize the topology vector
$A_2 = [1, 2, 3]$. In the old implementation $A_2$ would have to be built again
from scratch instead of simply splitting and building on to $A_1$. To try and do
this is in a slightly more clever way the new implementation is instead able to
either split or restore a topology, thus from $A_1$ we could either split an
edge and get $A_2$, or any other length three topology vector having $A_1$ as a
prefix, or we could restore and go back from $A_1$ to $A_0 = [1]$\footnote{From
  $A_0$ we could then either split again to e.g.\ the next length two topology
  vector $A_1' = [1, 3]$ or restore once again.}

The way this is implemented is a follows: All edges of the tree are kept in an
list, and edge being represented as the tuple $(a, b)$ meaning it starts at
point $p_a$ and ends at point $p_b$. Say we have $n$ terminals in all, and have
currently connected the $i$ first terminals. This means we have $i-2$ Steiner points
continuously enumerated from $n+1$ up to $n+i-1$ and $2i-3$ edges enumerated
continuously from $1$.

Now we wish to split some edge $e_j = (c, d)$ where $1 \le j \le 2i-3$, so any
of the existing edges, and connect the next terminal $p_{i+1}$ to the existing
topology. We do this by first changing the original edge to go from the first
end point $p_c$ to a new Steiner point $p_{n+i}$. Afterwards we append two new
edges to the list of edges, $e_{2i-2} = (d, n+i)$ and $e_{2i-1} = (i+1, n+i)$
thus connecting $p_d$ and $p_{i+1}$ to Steiner point $p_{n+i}$ as well. This
process is straight forward and if the accessing into the list and
appending\footnote{which is both the case if enough space is allocated for the
  list ahead of the appending.} is $\mathcal{O}(1)$ then the entire operation
will be $\mathcal{O}(1)$. The operation can be seen in
\cref{fig:splitting-topology}. In this way we can expand the topology one
terminal at a time without rebuilding every time.

\begin{figure}[htbp]
  \centering
  \input{gfx/splitting-topology}
  \caption[Implementation of topology splitting]{Splitting on edge $e_j$ with
    $i$ terminals currently connected, results in two new edges $e_{2i-2}$ and
    $e_{2i-1}$ and a change of the existing edge $e_j$. Finally we also get the new
    Steiner point $n+i$.\label{fig:splitting-topology}}
\end{figure}

The reason for this way of expanding the topology however lies with the wish to
also restore an edge. If we keep a list of all the edges we split
on\footnote{which we do, as this is the topology vector described in
  \cref{sec:representation}.} we can always restore the last edge split in the
following way. Select the last two edges, $(d, n+i), (i+1, n+i)$ in the list and
the edge $e_j = (c, n+i)$. Now change edge $e_j$ to $(c, d)$ and remove the last
two edges in the list. Thus we have $\mathcal{O}(1)$ methods for splitting and
restoring the topology, in a way which is consistent in the numbering with the
one described in \cref{sec:representation} and shown in
\cref{fig:algorithm-topologies}.

\subsection{Order of building and optimizing}
\label{sec:order-build-optim}

When going over the implementation done by \citeauthor{smith1992} a thing that
stroke me as odd initially was the way in which \citeauthor{smith1992} keeps
track of which topology vectors has been tried, and how he chooses which to try
next.

In the following description, $\ast$ is used to represent all possible values,
so e.g.\ $A = [1, \ast]$ would be the same as writing
$A = [1, 1], A = [1, 2], A = [1,3], A = [1, 4]$. The implementation does as
follows:
%
\begin{enumerate}
\item Start with the empty topology vector, meaning we have a topology with $3$
  terminals, $1$ Steiner point and $3$ edges.
\item Build and optimize all topology vectors of length $1$, i.e.\ $A = [\ast]$.
\item When finished with optimizing a topology store its length in a stack
  ordered such that the topology vector with the shortest tree length is on top.
\item Now select the top topology vector on the stack, and using this a prefix,
  build and optimize all topology vectors of length $2$. So say we have chosen $A = [1]$,
  then build $A = [1, \ast]$. Again put the
  topology vectors on a stack in order of their increasing tree length.
\item Continue building the topology vector in this way, until we have a
  topology vector which connects all terminals.
\item At this point go back a level, and choose the next topology vector on top
  of the stack, i.e.\ the second best. So say e.g.\ we have $5$ terminals in all
  and have just optimized $A = [1, \ast]$. Then if $A = [2]$ was the shortest tree
  after $A = [1]$ we would build and optimize $A = [2, \ast]$ next.
\item Continue building and optimizing the vectors in this way, putting them on,
  and popping them of the stack until the stack is empty.
\end{enumerate}
%
Another way to look at the approach, is to see the topology as a tree
structure. What we then do, is that we optimize every child of the node at which
we are currently at, and select the one with the shortest tree. At this node we
then again optimize all its children and select the shortest tree. When going
backwards we then just select the second best, third best and so on.

This approach in which we actually optimize a breadth and then select which
vector to continue the depth-first search on, may seem a bit elaborate in
contrast to performing a simple depth-first search in which we e.g.\ always kept
as far to the left as possible in the tree so we would optimize a topology of
$6$ terminals as
%
\begin{align}
  A & = [1]     \\
  A & = [1,1]   \\
  A & = [1,1,1] \\
  A & = [1,1,2] \\
    & \vdots    \\
  A & = [1,2]   \\
  A & = [1,2,1] \\
    & \vdots    \\
  A & = [2]     \\
    & \vdots
\end{align}
%
It does however turn out to be a much more efficient way of going through the
topologies than the more naive approach just described. The first iteration of
the new implementation did not use the approach taken by \citeauthor{smith1992},
but instead the naive approach. This caused a significant slow-down\footnote{No
  formal tests were made as the original implementation was several magnitudes
  faster, and thus it was obvious that something needed to be changed.} when
compared with the original implementation. Because of this, the new
implementation uses this more complex method.

The reason that the more complex method is better, lies in the number of
topologies which one is able to prune with the two approaches.

When using the more complex method we in general get better upper bounds much
faster, and thus we become able to prune more topologies and do so
faster. Imagine e.g.\ that we have $n = 6$. In the naive approach we would
simply start to plow through the topologies descending from $A = [1]$. However
it may very well be that $A = [3]$ shows a lot more promise, and if we had
optimized this we could have pruned both $A = [1]$ and maybe also $A =
[2]$. This is exactly what the more complex method tries to do by always going
the route which currently shows the most promise.

\section{Analytical solution to the Fermat-Torricelli problem}
\label{sec:analyt-solut-ferm}

An analytical solution for finding the Fermat-Torricelli point of three
prespecified points in 2D is presented by \textcite{uteshev2014}, which is the same as
finding the Steiner point of those three points. The article presents a
solution for two dimensions which will be described. However as we are working
in $\mathbb{R}^d$ a generalization of the solution to higher dimensions will
also be presented. This generalization is as far as I know, not presented
anywhere else in the literature.

The reason this is interesting is that \textcite{smith1992} mentions an
iterative process which updates points one at a time would not be as quick to
converge as the one he presents. There is however no experiments in the paper to
support this. Thus this analytical method is interesting, as it is actually not
computationally heavy to compute the location of a Steiner point, and as each
optimization moves a Steiner point to the exact location at which it minimizes
the length to its three neighbors at the time. One could therefore think that this might
actually converge quite fast.

An optimization method using the analytical method has been implemented in the
new implementation. To use this instead of \citeauthor{smith1992}'s iteration
give the flag \texttt{-iteration=simple} on the command-line\footnote{as opposed
  to either having no flag or having \texttt{-iteration=smith} which both uses
  \citeauthor{smith1992}'s iteration.}.

\subsection{2D}
\label{sec:2d}

The solution for two dimensions is presented by \textcite{uteshev2014} is for
the generalized Fermat-Torricelli problem, which given three non-colinear points
$p_1 = (x_1, y_1)$, $p_2 = (x_2, y_2)$ and $p_3 = (x_3, y_3)$ in the plane,
finds the point $p_\ast = (x_\ast, y_\ast)$ which gives a solution to the
optimization problem
%
\begin{equation}
  \min_{(x,y)} F(x,y) \quad \text{for} \quad F(x,y) = \sum_{j=1}^3 m_j
  \sqrt{{(x-x_j)}^2 + {(y-y_j)}^2}
\end{equation}
%
Where the weight of the $j$th point, $m_j$, is a real positive number. The
instance where $m_1 = m_2 = m_3 = 1$ is exactly the classical Fermat-Torricelli
problem, which has a unique solution either coinciding with one of the points
$p_1$, $p_2$, $p_3$ or with the Fermat-Torricelli or Steiner point of the
triangle $p_1 p_2 p_3$. The instance with unequal weights is known as the
generalized Fermat-Torricelli point.

As we only need the classical version of the problem, I have decided to rewrite
the proof done by \textcite{uteshev2014} for the 2D case with the weights set to
$1$. In a lot of ways, the proof is the same, however it does become a bit
simpler and cleaner. The original proof for the generalized version can be found
in \textcite{uteshev2014}. Thus what we wish to minimize is instead the
following problem:
%
\begin{equation}
  \min_{(x,y)} F(x,y) \quad \text{for} \quad F(x,y) = \sum_{j=1}^3
  \sqrt{{(x-x_j)}^2 + {(y-y_j)}^2}
\end{equation}
%
Existence and uniqueness of the solution is guaranteed by
\cref{thm:fermat-torricelli}
%
\begin{theorem}
\label{thm:fermat-torricelli}
  Denote the corner angles of the triangle $\triangle p_1 p_2 p_3$ by $\alpha_1, \alpha_2,
  \alpha_3$. Then if the conditions
  %
  \begin{equation}
    \label{eq:12}
    \left\{
      \begin{array}{c}
        1 < 2 + 2 \cos \alpha_1 , \\
        1 < 2 + 2 \cos \alpha_2 , \\
        1 < 2 + 2 \cos \alpha_3 , \\
      \end{array}
    \right.
  \end{equation}

  \PAWEL{Where do these equations come from?}

  are fulfilled then there exists a unique solution $p_\ast = (x_\ast, y_\ast)
  \in \mathbb{R}^2$ for the classic Fermat-Torricelli problem lying inside
  the triangle $\triangle p_1 p_2 p_3$. This point is a stationary point for the function
  $F(x,y)$, i.e.\ a real solution of the system
  \begin{equation}
    \label{eq:13}
    \sum_{j=1}^3 \frac{(x-x_j)}{\sqrt{{(x-x_j)}^2 + {(y - y_j)}^2}} = 0, \quad
    \sum_{j=1}^3 \frac{(y-y_j)}{\sqrt{{(x-x_j)}^2 + {(y - y_j)}^2}} = 0.
  \end{equation}

  \PAWEL{Why must the solution fulfill these?}

  If any of the conditions in \cref{eq:12} is violated then $F(x,y)$ attains its
  minimum value at the corresponding vertex of triangle.
\end{theorem}
%
Calculating the coordinates of the point $(x_\ast, y_\ast)$, if the conditions
in \cref{eq:12} are fulfilled, is done as follows
\begin{gather}
  \label{eq:15}
  x_\ast = \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \left( \frac{x_1}{K_1} +
  \frac{x_2}{K_2} + \frac{x_3}{K_3} \right), \quad
  y_\ast = \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \left( \frac{y_1}{K_1} +
  \frac{y_2}{K_2} + \frac{y_3}{K_3} \right)
  \intertext{with}
  \label{eq:11}
  F(x_\ast, y_\ast) = \min_{(x,y)} F(x,y) = \sqrt{d}
  \intertext{Here}
  \label{eq:1}
  r_{jl} = \sqrt{{(x_j - x_l)}^2 + {(y_j - y_l)}^2} = |p_j p_l|
  \quad \{j,l\} \in \{ 1, 2, 3 \} \\
  \label{eq:20}
  S = | x_1 y_2 + x_2 y_3 + x_3 y_1 - x_1 y_3 -x_3 y_2 - x_2 y_1 | \\
  \label{eq:3}
  \left\{
    \begin{array}{c}
      K_1 = \frac{\sqrt{3}}{2} (r_{12}^2 + r_{13}^2 - r_{23}^2) + S
      \\
      K_2 = \frac{\sqrt{3}}{2} (r_{12}^2 + r_{23}^2 - r_{13}^2) + S
      \\
      K_3 = \frac{\sqrt{3}}{2} (r_{23}^2 + r_{13}^2 - r_{12}^2) + S
    \end{array}
  \right. \\
  d = \frac{1}{\sqrt{3}} (K_1 + K_2 + K_3) \\
\end{gather}

\begin{proof}
To prove \cref{eq:15} the first step is to prove the two following
equations
%
\begin{align}
  \label{eq:16}
  K_1 K_2 + K_1 K_3 + K_2 K_3 &= 2 \sqrt{3} S d\\
  \label{eq:2}
  r_{23}^2 K_1 + r_{13}^2 K_2 + r_{12}^2 K_3 &= 2 S d
\end{align}
%
Both can proven by simply substituting the right hand sides with the definition
of each symbol, i.e.\ through direct computation. Using \cref{eq:16} we can
rewrite \cref{eq:15} as
%
\begin{gather}
  \label{eq:17}
  \left\{
    \begin{array}{c}
  x_\ast = \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}} \left( \frac{x_1}{K_1} +
    \frac{x_2}{K_2} + \frac{x_3}{K_3} \right) \\
  y_\ast = \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}} \left( \frac{y_1}{K_1} +
    \frac{y_2}{K_2} + \frac{y_3}{K_3} \right)
    \end{array}
  \right.  \intertext{Which will be useful in later calculations. The second
    part is to prove that}
  \sqrt{{(x_\ast - x_j)}^2 + {(y_\ast - y_j)}^2} = \frac{K_j}{\sqrt{3 d}},
\quad j \in \{ 1, 2, 3 \}\label{eq:18}
\end{gather}
%
As the calculations are similar for all $j$, only the one for $j = 1$ is shown
here. Thus
%
\begin{align}
  (x_\ast &- x_1)^2
  + {(y_\ast - y_1)}^2 \\
  &\stackrel{\mathclap{(\ref{eq:15})}}{=}
    {\left( \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \left( \frac{x_1}{K_1} +
    \frac{x_2}{K_2} + \frac{x_3}{K_3} \right) - x_1 \right)}^2 +
    {\left( \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \left( \frac{y_1}{K_1} +
    \frac{y_2}{K_2} + \frac{y_3}{K_3} \right) - y_1 \right)}^2 \\
  &= {\left( \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \right)}^2 \left[{\left(
    \frac{x_1}{K_1} + \frac{x_2}{K_2} + \frac{x_3}{K_3} -
    \frac{x_1 2 \sqrt{3} S d}{K_1 K_2 K_3} \right)}^2 + {\left(
    \frac{y_1}{K_1} + \frac{y_2}{K_2} + \frac{y_3}{K_3} -
    \frac{y_1 2 \sqrt{3} S d}{K_1 K_2 K_3} \right)}^2 \right] \\
  &\stackrel{\mathclap{(\ref{eq:16})}}{=}
    {\left( \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \right)}^2
    \left[ {\left( \frac{x_2}{K_2} + \frac{x_3}{K_3} - \frac{x_1}{K_2} -
    \frac{x_1}{K_3} \right)}^2 + {\left( \frac{y_2}{K_2} + \frac{y_3}{K_3} -
    \frac{y_1}{K_2} - \frac{y_1}{K_3} \right)}^2 \right] \\
  &= {\left( \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \right)}^2
    \left[ \frac{{(x_2 - x_1)}^2 + {(y_2 - y_1)}^2}{K_2^2} +
    \frac{{(x_3 - x_1)}^2 + {(y_3 - y_1)}^2}{K_3^2} \right. \\
  &\hspace{7em} \left. + \; 2 \frac{(x_2 - x_1)(x_3 - x_1) +
    (y_2 - y_1)(y_3 - y_1)}{K_2 K_3} \right] \\
  &\stackrel{\mathclap{(\ref{eq:1})}}{=}
    {\left( \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \right)}^2
    \left[ \frac{r_{12}^2}{K_2^2} + \frac{r_{13}^2}{K_3^2} +
    \frac{r_{12}^2 + r_{13}^2 - r_{23}^2}{K_2 K_3} \right] \label{eq:30} \\
  &= \frac{K_1^2}{{(2 \sqrt{3} S d)}^2}
    \left[ r_{12}^2 K_3^2 + r_{13}^2 K_2^2 +
    (r_{12}^2 + r_{13}^2 - r_{23}^2) K_2 K_3 \right] \\
  &= \frac{K_1^2}{{(2 \sqrt{3} S d)}^2}
    \left[ (r_{12}^2 K_3 + r_{13}^2 K_2) (K_2 + K_3) -
    r_{23}^2 K_2 K_3 \right] \\
  &\stackrel{\mathclap{(\ref{eq:2})}}{=}
    \frac{K_1^2}{{(2 \sqrt{3} S d)}^2}
    \left[ (2 S d - r_{23}^2 K_1) (K_2 + K_3) - r_{23}^2 K_2 K_3 \right] \\
  &= \frac{K_1^2}{{(2 \sqrt{3} S d)}^2}
    \left[ 2 S d (K_2 + K_3) - r_{23}^2 (K_1 K_2 + K_1 K_3 + K_2 K_3) \right] \\
  &\stackrel{\mathclap{(\ref{eq:16})}}{=}
    \frac{K_1^2}{{(2 \sqrt{3} S d)}^2}
    \left[ 2 S d (K_2 + K_3) - 2 \sqrt{3} r_{23}^2 S d \right] \\
  &= \frac{2 S d K_1^2}{{(2 \sqrt{3} S d)}^2}
    \left[ K_2 + K_3 - \sqrt{3} r_{23}^2 \right] \\
  &\stackrel{\mathclap{(\ref{eq:3})}}{=}
    \frac{K_1^2}{6 S d}
    \left[ 2 S \right] \\
  &= \frac{K_1^2}{3 d}
\end{align}
%
Thus
%
\begin{equation}
  \label{eq:29}
  \sqrt{{(x_\ast - x_1)}^2 + {(y_\ast - y_1)}^2}
  = \sqrt{\frac{K_1^2}{3 d}} = \frac{K_1}{\sqrt{3 d}}
\end{equation}
%
To finish the proof one would also have to show that $K_1, K_2, K_3$ are
nonnegative. The proof of this can be found in~\cite[p.~5-6]{uteshev2014}.

We can now prove \cref{eq:15}, by substituting it into \cref{eq:13}. As the
equations are similar for $x$ and $y$, only $x$ is shown here
%
\begin{align}
  0
  &= \frac{(x_\ast - x_1)}{\sqrt{{(x_\ast - x_1)}^2 + {(y_\ast - y_1)}^2}} +
    \frac{(x_\ast - x_2)}{\sqrt{{(x_\ast - x_2)}^2 + {(y_\ast - y_2)}^2}} +
    \frac{(x_\ast - x_3)}{\sqrt{{(x_\ast - x_3)}^2 + {(y_\ast - y_3)}^2}} \\
  &\stackrel{\mathclap{(\ref{eq:18})}}=
    \frac{(x_\ast - x_1)}{\frac{K_1}{\sqrt{3 d}}} +
    \frac{(x_\ast - x_2)}{\frac{K_2}{\sqrt{3 d}}} +
    \frac{(x_\ast - x_3)}{\frac{K_3}{\sqrt{3 d}}} \\
  &= \sqrt{3 d} \left[
    \frac{x_\ast - x_1}{K_1} +
    \frac{x_\ast - x_2}{K_2} +
    \frac{x_\ast - x_3}{K_3} \right] \\
  &= \sqrt{3 d} \left[
    x_\ast \left( \frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3} \right) -
    \left( \frac{x_1}{K_1} + \frac{x_2}{K_2} + \frac{x_3}{K_3} \right)
    \right] \\
  &\stackrel{\mathclap{(\ref{eq:17})}}=
    \sqrt{3 d} \left[
    \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}} \left( \frac{x_1}{K_1} +
    \frac{x_2}{K_2} + \frac{x_3}{K_3} \right)
    \left( \frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3} \right) -
    \left( \frac{x_1}{K_1} + \frac{x_2}{K_2} + \frac{x_3}{K_3} \right)
    \right] \\
  &= \sqrt{3 d} \left[
    \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}}
    \left( \frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3} \right) -
    1 \right] \\
  &= \sqrt{3 d} \left[ 1 - 1 \right] = 0
\end{align}
%
As can be seen \cref{eq:15} is a solution to \cref{eq:13}, and thus it minimizes
$F(x,y)$.
\end{proof}

\subsection{Generalization to higher dimensions}
\label{sec:gener-high-dimens}

Generalizing the solution described above to higher dimensions is in general
pretty straightforward. The biggest change is in the representation of $S$, and
the fact that the proof requires a bit of sum manipulation. This generalized
version of the proof is not known to have been presented in any paper before.

The generalization is as for the 2D variant here done for the classic
Fermat-Torricelli problem. It could however just as easily be done for the
weighted problem, by following the form of the proof in \textcite{uteshev2014}
instead of the form in \cref{sec:2d}.

The first change is that we no longer try to minimize a function of just two
parameters, but instead we intend to minimize a function of $d$ parameters,
where our points are contained in $\mathbb{R}^d$, i.e.\ we have $d$ dimensions. As we
normally represent this using vectors, we would not actually have $d$
parameters, but instead one parameter being a $d$-vector. Thus we wish to find a
point $p_\ast = (x_{(\ast,1)}, x_{(\ast,2)}, \ldots, x_{(\ast,d)}) \in \mathbb{R}^d$
which minimizes the following function for the three points
$p_1 = (x_{(1,1)}, x_{(1,2)}, \ldots, x_{(1,d)}), p_2 = (x_{(2,1)}, x_{(2,2)},
\ldots, x_{(2,d)}), p_3 = (x_{(3,1)}, x_{(3,2)}, \ldots, x_{(3,d)}) \in \mathbb{R}^d$.
%
\begin{gather}
  \min_{(x_1, x_2, \ldots, x_d)} F(x_1, x_2, \ldots, x_d) \quad \text{for} \quad
  F(x_1, x_2, \ldots, x_d) = \sum_{j=1}^3 \sqrt{\sum_{i=1}^d {(x_i -
    x_{(j,i)})}^2 }
  \\ \Updownarrow \\
  \label{eq:4}
  \min_{(p)} F(p) \quad \text{for} \quad
  F(p) = \sum_{j=1}^3 | p p_j |, \quad p \in \mathbb{R}^d
\end{gather}
%
The point $p_\ast$ now no longer has to be a solution to the equation system in
\cref{eq:13}, but to a similar system containing $d$ equations, as follows
%
\begin{equation}
\label{eq:7}
  \left.
    \begin{array}{c}
    \sum_{j=1}^3 \frac{(x_1-x_{(j,1)})}{| p p_j|} = 0
    \\
    \sum_{j=1}^3 \frac{(x_2-x_{(j,2)})}{| p p_j|} = 0
    \\
    \vdots
    \\
    \sum_{j=1}^3 \frac{(x_d-x_{(j,d)})}{| p p_j|} = 0
  \end{array}
  \right\} =
  \sum_{j=1}^3 \frac{(p - p_j)}{| p p_j |} = \vec{0}
\end{equation}
%
Then as before, if the conditions in \cref{eq:12} are fulfilled\footnote{If not
  the point is as before located at the corresponding vertex.} the solution to
\cref{eq:7} can be found as follows
%
\begin{gather}
  p_\ast = (x_{(\ast,1)}, x_{(\ast,2)}, \ldots, x_{(\ast,d)})\label{eq:8}
  \intertext{with}
  x_{(\ast, j)} = \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \left( \frac{x_{(1,j)}}{K_1} +
  \frac{x_{(2,j)}}{K_2} + \frac{x_{(3,j)}}{K_3} \right), \quad 0 < j \le d\label{eq:5}
  \intertext{and}
  F(p_\ast) = \min_{(p)} F(p) = \sqrt{d}\label{eq:21}
  \intertext{Here}
  \label{eq:14} r_{jl} = \sqrt{\sum_{i = 1}^d {(x_{(j,i)} - x_{(l,i)})}^2} = |p_j p_l|
  \quad \{j,l\} \in \{ 1, 2, 3 \} \\
  S = \frac{1}{2} \sqrt{(r_{12} + r_{23} + r_{13}) (r_{23} + r_{13}) (r_{12} +
    r_{13}) (r_{12} + r_{23})} \\
\end{gather}
%
And the rest of the expressions the same as in the 2D version. As can be seen,
all of the expressions, except for $S$ (and the distance which now of course
have more than two dimensions), retain the same representation as in the two
dimensional version. $S$ however looks quite different. Exactly how the new $S$
is related to the old $S$ is still unclear to me. \textcite{uteshev2014}
describes, that $S$ in 2D rewritten, can be recognized as the doubled area of
the triangle $\triangle p_1 p_2 p_3$. This geometrical understanding however
seems to be missing from the higher dimension case. Indeed the idea for the
representation of $S$ stems from talks with my supervisor Pawel Winter, who
pointed me to an earlier implementation of the general case, that he has done
himself. Thus an intuition of the new $S$ is missing, however as can be seen
from the proof below, it does indeed work.
%
\begin{proof}
As before to prove \cref{eq:5} (and thus \cref{eq:8}) the first step once
again is to prove \cref{eq:16} and \cref{eq:2}. These can as before be
established through direct computation, i.e.\ by simply substituting the
expressions for their definitions, until both sides only contain distances,
after which the two sides can be seen to be equal. Using \cref{eq:16} we can
rewrite \cref{eq:5} as
%
\begin{gather}
    x_{(\ast, j)} = \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}} \left( \frac{x_{(1,j)}}{K_1} +
  \frac{x_{(2,j)}}{K_2} + \frac{x_{(3,j)}}{K_3} \right), \quad 0 < j \le
d\label{eq:10}
\intertext{similarly to \cref{eq:17} in the 2D version. Secondly we wish
  to prove that}
  \sqrt{\sum_{i = 1}^d {(x_{(\ast,i)} - x_{j,i})}^2} = \frac{K_j}{\sqrt{3 d}
  }, \quad j \in \{ 1, 2, 3 \}\label{eq:19}
\end{gather}
As the calculations are similar for all $j$, only the one for $j = 1$ is shown
here. Thus
%
\begin{align}
  \sum_{i=1}^d (x_{(\ast,i)} &- x_{(1,i)})^2 \\
  &= \sum_{i=1}^d { \left( \frac{K_1 K_2 K_3}{2 \sqrt{3} S d}
    \left( \frac{x_{(1,i)}}{K_1} + \frac{x_{(2,i)}}{K_2} + \frac{x_{(3,i)}}{K_3}
    \right) - x_{(1,i)} \right) }^2 \\
  &= { \left( \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \right) }^2 \left[ \sum_{i=1}^d
    { \left( \frac{x_{(1,i)}}{K_1} + \frac{x_{(2,i)}}{K_2} +
    \frac{x_{(3,i)}}{K_3} - \frac{x_{(1,i)} 2 \sqrt{3} S d}{K_1 K_2 K_3}
    \right) }^2 \right] \\
  &\stackrel{\mathclap{(\ref{eq:16})}}{=}
    {\left( \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \right)}^2 \left[ \sum_{i=1}^d
    {\left( \frac{x_{(2,i)}}{K_2} + \frac{x_{(3,i)}}{K_3} - \frac{x_{(1,i)}}{K_2} -
    \frac{x_{(1,i)}}{K_3} \right)}^2 \right] \\
  &= {\left( \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \right)}^2 \left[ \sum_{i=1}^d
    \left( \frac{{(x_{(2,i)} - x_{(1,i)})}^2}{K_2^2} +
    \frac{{(x_{(3,i)} - x_{(1,i)})}^2}{K_3^2} \right. \right. \\
  &\hspace{7em} \left. \left. + \; 2 \frac{(x_{(2,i)} - x_{(1,i)})(x_{(3,i)} -
    x_{(1,i)})}{K_2 K_3} \right) \right] \\
  &= {\left( \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \right)}^2
    \left[ \frac{\sum_{i=1}^d {(x_{(2,i)} - x_{(1,i)})}^2}{K_2^2} +
    \frac{\sum_{i=1}^d {(x_{(3,i)} - x_{(1,i)})}^2}{K_3^2} \right. \\
  &+ \left. \frac{ \sum_{i=1}^d {(x_{(2,i)} - x_{(1,i)})}^2 +
    \sum_{i=1}^d {(x_{(3,i)} - x_{(1,i)})}^2 -
    \sum_{i=0}^d {(x_{(2,i)} - x_{(3,i)})}^2}{K_2 K_3} \right] \\
  &\stackrel{\mathclap{(\ref{eq:14})}}{=}
    {\left( \frac{K_1 K_2 K_3}{2 \sqrt{3} S d} \right)}^2
    \left[ \frac{r_{12}^2}{K_2^2} + \frac{r_{13}^2}{K_3^2} +
    \frac{r_{12}^2 + r_{13}^2 - r_{23}^2}{K_2 K_3} \right] \\
  \intertext{As this point we have exactly the same as in \cref{eq:30} and thus
  we can conclude}
  &= \frac{K_1^2}{3 d}
\end{align}
%
From here we can show that \cref{eq:5} is a solution to \cref{eq:7} in the
exactly the same way as
we proved that \cref{eq:15} was a solution to \cref{eq:13}, by substituting
\cref{eq:5} into \cref{eq:7} and using \cref{eq:10} and \cref{eq:19}.
\end{proof}

\section{Sorting the terminals}
\label{sec:sorting-terminals}

The idea of sorting the terminals before one starts to build and optimize
topologies is not new, and has also been suggested by \textcite{smith1992}. The
idea behind sorting the terminals, is to find some way of sorting the terminals,
such that the initial \acp{rmt} have become as large as possible, as fast as
possible. Doing this allows us to early on see if we need to discard a topology
vector as prefix, if its \ac{rmt} is longer than the upper bound, and thus be
able to prune more topologies, than if we added terminals in an arbitrary order.

At least two different kinds of sorting has been implemented by
\textcite{fonseca2014,vanlaarhoven2013}. Both methods have shown promising
results regarding the number of trees one needs to optimize in contrast to not
sorting the terminals.

The first method, presented by \textcite{vanlaarhoven2013}, sorts the terminals
by their decreasing distance to the centroid of the set of terminals, i.e.\ the
first terminal is the farthest away, and the last terminal is the closest. A
potential issue with this way of sorting, is that while the terminals may be far
from the centroid, they can potentially still be rather close to each other,
thus not pruning much faster.

The second method, presented by \textcite{fonseca2014}, sorts the terminals such
that the first three terminals $p_1, p_2, p_3$ maximize the sum of their
pairwise distances, and $p_i, i = 4, 5, \ldots, n$ is the farthest away from
$p_1, p_2, \ldots, p_{i-1}$. This method seems to have a lot of potential as it
tries to ensure that every terminal added is as far away from the current
\ac{rmt} as possible. However one could easily think of positions for the
terminals, such that the terminals $p_4, p_5, \ldots, p_n$ all lie close to each
other, and within the first three terminals. This could mean that the distance
added every step after the initial \ac{rmt} will be spread out on the next few
steps, instead of adding as much as possible as quickly as possible.

Here another method is proposed which is relatively similar to the one proposed
and implemented by \textcite{fonseca2014}, but a lot simpler and requiring fewer
steps. The initial idea for the method was to find the longest path in a fully
connected graph, where the nodes in the graph are the terminals. This problem is
however NP-hard~\cite{longestpathproblem}, and it seems a bit silly to solve a
NP-hard problem to try and improve on a NP-hard problem. Thus the method
proposed here sorts the terminals as follows: Select the initial terminal $p_1$
arbitrarily. From there select the next terminal $p_i, i = 2, 3, \ldots, n$ such
that the distance to $p_{i-1}$ is maximized.

The idea behind this approach, is that the method is extremely easy to implement
and hopefully gives better runtimes, comparable to those of e.g.\
\textcite{fonseca2014}.

If one wishes to use the sorting of the new implementation, this can be done by
passing the flag \texttt{-sort} on the command-line.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
