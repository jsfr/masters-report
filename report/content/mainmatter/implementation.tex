 {
\abnormalparskip{0pt}
\chapter{Implementation}
\label{cha:implementation}
}

This chapter touches on parts of the implementation which are done differently
from Smiths implementation.

The new implementation is written in Googles, still relatively new, language
Golang~\cite{GolangHomepage}

\section{Overview}
\label{sec:overview-1}

\TODO[inline]{Overview of the source code. What has been done where. About the
  language etc.}

\section{Building topologies}
\label{sec:building-topologies}

In Smiths implementation of the algorithm when generating topologies every new
topology is rebuilt completely from the bottom.  E.g.\ say that we have just
optimized the topology corresponding to the topology vector $A_1 = (1, 2)$.  If we
now wish to optimize the topology vector $A_2 = (1, 2, 3)$.  In the old
implementation $A_2$ would have to be built again from scratch instead of simply
splitting and building on to $A_1$.  To try and do this is in a slightly more
clever way the new implementation is instead able to either split or restore a
topology, thus from $A_1$ we could either split an edge and get $A_2$, or any
other length three topology vector, or we could restore and go back from $A_1$
to $A_0 = (1)$\footnote{From $A_0$ we could then either split again to e.g.\ the
next length two topology vector $A_1' = (1, 3)$ or restore once again.}

The way this is implemented is a follows:

- All edges of the tree is kept in an array/list. all edges a represented as a
tuple (j, k) representing an edge going from point j to point k
- When splitting an edge this is done

\begin{figure}[htbp]
  \centering
  \begin{tabular}{|c|ccc|c|ccc|c|}
    \multicolumn{1}{c}{$1$} &
    \multicolumn{3}{c}{ } &
    \multicolumn{1}{c}{$i$} &
    \multicolumn{3}{c}{ } &
    \multicolumn{1}{c}{$k$} \\
    \cline{1-2} \cline{4-6} \cline{8-9}
    $(a,b)$ & & \ldots & & $(c,d)$ & & \ldots & & $(e,f)$ \\
    \cline{1-2} \cline{4-6} \cline{8-9}
  \end{tabular}

  \begin{tabular}{|c|ccc|c|ccc|c|c|c|}
    \multicolumn{1}{c}{$1$} &
    \multicolumn{3}{c}{ } &
    \multicolumn{1}{c}{$i$} &
    \multicolumn{3}{c}{ } &
    \multicolumn{1}{c}{$k$} &
    \multicolumn{1}{c}{$k+1$} &
    \multicolumn{1}{c}{$k+2$} \\
    \cline{1-2} \cline{4-6} \cline{8-11}
    $(a,b)$ & & \ldots & & $(g,s)$ & & \ldots & & $(e,f)$ & $(c,s)$ & $(d,s)$ \\
    \cline{1-2} \cline{4-6} \cline{8-11}
  \end{tabular}
  \caption{Here be dragons}
\end{figure}

\TODO[inline]{Naive approach when building and pruning significantly increases run
time. Explain Smiths approach with a stack sorted on the best seen/most
promising results.}

\section{Analytical solution to the Fermat-Torricelli problem}
\label{sec:analyt-solut-ferm}

An analytical solution for finding the Fermat-Torricelli point of three
prespecified points is presented in~\cite{Uteshev2012}, which is the same as
finding the Steiner point of those three points.  The article presents a
solution for two dimensions which will be described.  However as we are dealing
with  higher dimensions, a generalization of the solution, not presented
in~\cite{Uteshev2012}, will also be described.

\TODO[inline]{Why is this interesting/how has it been used in the code?}

\subsection{2D}
\label{sec:2d}

The solution for two dimensions is presented in~\cite{Uteshev2012}.  The solution
presented is for the generalized Fermat-Torricelli problem, which given three
noncolinear points $P_1 = (x_1, y_1)$, $P_2 = (x_2, y_2)$ and $P_3 = (x_3, y_3)$
in the plane, finds the point $P_\ast = (x_\ast, y_\ast)$ which gives a solution
to the optimization problem
%
\begin{equation}
  \min_{(x,y)} F(x,y) \quad \text{for} \quad F(x,y) = \sum_{j=1}^3 m_j
  \sqrt{{(x-x_j)}^2 + {(y-y_j)}^2}
\end{equation}
%
Where the weight of point $j$th point, $m_j$, is a real positive number.  The
instance where $m_1 = m_2 = m_3 = 1$ is exactly the classical Fermat-Torricelli
problem, which has a unique solution either coinciding with one of the points
$P_1$, $P_2$, $P_3$ or with the Fermat-Torricelli or Steiner point of the
triangle $P_1 P_2 P_3$.  The instance with unequal weights is known as the
generalized Fermat-Torricelli point.

Existence and uniqueness of the solution is guaranteed by
\cref{thm:fermat-torricelli}
%
\begin{theorem}
\label{thm:fermat-torricelli}
  Denote the corner angles of the triangle $P_1 P_2 P_3$ by $\alpha_1, \alpha_2,
  \alpha_3$. Then if the conditions
  \begin{equation}
    \label{eq:12}
    \left\{
      \begin{array}{c}
        m_1^2 < m_2^2 + m_3^2 + 2 m_2 m_3 \cos \alpha_1 , \\
        m_2^2 < m_1^2 + m_3^2 + 2 m_1 m_3 \cos \alpha_2 , \\
        m_3^2 < m_1^2 + m_2^2 + 2 m_1 m_2 \cos \alpha_3 , \\
      \end{array}
    \right.
  \end{equation}
  \TODO[inline]{Describe these equations and from where they come.}
  are fulfilled then there exists a unique solution $P_\ast = (x_\ast, y_\ast)
  \in \mathbb{R}^2$ for the generalized Fermat-Torricelli problem lying inside
  the triangle $P_1 P_2 P_3$.  This point is a stationary point for the function
  $F(x,y)$, i.e.\ a real solution of the system
  \begin{equation}
    \label{eq:13}
    \sum_{j=1}^3 \frac{m_j(x-x_j)}{\sqrt{{(x-x_j)}^2 + {(y - y_j)}^2}} = 0, \quad
    \sum_{j=1}^3 \frac{m_j(y-y_j)}{\sqrt{{(x-x_j)}^2 + {(y - y_j)}^2}} = 0.
  \end{equation}
  \TODO[inline]{Describe these equations and from where they come.}
  If any of the conditions in \cref{eq:12} is violated then $F(x,y)$ attains its
  minimum value at the corresponding vertex of triangle.
\end{theorem}
%
Calculating the coordinates of the point $(x_\ast, y_\ast)$, if the conditions
in \cref{eq:12} are fulfilled, is done as follows
\begin{gather}
  \label{eq:15}
  x_\ast = \frac{K_1 K_2 K_3}{4 S \sigma d} \left( \frac{x_1}{K_1} +
  \frac{x_2}{K_2} + \frac{x_3}{K_3} \right), \quad
  y_\ast = \frac{K_1 K_2 K_3}{4 S \sigma d} \left( \frac{y_1}{K_1} +
  \frac{y_2}{K_2} + \frac{y_3}{K_3} \right)
  \intertext{with}
  \label{eq:11}
  F(x_\ast, y_\ast) = \min_{(x,y)} F(x,y) = \sqrt{d}
  \intertext{Here}
  \sigma = \frac{1}{2} \sqrt{-m_1^4 - m_2^4 - m_3^4
    + 2 m_1^2 m_2^2 + 2 m_1^2 m_3^2 + 2 m_2^2 m_3^2} \\
  \label{eq:1}
  r_{jl} = \sqrt{{(x_j - x_l)}^2 + {(y_j - y_l)}^2} = |P_j P_l|
  \quad \{j,l\} \in \{ 1, 2, 3 \} \\
  \label{eq:20}
  S = | x_1 y_2 + x_2 y_3 + x_3 y_1 - x_1 y_3 -x_3 y_2 - x_2 y_1 | \\
  \label{eq:3}
  \left\{
    \begin{array}{c}
      K_1 = (r_{12}^2 + r_{13}^2 - r_{23}^2) \sigma + (m_2^2 + m_3^2 - m_1^2) S
      \\
      K_2 = (r_{12}^2 + r_{23}^2 - r_{13}^2) \sigma + (m_1^2 + m_3^2 - m_2^2) S
      \\
      K_3 = (r_{23}^2 + r_{13}^2 - r_{12}^2) \sigma + (m_1^2 + m_2^2 - m_3^2) S
    \end{array}
  \right. \\
  d = \frac{1}{2 \sigma} (m_1^2 K_1 + m_2^2 K_2 + m_3^2 K_3) \\
\end{gather}

To prove \cref{eq:15} the first step is to prove the two following
equations\footnote{Note that all the following calculations can also be found
  in~\cite{Uteshev2012}, however as these are a bit convoluted they are also
  presented with a few clarifying intermediate calculations}
%
\begin{align}
  \label{eq:16}
  K_1 K_2 + K_1 K_3 + K_2 K_3 &= 4 \sigma S d \\
  \label{eq:2}
  r_{23}^2 K_1 + r_{13}^2 K_2 + r_{12}^2 K_3 &= 2 S d
\end{align}
%
\NOTE[inline]{Possibly actually do this calculations.}  Both can proven by
simply substituting the right hand sides with the definition of each symbol,
i.e.\ through direct computation. Using \cref{eq:16} we can rewrite \cref{eq:15}
as
%
\begin{gather}
  \label{eq:17}
  \left\{
    \begin{array}{c}
  x_\ast = \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}} \left( \frac{x_1}{K_1} +
    \frac{x_2}{K_2} + \frac{x_3}{K_3} \right) \\
  y_\ast = \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}} \left( \frac{y_1}{K_1} +
    \frac{y_2}{K_2} + \frac{y_3}{K_3} \right)
    \end{array}
  \right.
  \intertext{Which will be useful in later calculations.  The second part is to
    prove that}
  \sqrt{{(x_\ast - x_j)}^2 + {(y_\ast - y_j)}^2} = \frac{m_j K_j}{2 \sigma
    \sqrt{d}}, \quad j \in \{ 1, 2, 3 \}\label{eq:18}
\end{gather}
%
As the calculations are similar for all $j$, only the one for $j = 1$ is shown
here. Thus
%
\begin{align}
  {(x_\ast - x_1)}^2
  &+ {(y_\ast - y_1)}^2 \\
  &= {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \left( \frac{x_1}{K_1} +
    \frac{x_2}{K_2} + \frac{x_3}{K_3} \right) - x_1 \right)}^2 +
    {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \left( \frac{y_1}{K_1} +
    \frac{y_2}{K_2} + \frac{y_3}{K_3} \right) - y_1 \right)}^2 \\
  &= {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right)}^2 \left[{\left(
    \frac{x_1}{K_1} + \frac{x_2}{K_2} + \frac{x_3}{K_3} -
    \frac{x_1 4 S \sigma d}{K_1 K_2 K_3} \right)}^2 + {\left(
    \frac{y_1}{K_1} + \frac{y_2}{K_2} + \frac{y_3}{K_3} -
    \frac{y_1 4 S \sigma d}{K_1 K_2 K_3} \right)}^2 \right] \\
  &\stackrel{\mathclap{(\ref{eq:16})}}{=}
    {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right)}^2
    \left[ {\left( \frac{x_2}{K_2} + \frac{x_3}{K_3} - \frac{x_1}{K_2} -
    \frac{x_1}{K_3} \right)}^2 + {\left( \frac{y_2}{K_2} + \frac{y_3}{K_3} -
    \frac{y_1}{K_2} - \frac{y_1}{K_3} \right)}^2 \right] \\
  &= {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right)}^2
    \left[ \frac{{(x_2 - x_1)}^2 + {(y_2 - y_1)}^2}{K_2^2} +
    \frac{{(x_3 - x_1)}^2 + {(y_3 - y_1)}^2}{K_3^2} \right. \\
  &\hspace{7em} \left. + \; 2 \frac{(x_2 - x_1)(x_3 - x_1) +
    (y_2 - y_1)(y_3 - y_1)}{K_2 K_3} \right] \\
  &\stackrel{\mathclap{(\ref{eq:1})}}{=}
    {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right)}^2
    \left[ \frac{r_{12}^2}{K_2^2} + \frac{r_{13}^2}{K_3^2} +
    \frac{r_{12}^2 + r_{13}^2 - r_{23}^2}{K_2 K_3} \right] \\
  &= \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ r_{12}^2 K_3^2 + r_{13}^2 K_2^2 +
    (r_{12}^2 + r_{13}^2 - r_{23}^2) K_2 K_3 \right] \\
  &= \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ (r_{12}^2 K_3 + r_{13}^2 K_2) (K_2 + K_3) -
    r_{23}^2 K_2 K_3 \right] \\
  &\stackrel{\mathclap{(\ref{eq:2})}}{=}
    \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ (2 S d - r_{23}^2 K_1) (K_2 + K_3) - r_{23}^2 K_2 K_3 \right] \\
  &= \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ 2 S d (K_2 + K_3) - r_{23}^2 (K_1 K_2 + K_1 K_3 + K_2 K_3) \right] \\
  &\stackrel{\mathclap{(\ref{eq:16})}}{=}
    \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ 2 S d (K_2 + K_3) - 4 r_{23}^2 \sigma S d \right] \\
  &= \frac{2 S d K_1^2}{{(4 S \sigma d)}^2}
    \left[ K_2 + K_3 - 2 r_{23}^2 \sigma \right] \\
  &\stackrel{\mathclap{(\ref{eq:3})}}{=}
    \frac{K_1^2}{8 S d \sigma^2}
    \left[ 2 m_1^2 S \right] \\
  &= \frac{m_1^2 K_1^2}{4 \sigma^2 d}
\end{align}
%
To finish the proof one would also have to show that $K_1, K_2, K_3$ are
nonnegative.  The proof of this can be found~\cite[p.~5-6]{Uteshev2012}.

We can now prove \cref{eq:15}, by substituting it into \cref{eq:13}. As the
equations are similar for $x$ and $y$, only $x$ is shown here
%
\begin{align}
  0
  &= \frac{m_1 (x_\ast - x_1)}{\sqrt{{(x_\ast - x_1)}^2 + {(y_\ast - y_1)}^2}} +
    \frac{m_2 (x_\ast - x_2)}{\sqrt{{(x_\ast - x_2)}^2 + {(y_\ast - y_2)}^2}} +
    \frac{m_3 (x_\ast - x_3)}{\sqrt{{(x_\ast - x_3)}^2 + {(y_\ast - y_3)}^2}} \\
  &\stackrel{\mathclap{(\ref{eq:18})}}=
    \frac{m_1 (x_\ast - x_1)}{\frac{m_1 K_1}{2 \sigma \sqrt{d}}} +
    \frac{m_2 (x_\ast - x_2)}{\frac{m_2 K_2}{2 \sigma \sqrt{d}}} +
    \frac{m_3 (x_\ast - x_3)}{\frac{m_3 K_3}{2 \sigma \sqrt{d}}} \\
  &= 2 \sigma \sqrt{d} \left[
    \frac{x_\ast - x_1}{K_1} +
    \frac{x_\ast - x_2}{K_2} +
    \frac{x_\ast - x_3}{K_3} \right] \\
  &= 2 \sigma \sqrt{d} \left[
    x_\ast \left( \frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3} \right) -
    \left( \frac{x_1}{K_1} + \frac{x_2}{K_2} + \frac{x_3}{K_3} \right)
    \right] \\
  &\stackrel{\mathclap{(\ref{eq:17})}}=
    2 \sigma \sqrt{d} \left[
    \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}} \left( \frac{x_1}{K_1} +
    \frac{x_2}{K_2} + \frac{x_3}{K_3} \right)
    \left( \frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3} \right) -
    \left( \frac{x_1}{K_1} + \frac{x_2}{K_2} + \frac{x_3}{K_3} \right)
    \right] \\
  &= 2 \sigma \sqrt{d} \left[
    \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}}
    \left( \frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3} \right) -
    1 \right] \\
  &= 2 \sigma \sqrt{d} \left[ 1 - 1 \right] = 0
\end{align}
%
As can be seen \cref{eq:15} is a solution to \cref{eq:13}.  Secondly we can also
prove \cref{eq:11}
%
\begin{align}
  F(x_\ast,y_\ast)
  &= \sum_{j=1}^3 m_j \sqrt{{(x_\ast - x_j)}^2 + {(y_\ast - y_j)}^2} \\
  &\stackrel{\mathclap{(\ref{eq:18})}}{=}
    \sum_{j=1}^3 \frac{m_j^2 K_j}{2 \sigma \sqrt{d}} \\
  &\stackrel{\mathclap{(\ref{eq:20})}}{=}
    \frac{2 \sigma d}{2 \sigma \sqrt{d}} = \sqrt{d}
\end{align}

\subsection{Generalization to higher dimensions}
\label{sec:gener-high-dimens}

Generalizing the solution described above to higher dimensions is in general
pretty straightforward.  The biggest change is in the representation of $S$, and
the fact that the proof requires a bit of sum manipulation.  This generalized
version of the proof is not known to have been presented in any paper before.

The first change is that we no longer try to minimize a function of just two
parameters, but instead we intend to minimize a function of $d$ parameters,
where our points are contained in $R^d$, i.e.\ we have $d$ dimensions.  As we
normally represent this using vectors, we would not actually have $d$
parameters, but instead one parameter being a $d$-vector. Thus we wish to find a
point $P_\ast = (x_{(\ast,1)}, x_{(\ast,2)}, \ldots, x_{(\ast,d)}) \in R^d$
which minimizes the following function for the three points
$P_1 = (x_{(1,1)}, x_{(1,2)}, \ldots, x_{(1,d)}), P_2 = (x_{(2,1)}, x_{(2,2)},
\ldots, x_{(2,d)}), P_3 = (x_{(3,1)}, x_{(3,2)}, \ldots, x_{(3,d)}) \in R^d$.
%
\begin{gather}
  \min_{(x_1, x_2, \ldots, x_d)} F(x_1, x_2, \ldots, x_d) \quad \text{for} \quad
  F(x_1, x_2, \ldots, x_d) = \sum_{j=1}^3 m_j \sqrt{\sum_{i=1}^d {(x_i -
    x_{(j,i)})}^2 }
  \\ \Updownarrow \\
  \label{eq:4}
  \min_{(P)} F(P) \quad \text{for} \quad
  F(P) = \sum_{j=1}^3 m_j | P P_j |, \quad P \in R^d
\end{gather}
%
Where $|P P_j|$ is the Euclidean distance between $P$ and $P_j$.
The point $P_\ast$ now no longer has to be a solution to the
equation system in \cref{eq:13}, but to a similar system containing $d$
equations, as follows
%
\begin{equation}
\label{eq:7}
  \left\{
    \begin{array}{c}
    \sum_{j=1}^3 \frac{m_j(x_1-x_{(j,1)})}{| P P_j|} = 0
    \\
    \sum_{j=1}^3 \frac{m_j(x_2-x_{(j,2)})}{| P P_j|} = 0
    \\
    \vdots
    \\
    \sum_{j=1}^3 \frac{m_j(x_d-x_{(j,d)})}{| P P_j|} = 0
  \end{array}
  \right.
\end{equation}
%
Then as before, if the conditions in \cref{eq:12} are fulfilled\footnote{If not
  the point is as before located at the corresponding vertex.} the solution to
\cref{eq:7} can be found as follows
%
\begin{gather}
  P_\ast = (x_{(\ast,1)}, x_{(\ast,2)}, \ldots, x_{(\ast,d)})\label{eq:8}
  \intertext{with}
  x_{(\ast, j)} = \frac{K_1 K_2 K_3}{4 S \sigma d} \left( \frac{x_{(1,j)}}{K_1} +
  \frac{x_{(2,j)}}{K_2} + \frac{x_{(3,j)}}{K_3} \right), \quad 0 < j \le d\label{eq:5}
  \intertext{and}
  F(P_\ast) = \min_{(P)} F(P) = \sqrt{d}\label{eq:21}
  \intertext{Here}
  \label{eq:14} r_{jl} = \sqrt{\sum_{k = 1}^d {(x_{(j,k)} - x_{(l,k)})}^2} = |P_j P_l|
  \quad \{j,l\} \in \{ 1, 2, 3 \} \\
  S = \frac{1}{2} \sqrt{(r_{12} + r_{23} + r_{13}) (r_{23} + r_{13}) (r_{12} +
    r_{13}) (r_{12} + r_{23})} \\
\end{gather}
%
And the rest of the expressions the same as in the 2D version.  As can be seen,
all of the expressions, except for $S$ (and the distance which now of course
have more than two dimensions), retain the same representation as in the
two dimensional version. $S$ however looks quite different. \TODO{Explain how
  these two S are the same, and maybe give Pawel credit for this one?}

As before to prove \cref{eq:5} (and thus \cref{eq:8}) the first step once again
is to prove \cref{eq:16} and \cref{eq:2}.  These can as before be established
through direct computation, i.e.\ by simply substituting the expressions for
their definitions, until both sides only contain weights or distances, after
which the two sides can be seen to be equal. Using \cref{eq:16} we can rewrite
\cref{eq:5} as
%
\begin{gather}
    x_{(\ast, j)} = \frac{1}{\frac{1}{K_1} + \frac{1}{K_2} + \frac{1}{K_3}} \left( \frac{x_{(1,j)}}{K_1} +
  \frac{x_{(2,j)}}{K_2} + \frac{x_{(3,j)}}{K_3} \right), \quad 0 < j \le
d\label{eq:10}
\intertext{similarly to \cref{eq:17} in the 2D version. Secondly we wish
  to prove that}
  \sqrt{\sum_{i = 1}^d {(x_{(\ast,i)} - x_{j,i})}^2} = \frac{m_j K_j}{2 \sigma
    \sqrt{d}}, \quad j \in \{ 1, 2, 3 \}\label{eq:19}
\end{gather}
As the calculations are similar for all $j$, only the one for $j = 1$ is shown
here. Thus
%
\begin{align}
  \sum_{i=1}^d {(x_{\ast,i} - x_{1,i})}^2
  &= \sum_{i=1}^d { \left( \frac{K_1 K_2 K_3}{4 S \sigma d}
    \left( \frac{x_{(1,i)}}{K_1} + \frac{x_{(2,i)}}{K_2} + \frac{x_{(3,i)}}{K_3}
    \right) - x_{(1,i)} \right) }^2 \\
  &= { \left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right) }^2 \left[ \sum_{i=1}^d
    { \left( \frac{x_{(1,i)}}{K_1} + \frac{x_{(2,i)}}{K_2} +
    \frac{x_{(3,i)}}{K_3} - \frac{x_{(1,i)} 2 S \sigma d}{K_1 K_2 K_3}
    \right) }^2 \right] \\
  &\stackrel{\mathclap{(\ref{eq:16})}}{=}
    {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right)}^2 \left[ \sum_{i=1}^d
    {\left( \frac{x_{(2,i)}}{K_2} + \frac{x_{(3,i)}}{K_3} - \frac{x_{(1,i)}}{K_2} -
    \frac{x_{(1,i)}}{K_3} \right)}^2 \right] \\
  &= {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right)}^2 \left[ \sum_{i=1}^d
    \left( \frac{{(x_{(2,i)} - x_{(1,i)})}^2}{K_2^2} +
    \frac{{(x_{(3,i)} - x_{(1,i)})}^2}{K_3^2} \right. \right. \\
  &\hspace{7em} \left. \left. + \; 2 \frac{(x_{(2,i)} - x_{(1,i)})(x_{(3,i)} -
    x_{(1,i)})}{K_2 K_3} \right) \right] \\
  &= {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right)}^2
    \left[ \frac{\sum_{i=1}^d {(x_{(2,i)} - x_{(1,i)})}^2}{K_2^2} +
    \frac{\sum_{i=1}^d {(x_{(3,i)} - x_{(1,i)})}^2}{K_3^2} \right. \\
  &+ \left. \frac{ \sum_{i=1}^d {(x_{(2,i)} - x_{(1,i)})}^2 +
    \sum_{i=1}^d {(x_{(3,i)} - x_{(1,i)})}^2 -
    \sum_{i=0}^d {(x_{(2,i)} - x_{(3,i)})}^2}{K_2 K_3} \right] \\
  &\stackrel{\mathclap{(\ref{eq:14})}}{=}
    {\left( \frac{K_1 K_2 K_3}{4 S \sigma d} \right)}^2
    \left[ \frac{r_{12}^2}{K_2^2} + \frac{r_{13}^2}{K_3^2} +
    \frac{r_{12}^2 + r_{13}^2 - r_{23}^2}{K_2 K_3} \right] \\
  &= \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ r_{12}^2 K_3^2 + r_{13}^2 K_2^2 +
    (r_{12}^2 + r_{13}^2 - r_{23}^2) K_2 K_3 \right] \\
  &= \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ (r_{12}^2 K_3 + r_{13}^2 K_2) (K_2 + K_3) -
    r_{23}^2 K_2 K_3 \right] \\
  &\stackrel{\mathclap{(\ref{eq:2})}}{=}
    \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ (2 S d - r_{23}^2 K_1) (K_2 + K_3) - r_{23}^2 K_2 K_3 \right] \\
  &= \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ 2 S d (K_2 + K_3) - r_{23}^2 (K_1 K_2 + K_1 K_3 + K_2 K_3) \right] \\
  &\stackrel{\mathclap{(\ref{eq:16})}}{=}
    \frac{K_1^2}{{(4 S \sigma d)}^2}
    \left[ 2 S d (K_2 + K_3) - 4 r_{23}^2 \sigma S d \right] \\
  &= \frac{2 S d K_1^2}{{(4 S \sigma d)}^2}
    \left[ K_2 + K_3 - 2 r_{23}^2 \sigma \right] \\
  &\stackrel{\mathclap{(\ref{eq:3})}}{=}
    \frac{K_1^2}{8 S d \sigma^2}
    \left[ 2 m_1^2 S \right] \\
  &= \frac{m_1^2 K_1^2}{4 \sigma^2 d}
\end{align}
%
From here we can that \cref{eq:5} is a solution to \cref{eq:7} in the exact same way as
we proved that \cref{eq:15} was a solution to \cref{eq:13}, by substituting
\cref{eq:5} into \cref{eq:7} and using \cref{eq:10} and \cref{eq:19}. The same
goes for proving \cref{eq:21}.

\section{Sorting points}
\label{sec:sorting-points}

\TODO[inline]{longest path in fully connected graph is NP-complete [ref]. Greedy approach
shows promising results. See also Smith+ in \cite{Fampa2008}}

\chapterbreak{}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
