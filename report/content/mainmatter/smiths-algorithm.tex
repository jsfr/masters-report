{ \abnormalparskip{0pt}
\chapter{\citeauthor{smith1992}'s algorithm}
\label{cha:algorithm} }

% Short introduction to the chapter (max 1/2 page)

The following chapter will introduce the algorithm for finding \acp{smt} in
$d$-space, as proposed by \textcite{smith1992}.

The chapter will mainly be concerned with the design of the algorithm, and the
proofs and theorems surrounding it. It will however also delve into the actual
implementation done by \citeauthor{smith1992} when this is relevant.

Finally the chapter discuss some more dubious parts of the implementation done
by \citeauthor{smith1992}.

Apart from the already used $n$ and $k$, for clarity we define the following symbols
%
\begin{itemize}
\item $R$, the set of terminals.
\item $S$, the set of Steiner points.
\item $\mathcal{T} \equiv E(T)$, the pre-specified topology. Defined as all
  edges which the tree $T$ contains in the form $(i, j)$ where $p_i$ and
  $p_j$ are contained in either $S$ or $R$ and an edge exists in the
  topology from $p_i$ to $p_j$ or vice versa.\footnote{Notice that in
    \cref{cha:preliminaries} we mainly represented and edge between the points
    $p_a$ and $p_b$ as $ab$ as this is really simple. However this notation is not
    very good if e.g.\ one point is labelled $l+1$. Thus we adopt this extra
    notation as well, and may use these interchangebly.}
\item $L$, the length of the tree. Defined as the sum of the euclidean lengths
of all edges in the tree.
\end{itemize}

\section{Overview}
\label{sec:overview}

In general the algorithm follows the following form, proposed by \textcite{gilbert1968}.

\begin{enumerate}
\item Enumerate all Steiner topologies on $n$ terminals and $k$, $0 \le k \le
n-2$ Steiner points.
\item Optimize the coordinates of the Steiner points for each topology, to find
the shortest possible Euclidean embedding (tree) of that topology.
\item Select, and output, the shortest tree found.
\end{enumerate}

In other words this approach might be seen as an exhaustive search of the
solution space, and the approach described is simply to ``try all
possibilities''.

\citeauthor{smith1992} however avoids necessarily having to enumerate and optimize all
topologies, by doing the following: Firstly he only looks at \acp{fst} as it
turns out we can regard any topology which is non-full, as a \ac{fst} where
some points overlap.

Secondly the algorithm builds the topologies, by a branching process where it
first builds the topology for $3$ fixed terminals (and $1$ Steiner point). From
this it then build all descendants having $4$ terminals (and $2$ Steiner
points). Then again from each of these it builds the descendants having $5$
terminals (and $3$ Steiner points), etc.\, until it has built all descendants
having $n$ terminals (and $n-2$ Steiner points). The idea behind this way of
building the \acp{fst} is to allow a branch-and-bound approach where we wish to
prune all descendants of a topology if we have an upper bound with a length,
lower than the length we get when optimizing the tree of the underlying
topology.  This process will be described more thoroughly in
\cref{sec:generation}.

The optimization of a tree is done by an iterative process which updates all
Steiner points of the tree every iteration, which \citeauthor{smith1992}
describes as an iteration ``analogous to a Gauss-Seidel
iteration''~\cite[p.~145]{smith1992}. The equations generated by each iteration
are solved using Gaussian elimination. This iteration and how to solve it is
described in \cref{sec:optim-presp-topol}.

\section{Topologies}
\label{sec:topologies}

The first step of the algorithm is to generate topologies. It is therefore
natural that we need some way of representing and generating these topologies.

The algorithm only considers \acp{fst}, where $k = n - 2$. This simplification
is allowed, as we can simply regard any Steiner tree with $k \le n - 2$ as a
\ac{fst} where some edges have length zero and thus some points have
``merged''.

An incentive for only looking at \acp{fst} can be found in \textcite{gilbert1968}
which presents a table, here presented in \cref{tab:number-of-topologies},
containing the number of possible Steiner topologies for different numbers of
terminals and Steiner points. As can clearly be seen the number of \acp{fst}
(the diagonal) is a lot smaller than the total number of topologies. Thus not
having to optimize the non-full topologies is clearly desirable.

\begin{table}[htbp]
  \centering
  \begin{tabular}{cccccc}
    \toprule
    $k \backslash n$ & 3    & 4    & 5     & 6      & 7      \\
    \hhline{~-----}
    0   & 3          & 12         & 60          & 360          & 2520         \\
    1   & \textbf{1} & 12         & 120         & 1200         & 12600        \\
    2   &            & \textbf{3} & 75          & 1350         & 22050        \\
    3   &            &            & \textbf{15} & 630          & 17640        \\
    4   &            &            &             & \textbf{105} & 6615         \\
    5   &            &            &             &              & \textbf{945} \\
    \bottomrule
  \end{tabular}
  \caption[Number of possible topologies]{The number of possible topologies for a Steiner tree with $n$
    terminals and $k$ Steiner points.\label{tab:number-of-topologies}}
\end{table}

Note however that even with this simplification the number of \acp{fst} is
still exponential in $N$, which is clear from \cref{cor:number-of-fsts}.

\subsection{Representation}
\label{sec:representation}

It turns out that every \ac{fst} can be represented using a vector, in
particular we utilize the following theorem put forth by \textcite{smith1992}

\begin{theorem}
  There is an 1--1 correspondence between \acp{fst} with $n \ge 3$ terminals,
  and $(n-3)$-vectors $\vec{a}$, whose $i$th entry $a_i$ is an integer between
  $1 \le a_i \le 2 i + 1$.
\end{theorem}

Here terminals have indicies $1, 2, \ldots, n$, and Steiner points have indicies
$n + 1, n + 2, \ldots, 2 n - 2$.

\begin{proof}
The proof of this theorem is done constructively by induction on $n$. It is
clear that the smallest \ac{fst}\footnote{This holds not only for \acp{fst}
  but for any Steiner topology} we can construct, must have $n = 3$ as the
number of Steiner points is $n - 2 = 1$. Thus we start with the initial null
vector $\vec{a} = ()$ corresponding to the unique \ac{fst} for the terminals
$\{1, 2, 3\}$ connected through the respective edges $\{1, 2, 3\}$ and one
Steiner point $n+1$ as seen in \cref{fig:algorithm-topology-1}. After this
first step, each entry of the topology vector is considered, one at a time,
where the $i$th entry of the topology vector describes the insertion of the
$(n+1+i)$th Steiner point on the edge $a_{i}$ and its connection to the
$(i+3)$th terminal. Thus for the $i$th insertion we will have
$2i+1$\footnote{At the first iteration we clearly have $3$ edges to insert on, and
  as each subsequent insertion generates two new edges we have, after $i$
  insertions, $3 + \underbrace{2 + \cdots + 2}_{2 i} = 2 i + 1$.} different
edges on which we can insert the Steiner point $n + 1 + i$ and connect it to the
regular point $i+3$.
\end{proof}

\begin{figure}[htbp] \centering
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-1}
    \caption{The initial null vector.\label{fig:algorithm-topology-1}}
  \end{subfigure}\hspace{1em}%
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-2}
    \caption{Connecting point 4 on edge 2.\label{fig:algorithm-topology-2}}
  \end{subfigure}\hspace{1em}%
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-3}
    \caption{Connecting point 5 on edge 4.\label{fig:algorithm-topology-3}}
  \end{subfigure}
  \caption[Construction of FSTs]{Construction of the \acp{fst} corresponding
to the vector $\vec{a} = (2, 4)$.\label{fig:algorithm-topologies}}
\end{figure}

Furthermore we get a corollary saying that the number of \acp{fst} is
exponential in $n$

\begin{corollary}
\label{cor:number-of-fsts}
The number of \acp{fst} on $n$ terminals is
%
\[
  1 \cdot 3 \cdot 5 \cdots (2n - 7) \cdot (2n - 5) = \prod_{i=0}^{n-3} 2i+1
\]
%
I.e.\ the number of \acp{fst} is exponential in $n$.
\end{corollary}

Which is clear as we must insert $n-2$ Steiner points, where the null vector is
the $0$th iteration. Thus the last iteration must be $n-3$, and for each
iteration we have $2i+1$ different insertions.

Something which might not be completely obvious from the above proof is, that we
not only generate all topologies, but that we generate all topologies exactly
once. This is important as duplicate topologies could significantly
increase the run time of the algorithm.

\FIXME[inline]{Prove the above statement. I.e.\ that we generate every FST and
  that we cannot generate the same topology twice. This seems to be possible
  using \textcite[p.~11--13]{brazil2015}.}

The way \citeauthor{smith1992} chooses to enumerate the edges is not explained
outright, but only in the form of a visual
example~\cite[p.~143]{smith1992}. Exactly how one enumerates the edges on a
split is actually not so important, more so is it important to keep it
consistent. The way \citeauthor{smith1992} does this, is as in
\cref{fig:algorithm-topologies}. That is when inserting Steiner point
$p_{n+1+i}$ on the edge $e_{a_i}$\footnote{Here $a_i$ is the $i$-th entry in the
  topology vector.} going from point $p_{a_i}$ to point $p_j$ with
$n < j < n+i+1$, split it such that we get the following three
edges\footnote{Remember that an edge is represented as $(i,j)$ meaning that it
  runs from point $p_i$ to point $p_j$.}: $e_{a_{i}} = (a_{i}, N+1+i)$,
$ e_{2i + 2} = (i+3,N+1+i)$ and $ e_{2i + 3} = (j, N+1+i)$. This is also the way
the new implementation enumerates the edge.

\subsection{Generation}
\label{sec:generation}

Using the representation described in \cref{sec:representation} the problem of
generating all topologies can now be done as a backtracking problem generating
all $(n-3)$-topology vectors. An example of how the generation works can be seen
in \cref{fig:topology-sprouting}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.8\textwidth]{gfx/tikz/topology-sprouting}
  \caption[Splitting of an edge]{Upon splitting an edge and connecting a new
    terminal to the topology the algorithm can perform the split on any of the
    existing edges. Thus in this case the algorithm branches into three
    different topologies. In general the splitting a topology results in the topology
    branching into as many new topologies as there are edges in the topology on which
    the split is perfomed.\label{fig:topology-sprouting}}
\end{figure}

To further speed up the generation of topologies, or rather to avoid generating
unnecessary topologies, \citeauthor{smith1992} also utilizes the following theorem
%
\begin{theorem}
  For any set of $n$ distinct terminals in any euclidean space, the length of
  the shortest tree, interconnecting $n-1$ points, with topology vector
  $a_1 \cdots a_{n-4}$ is no greater than the length of the shortest tree,
  interconnecting $n$ points, with topology vector $a_1 \cdots a_{n-3}$.
\end{theorem}
%
The above theorem is easily seen to be true, simply by considering removing the
edge $e$ connecting the last terminal $n$ to the rest of the tree. This will
obviously shorten the tree, or if the edge was length zero it would remain the
same. Furthermore upon optimizing the tree with the last point removed, we
would either get a further shortening, or it would remain the same.

\NOTE[inline]{Consider expanding the above proof with the following type of
  proof: delete the edge incident with the terminal n. Replace the Steiner point
  prevoiusly adjacent to n and its two edges by a straight line connecting the
  other endpoints of these two edges. This tree is not longer than the one
  before n was remove and it has the topology of the tree with n-1
  terminals. Hance it cannot be shorter than the RMT with this topology.}

The algorithm utilizes this to prune in the following way. Imagine we have
found some upper bound for the \ac{smt}. If we then optimize any tree with a generated
topology vector which does not yet include all the terminals, and it turns out
to have length greater than the upper bound, we can prune all topologies that
would be this vector. This is possible as the length of the larger trees
cannot become any smaller than the length of the current, and thus cannot become
smaller than the length of the upper bound.

In general we need to generate topologies and optimize their respective trees
depth-first. The reason for this is two-fold: Firstly going breadth-first would
mean that we generate all \acp{fst} as the very last topologies to be
generated. Thus we would not get an upper bound before this stage, and would
therefore be unable to prune any significant branches, as all that is left to
optimize at this point are leaves\footnote{I.e.\ the \acp{fst}.}. We could of
course find some upper bound using a heuristic, e.g.\ a Minimum Spanning
Tree. This would possibly allow us to prune some branches, but would not change
that fact that we would not be able to benefit from possibly updating the upper
bound if we found a \ac{fst} with a smaller length. Thus going depth-first is
advantageous as it allows us to not only get an upper bound without the need of
a heuristic, but also allows us to update the upper bound and use this, should
we find a new better one.

Secondly is also the concern of memory usage of the actual implementation. If we
were to perform the algorithm breadth-first it would mean keeping track of
earlier topologies, which would drastically increase memory usage, in contrast
to depth-first we one only needs to remember the current way down.

The actual implementation of the backtracking is not described by the article
part of \textcite{smith1992}, but only by the source code in the appendix. The
details of the new implementation will be discussed in
\cref{cha:implementation}. The original implementation faces some problems,
one of which is the fact that the entire topology is rebuilt from scratch every
time we need to either add of remove a point. These will be discussed in
\cref{sec:implementation}.

\section{Optimization of a prespecified topology}
\label{sec:optim-presp-topol}

When a topology has been found, we need some way of optimizing the positions of the
Steiner points. The approach taken by \citeauthor{smith1992} is in general to create an
iteration with $n-2$ equations, one for each Steiner point, and incrementally
optimize all of them every iteration. This is done by solving the equations
using Gauss elimination. The section will be concerned with describing the
iteration, the proof of correctness and convergence, the speed of convergence
and the error function for deciding convergence.

\subsection{Simple iteration}
\label{sec:simple-iteration}

To first get a sense of how we might go about optimizing the tree in a simple
manner we first look at a iteration which only optimizes one Steiner point at a
time. This is a simpler approach to the one proposed by \textcite{smith1992},
but according to \citeauthor{smith1992} will probably also take longer time to
converge. The iteration is described by \cref{alg:simple-iteration}. The
algorithm is intentionally a bit vague, as even the simple iteration has several
choices we can make.

\begin{algorithm}[htbp]
  \KwData{
    \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,label=-,leftmargin=10pt]
    \item A \ac{fst} containing the Steiner points $S$, terminals $R$, and edges $E$
    \item A small positive number $\epsilon$
    \end{itemize}
  }
  \KwResult{The optimized \ac{fst}}
  \ForEach{$s \in S$}{
    $\textit{error} \gets \infty$\;
    \While{$\textit{error} > \epsilon$}{
      optimize $s$ with respect to its neighbors in $T$\;
      recalculate \textit{error}\;
    }
  }
  \Return{$T$}
  \caption[Simple iteration]{Pseudo code describing the optimization strategy
    using the simple iteration. The algorithm describes running a single
    optimization. In general one would have to run the algorithm several times,
    until the error of the entire tree falls below some threshold.\label{alg:simple-iteration}}
\end{algorithm}

First we should choose how a single Steiner point is optimized. As earlier
described this is the same as the Fermat-Torricelli problem, and thus we can go
about finding the Steiner point in all the same ways. Secondly one needs to
decide some way of calculating the error of the current Steiner point. This
could e.g.\ be done as in \textcite{smith1992}\footnote{Note that we would then
  not sum the error as is done by \citeauthor{smith1992}, but as we only need
  the error of the current Steiner point, and not the entire tree.}.  Notice
that if we use a method such as e.g.\ the analytical one described in
\cref{sec:analyt-solut-ferm} the inner loop is redundant, as the error after
placing the Steiner point will always be zero.

The problem \citeauthor{smith1992} poses for the simple iteration described here is that while it
quickly becomes optimal \textit{locally} it may still converge slowly as it does not
regard the \textit{global} structure. This statement is however not qualified
any further in the article.

As an iteration, using an analytical solution to the Fermat-Torricelli problem
described by \textcite{uteshev2014}, has actually been implemented the above
statement will be further discussed in \cref{cha:experiments} and \cref{cha:discussion}.

\subsection{\citeauthor{smith1992}'s iteration}
\label{sec:smiths-iteration}

Now that we have looked at a simple way of optimizing the topology, by finding
the coordinates of one Steiner point at a time, we can progress to the iteration
proposed by \textcite{smith1992}. This iteration optimizes all points in every
iteration.

To optimize the topology \citeauthor{smith1992} proposes the following

\begin{theorem}
  As the $i$th step in an iterative process, $i= 0, 1, \ldots$, solve the $n-2$
  equations\footnote{Note that in the second sum of \cref{eq:6}
    \textcite{smith1992} gives the index $j: jk \in T, j \in S$. For the rewrite
    to make sense, this should one have been $j: jk \in T$. Thus I believe this
    is just a typo and have progressed as such.}\footnote{Note that since the
    unknowns in the iteration are vectors we actually have $d$ independent
    equation systems of $n-2$ equations.}
%
\begin{alignat}{2}
  p_l^{(i+1)}
  &= \frac{\sum_{j : (j,l) \in \mathcal{T}}
    \frac{p_j^{(i+1)}}{|p_l^{(i)} - p_j^{(i)}|}}
  {\sum_{j: (j,l) \in \mathcal{T}} \frac{1}{|p_l^{(i)} - p_j^{(i)}|}},
  & \quad n + 1
  &\le l \le 2 n - 2 \label{eq:6}
  \intertext{which can be rewritten as follows, %
    to show that forces on $p_l$ is in equilibrium}
  \vec 0
  &= \sum_{j: (j,l) \in \mathcal{T}} \frac{p_l^{(i+1)} - p_j^{(i+1)}}
  {|p_l^{(i)} - p_j^{(i)}|},
  & \quad n + 1
  &\le l \le 2 n - 2 \label{eq:9}
\end{alignat}
%
for the unknowns $p_l^{(i+1)}$ where $p_l \in S$ represents the $\mathit{i}$th
Steiner point, being a $\mathit{d}$-vector. Then from all initial choices of
Steiner point coordinates $p_l^{(0)}$, except for a set of measure zero in
$\mathbb{R}^{(n-2)d}$, the iteration converges to the unique optimum Steiner
point coordinates\footnote{Note that if we have an edge of length zero, the
  iteration is not actually defined in the way we have written it, as we get a
  division by zero. \textcite[p.~147--148]{smith1992} argues as to why this is
  not a problem, and how to resolve such a situation. This argument will be
  discussed later in this section. Thus the proof of convergence, for now,
  assumes this situation does not happen.} that minimize the total tree length
$L$. This convergence happens in such a way that the sequence of tree lengths
$L(S^{(i)})$ is monotonically decreasing.
\end{theorem}

Before discussing the convergence we define the length $L$ as is normally the
case, calculated as the sum of all edge lengths. In this case we use the regular
Euclidean ($\mathcal{L}_2$) norm. One could however think to possibly use other
norms, e.g.\ the rectilinear ($\mathcal{L}_1$) norm. Thus

\begin{equation}
  \label{eq:23}
  L(S) = \sum_{j,l : (j,l) \in \mathcal{T}} | p_l - p_j |
\end{equation}

Note that \textcite{smith1992} defines $L$ as a function of the set of
Steiner points $S$ instead of the tree $T$. Here we follow this
convention, as we need the function when we argue about the iteration, in which
only the Steiner points an move and the terminals remain fixed. Also remember
that $\mathcal{T} \equiv E(T)$.

\begin{proof}
The proof of convergence is as follows: After the first iteration all Steiner
points lie within the convex hull of the terminals\NOTE{Maybe describe how this
can be seen?}. By the Bolzano-Weierstrass theorem any infinite  sequence of
points inside a compact region (such as a convex hull) has some infinite
subsequence approaching a limit point. We therefore wish to show that the only
limit point which can exist, represents the \ac{rmt} for the underlying topology.

Firstly after each ($i$th) iteration, according to \textcite{smith1992},
$S^{(i+1)}$ exactly minimizes the following quadratic form $Q^{(i)}(S)$
%
\begin{equation}
  Q^{(i)}(S) = \sum_{j,l : (j,l) \in \mathcal{T}, l \in S, j < l }
  \frac{|p_l - p_j|^2}{|p^{(i)}_l - p^{(i)}_j|}
\end{equation}
%
Note that the two last conditions on the sum are redundant, are as far as I can
tell are only given by \citeauthor{smith1992} to emphasize the way in which
trees are split and edges are numbered. It is easily seen that $Q(S)$ is related to
$L(S)$ in the following way
%
\begin{align}
  Q^{(i)}(S^{(i)})
  &= \sum_{(j,l) \in \mathcal{T}}
    \frac{|p^{(i)}_l - p^{(i)}_j|^2}{|p^{(i)}_l - p^{(i)}_j|} \\
  &= \sum_{(j,l) \in \mathcal{T}} |p^{(i)}_l - p^{(i)}_j| \\
  &= L(S^{(i)}) \label{eq:24}
\end{align}
%
Smith describes that, using \textcite{gilbert1968}'s mechanical model one can
think of $Q$ as the potential energy of a system of ideal springs on the tree
edges, where the force constant of each spring is proportional to the reciprocal
of its original length before each iteration. The $i$th iteration causes all
springs to relax, minimizing $Q^{(i)}$. It is however still a unclear to me that
$S^{(i+1)}$ really minimize $Q^{(i)}$.

\NOTE[inline]{Do you know why the above is the case?}

However as the proof of convergence hinges on this fact\footnote{As we need to
  utilize that $Q^{(i)}(S^{(i)}) \ge Q^{(i)}(S^{(i+1)})$} we can only proceed if
this is the case. Thus I here assume that this is correct. We then do as follows
%
\begin{align}
  L(S^{(i)}) \; &\refequal{eq:24} \; Q^{(i)}(S^{(i)}) \\
                &\ge Q^{(i)}(S^{(i+1)}) \\
                &= \sum_{j, l : (j,l) \in \mathcal{T}}
                  \frac{{\lenpljii}^2}
                  {\lenplji} \\
  \intertext{using that we can add and subtract the same thing without changing
  the equation, we do}
                &= \sum_{j, l : (j,l) \in \mathcal{T}}
                  \frac{{(\lenpljii + \lenplji - \lenplji)}^2}
                  {|p_l^{(i)} - p_j^{(i)}|} \label{eq:25}
\end{align}
%
We then write out and reorder the numerator of the fraction in \cref{eq:25},
here name $\textit{num}$. To further simplify the equation, as it does otherwise
get quite hairy, we define $a = \lenpljii$ and $b = \lenplji$. Thus
%
\begin{align}
  \label{eq:26}
  \textit{num}
  &= {(\lenpljii + \lenplji - \lenplji)}^2 \\
  &= {(a + b - b)}^2 \\
  &= (a^2 + ab - ab) + (ba + b^2 - b^2) + (-ba - b^2 + b^2) \\
  &= (a^2 + b^2 - ab - ab) + (b^2 - b^2 - b^2) + (ab + ba) \\
  &= 2ab - b^2 + {(a - b)}^2
    \intertext{then dividing the numerator again with the denominator from
    \cref{eq:25} $\textit{denom} = \lenplji = b$ we get}
    \frac{\textit{num}}{\textit{denom}}
  &= \frac{2ab - b^2 + {(a - b)}^2}{b} = 2a - b + \frac{{(a-b)}^2}{b} \label{eq:27}
\end{align}
%
Finally we can go back to \cref{eq:25} and insert the fraction we have in
\cref{eq:27}. Thus
%
\begin{align}
  ~(\ref{eq:25})
  &= \sum_{j, l : (j,l) \in \mathcal{T}}
    \left[ 2a - b + \frac{{(a-b)}^2}{b} \right] \\
  &= 2 \sum_{j, l : (j,l) \in \mathcal{T}} \lenpljii -
    \sum_{j, l : (j,l) \in \mathcal{T}} \lenplji \\
  &\quad + \sum_{j, l : (j,l) \in \mathcal{T}}
    \frac{{(\lenpljii - \lenplji)}^2}{\lenplji} \\
  &\refequal{eq:23} \; 2 L(S^{(i+1)}) - L(S^{(i)}) \\
  &\quad + \sum_{j, l : (j,l) \in \mathcal{T}}
    \frac{{(\lenpljii - \lenplji)}^2}{\lenplji} \Leftrightarrow \\
  2 L(S^{(i)}) &\ge 2 L(S^{(i+1)}) \\
  &\quad + \sum_{j, l : (j,l) \in \mathcal{T}}
    \frac{{(\lenpljii - \lenplji)}^2}{\lenplji}
\end{align}
%
As the last sum is obviously non-negative\footnote{The numerator is squared and
  thus non-negative, and the denominator is a length and thus non-negative.}, we
can remove it without the validity of the equation changing, and thus
%
\begin{equation}
  L(S^{(i)}) \ge L(S^{(i+1)})
\end{equation}
%
We now know that performing an iteration, will always either decrease the length
of the tree, or it will remain the same. The only way that it can remain the
same, i.e.\ that we can have equality $L(S^{(i)}) = L(S^{(i+1)})$ is if we are
at a fixed point of the iteration. There are only two types of fixed
points---the optimum\footnote{Which is the \ac{rmt} for this topology.} and
certain places where one or more edges have length zero. As earlier described
the iteration as we have written it is not strictly defined, due to a division
with zero. However according to \citeauthor{smith1992} the singularity is
removable\footnote{How this is the case is unclear and by unexplained by
  \citeauthor{smith1992}.}~\cite{removablesingularity} and thus we remove it.

The next part of the proof again hinges on a postulate by \citeauthor{smith1992}
which is not clearly true to me---that the non-optimum fixed points are unstable
in any direction.

Firstly \citeauthor{smith1992} claims that the non-optimum fixed points are
unstable in any $L$-decreasing direction, and thus a small perturbation of the
Steiner points $S$ in any such direction will cause the iteration to
continue. This makes sense as we know that $L$ can never \textit{increase} when
we run the iteration. Thus after decreasing $L$ by a small perturbation, either
we will be past the fixed point and the iteration will continue to decrease, or
we hit another fixed point\footnote{at which point we then again will have to
  use a small perturbation.}. \citeauthor{smith1992} then refers to
\textcite{gilbert1968} who have pointed out that optimizing a pre-specified
Steiner topology is a ``strictly convex'' optimization problem, i.e.\ the only
local optimum is global. This means that there will always be a $L$-decreasing
direction if we are not yet at the optimum.

This currently means we need to find a $L$-decreasing perturbation for
the iteration to continue its convergence. \citeauthor{smith1992} however
further argues that any small step in the opposite direction of a $L$-decreasing
one would also cause the iteration to resume decreasing $L$. The argument for
this is based on the physical spring model of Steiner trees, introduced by
\textcite{gilbert1968}. I have unfortunately not been able to follow the
argument \textcite{smith1992} gives, and after much discussion with my
supervisor Pawel Winter it is still unclear to both of us whether this argument
actually holds.

If we assume that the argument holds, then the iteration, when it is at a
non-optimum fixed point, is unstable in every direction. Thus we have a
$0$-measure\footnote{as we can disregard all these points because of their
  instability.} set of initial iterates which end up at the non-optimal fixed
points. Finally we can the conclude that any Bolzano-Weierstrass subsequence
limit tree must be a fixed point of the iteration. This is done by continuity of
$L$, the iteration function and since the only points where the iteration does
not decrease $L$ are fixed points. However as all non-optimum fixed points are
in the 0-measure set, we have ruled those out and thus the only fixed point is
the optimum.
\end{proof}

\NOTE[inline]{Possibly discuss the unclear parts which i have just assumed to be
true to keep the proof going.}

\subsubsection{Solving the equations}
\label{sec:solving-equations}

The equation system of each iteration can be solved using Gaussian elimination
as described by \textcite[p.~148--149]{smith1992}. Roughly the Gaussian
elimination described and implemented by \citeauthor{smith1992}\footnote{and
  also the new implementation.} is done by identifying an equation which only
has at most one unknown on the right hand side. This corresponds to a Steiner
point being connected to two or more terminals, which is always guaranteed to
exist. Working from this point we can then eliminate a variable by substitution
in the other equations, which will result in at least one equation having only
one unknown on the right hand side. In the end we just do backward substitution.
Doing the elimination from this specific point, instead of just some random
point means that we can a perform all elimination in $\mathcal O(N)$. Since we
$d$ linear systems we can then solve them all in $\mathcal{O}(n d)$. The details
can be found in~\cite[p.~148--149]{smith1992}.

\subsubsection{Speed of convergence}
\label{sec:speed-convergence}

According to \textcite[p.~150]{smith1992} the iteration has a good speed of
convergence. In most cases geometric convergence will occur. However some
degenerate configurations of terminals can cause sublinear convergence.

\section{Questionable elements of \citeauthor{smith1992}'s article}
\label{sec:quest-elem-smiths}

There are a number of elements in the article and the given
implementation~\cite{smith1992} which are somewhat questionable. These I will
try to discuss here.

\subsection{Choice of error function}
\label{sec:choice-error-funct}

To detect when convergence has happened \citeauthor{smith1992} proposes using a special
error function instead of the one often used where one stops when the
improvements from iteration to iteration falls below some $\epsilon$. The
function proposed by \citeauthor{smith1992} is
%
\begin{equation}
  E^2 = \sum_{
    \begin{array}{c} i \in S \\ (i,j) \in \mathcal{T} \\ (i,l) \in \mathcal{T} \\ j \ne l
    \end{array}} \text{pos} (2 (\vec x_j - \vec x_i) \cdot (\vec x_l - \vec x_i)
+ | \vec x_j - \vec x_i | \cdot | \vec x_l - \vec x_i |)
\end{equation}
%
Here $\text{pos}(x) = \max(x, 0)$. This error function sums together all angles which
are smaller than $120^{\circ}$. The error function is derived from calculating
the angle between edges in the following way:
%
\begin{align}
  \cos v & = \frac{\vec a \cdot \vec b}
    {|\vec a| \cdot |\vec b|}                 \\
  \intertext{If we then wish to find the cosine for $120^{\circ}$}
  \cos 120^{\circ}
         & = -\frac{1}{2} = \frac{\vec a \cdot \vec b}
    {|\vec a| \cdot |\vec b|} \Leftrightarrow \\
  0      & = \vec a \cdot \vec b + \frac{1}{2}
    (|\vec a| \cdot |\vec b|) \Leftrightarrow \\
  0      & = 2 (\vec a \cdot \vec b) + |\vec a| \cdot |\vec b|
\end{align}
%
Thus any angle less than $120^{\circ}$ gives a positive number when inserted in
the function, and any angle below will give a negative number (which is set to
zero by $\text{pos}(x)$).

Why this error function is supposed to be better is not explained very
thoroughly by \citeauthor{smith1992}. It is only referred to that doing some
experimentation which led him to believe that this would be a good function. The
idea behind the function seems to be that, as the final tree should only have
angles of $120^{\circ}$ any angle less than this must still need to be
optimized.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{gfx/tikz/errorfunction}
  \caption[Possible problem with the error function]{The left part of the figure
    shows how the use of the current error function might result in a suboptimal
    tree. The right part of the figure shows the optimal tree. Notice that
    this topology does not result in the \ac{smt} for these
    terminals.\label{fig:error-function}}
\end{figure}

There are however some problems with this choice of error function. Consider
the configuration as shown in the first tree of \cref{fig:error-function}. Here
there are two pairs of terminals, very far apart. These are connected by two
Steiner points as seen. Running \citeauthor{smith1992}'s iteration the two Steiner points will
move together, and up towards the center of the rectangle of the four
terminals. However suppose the two points reach each other before they reach
the center. In this case the angles on the left and right will be more than
$120^{\circ}$ and thus not count towards the error. Furthermore the edge
between the Steiner points become zero (or practically zero). This means, that
even though the angles using this edge all are less than $120^{\circ}$ and would
count towards the error, the error they contribute with is zero, which is clear
by looking at the definition of the error.

This means that the algorithm will stop before it should with a tree which has
not been minimized.

Note however that the topology used in \cref{fig:error-function} is not the one
which gives the \ac{smt} for the four terminals. The optimal topology would have
the top pair connected with one Steiner point, the two bottom terminals
connected with another Steiner point, and those two Steiner points
connected. This topology when optimized will not face the same problem, as the
two Steiner points will not move towards each other as illustrated in
\cref{fig:error-function2}. Thus it is still unclear whether this problem can
occur with a topology which optimizes to a \ac{smt}.  Should this be the case
then we can for sure say that the algorithm is only a approximation algorithm,
and not an exact algorithm\footnote{Some of the other problems in the article
  also casts doubt on whether the algorithm is actually exact. E.g.\ the earlier
described uncertainty about the convergence of the iteration function.}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{gfx/tikz/errorfunction2}
  \caption[Possible problem with the error function]{The left part of the figure
    shows the connecting and optimizing of the optimal topology for the given
    terminals. Notice how the Steiner points do not pull together, but
    instead apart, leading to the optimal tree. Thus this topology does not
    suffer the problem with the error function described in
    \cref{fig:error-function}.\label{fig:error-function2}}
\end{figure}

\subsection{The implementation}
\label{sec:implementation}

The implementation by \citeauthor{smith1992} contains several unexplained variables and conditionals not
directly related to the algorithm as it is explained in the article by \textcite{smith1992}.

\subsubsection{SCALE}
\label{sec:scale}

The variable \texttt{SCALE} is used by \citeauthor{smith1992} when calculating the initial
placement of a Steiner point. Steiner points are initially placed at the
centroid of its three neighbors. However the point is not placed exactly at the
centroid, but is instead pertubed slightly. To each coordinate of the centroid
is added $0.001 \cdot \texttt{SCALE} \cdot \texttt{RANDM()}$, where \texttt{RANDM()} is a
random number between $0$ and $1$. \texttt{SCALE} is a number defined as shown
in \cref{fig:randm}.

\begin{figure}[htbp]
\begin{c-code}
SCALE = 0.0;
for (i = 1; i <= NUMSITES; i++) {
  for (j = 0; j < DIMENSION; j++) {
    q = XX[i][j] - XX[1][j];
    if (q < 0.0) q = -q;
    if (q > SCALE) SCALE = q;
    printf(" %g", XX[i][j]);
  }
  printf("\n");
}
printf("SCALE = %g\n", SCALE);
\end{c-code}
  \captionof{listing}[Use of \texttt{SCALE} in \citeauthor{smith1992}'s implementation]{The implementation
    of \texttt{SCALE} in the original implementation. As can be seen it only
    compares the dimensions of the first terminal with the others, and not every
    terminal with every other terminal.\label{fig:randm}}
\end{figure}

As can be seen the variable finds the largest difference in coordinates between the first
terminal and all other terminals. Why this extra variable should be a part of
the perturbation is however unclear. The name of the variable might be a hint,
as it seems \citeauthor{smith1992} wished to scale the perturbation in correspondence to the
points. If this is the case however the code in \cref{fig:randm} seems to be
faulty as it should then not only compare the first point to all other, but
instead compare every point, to every other.

Thus it seems that the variable is not only not needed, but actually faulty.
This of course depends on what \citeauthor{smith1992} is trying to achieve with \texttt{SCALE}.
But overall the variable seems unneeded, and thus the new implementation does
not use \texttt{SCALE}.

\subsubsection{tol}
\label{sec:tol}

\texttt{tol} is the parameter given to the optimize function and is described as
a small positive number. In the code whenever the function is called the
variable has the value $0.0001*r/\text{NUMSITES}$ where $r$ is the current
error and and $\text{NUMSITES}$ is the number of terminals, $n$. Having some
small positive number in the optimize function makes sense, as it is used to
avoid divisions with zero (by adding it to the edge lengths, some of which may
be zero). However the specific choice which is both dependent on the error and
the number of terminals seems selected somewhat at random. A perhaps more
obvious choice would be some small \textit{fixed} number, e.g.\ just $0.0001$.

\NOTE[inline]{If time run different trees with this random
  factor, and with a fixed random factor and see if it makes a
  difference. Current experimentation shows that a fixed tol can make the
  iteration move very slowly when small optimizations has to be made. This might
  be an argument for letting it depend on the error and N. The exact numbers
  seem to originate from experiments though.}

\subsubsection{Loop condition when optimizing}
\label{sec:loop-condition-when-1}

In the main function when performing the optimization of a topology, the loop
condition seems to suffer from the same arbitrariness as both \texttt{tol} and
\texttt{SCALE}. Depending on whether we are optimizing a topology without all
terminals, or a topology with all terminals and $n-2$ Steiner points the
condition is either $r > 0.005 \cdot q$ or $r > q \cdot 0.0001$\footnote{The
  difference in the condition stems from wishing higher precision when
  optimizing a tree of a topology with all terminals.}, $r$ being the error and
$q$ the tree length. The problem here is that there seems to be no direct relation
between the error and the length of the tree, and as such this loop condition
seems very arbitrary. The loop condition however seems to work in practice.

\subsubsection{If clause when outputting tree}
\label{sec:if-clause-when}

This is most probably a bug. When a full topology with $n-2$ Steiner points is
optimized its length is compared to the current upper bound, \texttt{STUB}. If
it is better than the upper bound, this is updated to the newly found tree
length. However the tree is only outputted as a record if its length is smaller
than the previous upper bound times $0.99999$. This means we risk situations
where the upper bound is updated, but the tree never outputted. Most likely
this extra check should have been removed. As we are dealing with floating
point arithmetic doing the multiplication might make sense in the initial check,
however the double check is faulty and may lead to wrong behavior. The faulty
snippet and its possible corrections can be found in \cref{fig:if-clause-snippet}.

\begin{figure}[htbp]
\begin{c-code}
/* The faulty snippet */
if (q < STUB) {
  printf("\nnew record length %20.20g\n", q);
  for (i = 1; i <= k; i++) BESTVEC[i] = A[i];
  if (q < STUB*0.99999) output_tree();
  STUB = q;
}

/* First possible correction */
if (q < STUB) {
  printf("\nnew record length %20.20g\n", q);
  for (i = 1; i <= k; i++) BESTVEC[i] = A[i];
  output_tree();
  STUB = q;
}

/* Second possible correction */
if (q < STUB*0.99999) {
  printf("\nnew record length %20.20g\n", q);
  for (i = 1; i <= k; i++) BESTVEC[i] = A[i];
  output_tree();
  STUB = q;
}
\end{c-code}
  \captionof{listing}[Error in \citeauthor{smith1992}'s implementation when outputting trees]{The
    second conditional in the first code snippet may result in some \ac{smt}
    being set as upper bound, but outputted. This is fixed by both of the two
    next snippets.\label{fig:if-clause-snippet}}
\end{figure}

\chapterbreak{}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
