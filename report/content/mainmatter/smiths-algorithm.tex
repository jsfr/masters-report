{ \abnormalparskip{0pt}
\chapter{Smith's algorithm}
\label{cha:algorithm} }

% Short introduction to the chapter (max 1/2 page)

The following chapter will introduce the algorithm for finding \glspl{smt} in
$d$-space, as proposed by Smith~\cite{Smith1992}.

The chapter will mainly be concerned with the design of the algorithm, and the
proofs and theorems surrounding it.  It will however also delve into the actual
implementation done by Smith when this is relevant.

\section{Overview}
\label{sec:overview}

In general the algorithm follows the following form, proposed by Gilbert and
Pollak~\cite{Gilbert1968}.

\begin{enumerate}
\item Enumerate all Steiner topologies on $N$ terminals and $K$, $0 \le K \le
N-2$ Steiner points.
\item Optimize the coordinates of the Steiner points for each topology, to find
the shortest possible Euclidean embedding (tree) of that topology.
\item Select, and output, the shortest tree found.
\end{enumerate}

In other words this approach might be seen as an exhaustive search of the
solution space, and the approach described is simply to ``try all
possibilities''.

Smith however avoids necessarily having to enumerate and optimize all
topologies, by doing the following.  Firstly he only looks at \glspl{fst} as it
turns out we can regard any topology which is non-full, as a \gls{fst} where
some points overlap.

Secondly the algorithm builds the topologies, by a branching process where it
first builds the topology for $3$ terminals (and $1$ Steiner point).  From this
is then build all descendants having $4$ terminals (and $2$ Steiner points),
from each of these it builds the descendants having $5$ terminals (and $3$
Steiner points), etc.\ until it has built all descendants having $N$ terminals
(and $N-2$ Steiner points).  The idea behind this way of building the
\glspl{fst} is to allow a \gls{bnb} approach where we wish to prune all
descendants of a topology if we have a better upper bound than the length of the
current topology.  This process will be described more thoroughly in
\cref{sec:generation}.

The optimization of a topology is done by an iterative process which updates all
Steiner points of the topology every iteration, which Smith describes as an
iteration ``analogous to a Gauss-Seidel iteration''~\cite[p.~145]{Smith1992}.
The equations generated by each iteration are solved using Gaussian
elimination.  This iteration and how to solve it is described in
\cref{sec:optim-presp-topol}.

\section{Topologies}
\label{sec:topologies}

The first step of the algorithm is to generate topologies.  It is therefore
natural that we need some way of representing and generating these topologies.

The algorithm only considers \glspl{fst} where $K = N - 2$.  This simplification
is allowed, as we can simply regard any Steiner tree with $K \le N - 2$ as a
\gls{fst} where some edges have length zero and thus some points have
``merged''.

An incentive for only looking at \glspl{fst} can be found in~\cite{Gilbert1968}
which presents a table, here presented in \cref{tab:number-of-topologies},
containing the number of possible Steiner topologies for different numbers of
terminals and Steiner points.  As can clearly be seen the number of \glspl{fst}
(the diagonal) is a lot smaller than the total number of topologies.  Thus not
having to optimize the non-full topologies is clearly desirable.

\begin{table}[htbp]
  \centering
  \begin{tabular}{cccccc}
    \toprule
    $S$ & $N = 3$    & $N = 4$    & $N = 5$     & $N = 6$      & $N = 7$      \\
    \midrule
    0   & 3          & 12         & 60          & 360          & 2520         \\
    1   & \textbf{1} & 12         & 120         & 1200         & 12600        \\
    2   &            & \textbf{3} & 75          & 1350         & 22050        \\
    3   &            &            & \textbf{15} & 630          & 17640        \\
    4   &            &            &             & \textbf{105} & 6615         \\
    5   &            &            &             &              & \textbf{945} \\
    \bottomrule
  \end{tabular}
  \caption{The number of possible topologies for a Steiner tree with $N$
    terminals and $S$ Steiner points.\label{tab:number-of-topologies}}
\end{table}

Note however that even with this simplification the number of \glspl{fst} is
still exponential in $N$, which is clear from \cref{cor:number-of-fsts}.

\subsection{Representation}
\label{sec:representation}

It turns out that every \gls{fst} can be represented using a vector, in
particular we utilize the following theorem put forth by Smith~cite{Smith1992}

\begin{theorem}
  There is an 1--1 correspondence between \glspl{fst} with $N \ge 3$ terminals,
  and $(N-3)$-vectors $\vec{a}$, whose $i$th entry $a_i$ is an integer between
  $1 \le a_i \le 2 i + 1$.
\end{theorem}

Here terminals have indicies $1, 2, \ldots, N$, and Steiner points have indicies
$N+1, N+2, \ldots, 2N-2$.

The proof of this theorem is done constructively by induction on $N$.  It is
clear that the smallest \gls{fst}\footnote{This holds not only for \glspl{fst}
  but for any Steiner topology} we can construct, must have $N = 3$ as the
number of Steiner points is $N - 2 = 1$.  Thus we start with the initial null
vector $\vec{a} = ()$ corresponding to the unique \gls{fst} for the terminals
$1$, $2$ and $3$ connected through the respective edges 1, 2 and 3 and one
Steiner point $N+1$ as seen in \cref{fig:algorithm-topology-1}.  After this
first step, each entry of the topology vector is considered, one at a time,
where the $i$th entry of the topology vector describes the insertion of the
$(N+1+i)$th Steiner point on the edge $a_{i}$ and its connection to the
$(i+3)$th terminal.  Thus for the $i$th insertion we will have
$2i+1$\footnote{At the first iteration we clearly have 3 edges to insert on, and
  as each subsequent insertion generates two new edges we have, after $i$
  insertions, $3 + \underbrace{2 + \cdots + 2}_{2 i} = 2 i + 1$.} different
edges on which we can insert the Steiner point $N+1+i$ and connect it to the
regular point $i+3$.

\begin{figure}[htbp] \centering
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-1}
    \caption{The initial null vector.\label{fig:algorithm-topology-1}}
  \end{subfigure} ~
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-2}
    \caption{Connecting point 4 on edge 2.\label{fig:algorithm-topology-2}}
  \end{subfigure} ~
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-3}
    \caption{Connecting point 5 on edge 4.\label{fig:algorithm-topology-3}}
  \end{subfigure}
  \caption[Construction of FSTs]{Construction of the \glspl{fst} corresponding
to the vector $\vec{a} = (2, 4)$.\label{fig:algorithm-topologies}}
\end{figure}

Furthermore we get a corollary saying that the number of \glspl{fst} is
exponential in $N$

\begin{corollary}
\label{cor:number-of-fsts} The number of \glspl{fst} on $N$ terminals is
\[\prod_{i=0}^{N-3} 2i+1 = 1 \cdot 3 \cdot 5 \cdots (2N - 5)\] I.e.\ the number
of \glspl{fst} is exponential in $N$.
\end{corollary}

Which is clear as we must insert $N-2$ Steiner points, where the null vector is
the $0$th iteration.  Thus the last iteration must be $N-3$, and for each
iteration we have $2i+1$ different insertions.

Something which might not be completely obvious from the above proof is, that we
not only generate all topologies, but that we generate all topologies exactly
once.  This is important as duplicate topologies would risk significantly
increase the run time of the algorithm.

\FIXME[inline]{Prove the above statement.  I.e.\ that we cannot generate the same
  topology twice.}

The way Smith chooses to enumerate the edges is not explained outright, but only
in the form of a visual example~\cite[p.~143]{Smith1992}.  Exactly how one
enumerates the edges on a split is actually not so important, more so is it
important to keep it consistent.  The way Smith does it is as in
\cref{fig:algorithm-topologies}.  That is when inserting Steiner point $N+1+i$
on the $\text{edge}~a_{i}$ going from $\text{vertex}~a_i$ to $\text{vertex}~j$
with $N < j < N+i+1$, split it such that we get the following three
edges\footnote{An edge is represented as $x = (y, z)$, meaning the edge with
  index $x$ going from the vertex with index $y$ to the vertex with index
  $z$.}

\begin{itemize}
\item edge $a_{i} = (a_{i},N+1+i)$
\item edge $ 2i + 2 = (i+3,N+1+i)$
\item edge $ 2i + 3 = (j,N+1+i)$
\end{itemize}

\subsection{Generation}
\label{sec:generation}

Using the representation described in \cref{sec:representation} the problem of
generating all topologies can now be done as a backtracking problem generating
all $(N-3)$-topology vectors.  An example of how the generation works can be seen
in \cref{fig:topology-sprouting}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.8\textwidth]{gfx/tikz/topology-sprouting}
  \caption[Here be dragons.]{Upon splitting an edge and connecting a new
    terminal to the topology the algorithm can perform the split on any of the
    existing edges.  Thus in this case the algorithm branch in to three
    different topologies.  In general the splitting result in the topology
    branching into as many branches as there are edges in the topology on which
    the split is perfomed.\label{fig:topology-sprouting}}
\end{figure}

To further speed up the generation of topologies, or rather to avoid generating
unnecessary topologies, Smith also utilizes the following theorem
%
\begin{theorem}
  For any set of $N$ distinct terminals in any Euclidean space, the length of
  the shortest tree, interconnecting $N-1$ points, with topology vector
  $a_1 \cdots a_{N-4}$ is no greater than the length of the shortest tree,
  interconnecting $N$ points, with topology vector $a_1 \cdots a_{N-3}$.
\end{theorem}
%
The above theorem is easily seen to be true, simply by considering removing the
edge $e$ connecting the last terminal $N$ to the rest of the tree.  This will
obviously shorten the tree, or if the edge was length zero it would remain the
same.  Furthermore upon optimizing the tree with the last point removed, we
would either get a further shortening, or it would remain the same.

The algorithm utilizes this to prune in the following way.  Imagine we have
found some upper bound for the \gls{smt}.  If we then optimize any generated
topology vector which does not yet include all the terminals, and it turns out
to have length greater than the upper bound, we can prune any topologies that we
would have generated from this vector, as the length of the larger topologies
cannot become any smaller than the length of the current, and thus cannot become
smaller than the length of the upper bound.

This way of pruning means the algorithm must generate and optimize topologies
depth-first to get a good upper bound as quickly as possible.  If it was instead
breadth-first, nothing would  be left to prune anything, as all the \glspl{fst}
would be the last topologies on which we optimize, and these are the only ones
that can give a new upper bound.

The actual implementation of the backtracking is not described by Smith in
article itself, but only by reading the code.  The details of the current
implementation will be discussed in \cref{cha:implementation}.  The
implementation does face some problems, one of which is the fact that the entire
topology is rebuilt from scratch every time we need to either add of remove a
point.

\section{Optimization of a prespecified topology}
\label{sec:optim-presp-topol}

When a topology has been found, we need a way of optimizing the positions of the
Steiner points.  The approach taken by Smith is in general to create an
iteration with $N-2$ equations, one for each Steiner point, and incrementally
optimize all of them every iteration.  This is done by solving the equations
using Gauss elimination.  The section will be concerned with describing the
iteration, the proof of correctness and convergence, the speed of convergence
and the error function for deciding convergence.

Apart from the the already used $N$ and $K$, we define the following symbols as
well
%
\begin{itemize}
\item $R$, the set of terminals.
\item $S$, the set of Steiner points.
\item $T$, the pre-specified topology.  Defined as all edges it contains in the
form $(j,k)$ where $\vec x_j$ and $\vec x_k$ are contained in either $S$ or $R$
and an edge exists in the topology from $x_j$ to $x_k$ or vice versa.
\item $L$, the length of the tree.  Defined as the sum of the euclidean lengths
of all edges in the topology $T$.
\end{itemize}

\NOTE[inline]{Smith is inconsistent with having the sets as containing either
the points of the indicies.  This could perhaps be fixed and clarified here.}

\subsection{Simple iteration}
\label{sec:simple-iteration}

To first get a sense of how we might go about optimizing the tree in a simple
manner we first look at a iteration which only optimizes one Steiner point at a
time.  This is a simpler approach to the one proposed by Smith, but will
probably also take longer time to converge.  The iteration is described by
\cref{alg:simple-iteration}.  The algorithm is intentionally a bit vague, as
even the simple iteration has several choices we can make.

\begin{algorithm}[htbp]
  \KwData{
    \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,label=-,leftmargin=10pt]
    \item A \gls{fst} containing the Steiner points $S$, terminals $R$, and edges $E$
    \item A small positive number $\epsilon$
    \end{itemize}
  }
  \KwResult{The optimized \gls{fst}}
  \ForEach{$s \in S$}{
    $\textit{error} \gets \infty$\;
    \While{$\textit{error} > \epsilon$}{
      optimize $s$ with respect to its neighbors in $T$\;
      recalculate \textit{error}\;
    }
  }
  \Return{$T$}
  \caption[Simple iteration]{Pseudo code describing the optimization strategy
    using the simple iteration.\label{alg:simple-iteration}}
\end{algorithm}

First of we should choose how a single Steiner point is optimized.  This is in
reality the problem of finding the Fermat-Torricelli point~\missingref{maybe a
  ref here?} for three points connected to the Steiner point we wish to
optimize.  There are several ways this can be done -- Either one can go about it
geometrically~\missingref{ref here!}, iteratively (using e.g.\ a descent
algorithm)~\missingref{ref here!} or analytically~\missingref{ref of the article
  from alexei here.}.  The geometrical approach cannot be taken for any
dimension above 2~\TODO{Check if this is correct, and add a reference}.  There
are several iterative approaches, but all have in common that several iterations
have to performed to place a single Steiner point.  Finally there exists the
analytical solution, which seems promising as it immediatly places a Steiner
point correctly according to the points it is connected to.  Of course some of
the neighbors may also be Steiner points, and thus when they are replaced the
Steiner point will have to be recalculated.

\TODO[inline]{Prove that the algorithm terminates.  And that it does so with an optimal
  solution.  This can probably be seen from the physical spring model, where we
  relax the springs}

The problem Smith poses for the simple iteration described here is that while it
quickly becomes optimal ``locally'' it may still converge slowly as it does not
regard the ``global'' structure.

\subsection{Smith's iteration}
\label{sec:smiths-iteration}

Now that we have looked at a simple way of optimizing the topology, by finding
the coordinates of one Steiner point at a time, we can progress to the iteration
proposed by Smith.  This iteration optimizes all points in every iteration.

To optimize the topology Smith proposes iteratively solving the $N-2$
equations\footnote{Note that in the second sum of \cref{eq:6} Smith has given
  the index $j:(j,k) \in T, j \in S$.  For the rewrite to make sense, this should
one have been $j:(j,k) \in T$.  Thus I believe this is just a typo and have
progressed as such.}
%
\begin{alignat}{2}
  \label{eq:6} \vec x_k^{(i+1)}
  &= \frac{\sum_{j : (j, k) \in T}
    \frac{\vec x_j^{(i+1)}}{|\vec x_k^{(i)} - \vec x_j^{(i)}|}}
    {\sum_{j: (j,k) \in T}
    \frac{1}{|\vec x_k^{(i)} - \vec x_j^{(i)}|}}, & \quad
    N+1 &\le k \le 2N-2
\intertext{which can be rewritten as}
\label{eq:9}
  \vec 0
  &= \sum_{j:(j,k) \in T}
    \frac{\vec x_k^{(i+1)} - \vec x_j^{(i+1)}}
    {|\vec x_k^{(i)} - \vec x_j^{(i)}|}, & \quad
     N+1 &\le k \le 2N-2
\end{alignat}
%
Here $\vec x_k \in S$ represents the $k$-th Steiner point, being a
$d$-vector.  Iteratively solving this system from the initial coordinates
$x^{(0)}_k$, the iteration will converge towards the unique optimum Steiner
point coordinates that minimize the total length, $L$\footnote{The tree length
  is as is normally the case calculated as the sum of all edge lengths.  In this
  case we use the regular Euclidean (L2) norm.  One could however think to
  possibly use other norms, e.g.\ the rectilinear L1.}, of the tree.  The
convergence happens in such a way that the sequence of tree lengths $L(S^{(i)})$
is monotonically decreasing.  Note that since the unknowns in the iteration are
vectors we actually have $d$ independent equation systems of $N-2$ equations, $d$ being the
dimension.

The proof that this iteration actually converges to an optimal solution is done
as following.  Firstly after each ($i$th) iteration, $S^{(i+1)}$ exactly
minimizes the following quadratic form $Q^{(i)}(S)$
%
\begin{equation}
  \label{eq:7}
  Q^{(i)}(S) = \sum_{(j,k) \in T, k \in S, j < k } \frac{|\vec x_k - \vec x_j|^2}{|\vec x^{(i)}_k - \vec x^{(i)}_j|}
\end{equation}
%
It is easily seen that $Q(S)$ is related to the length $L(S)$ as follows
%
\begin{equation}
  \label{eq:8}
  L(S^{(i)}) = Q^{(i)}(S^{(i)}), \quad \nabla L(S^{(i)}) \propto \nabla Q^{(i)}(S^{(i)})
\end{equation}

Secondly one can think of $Q$ as the potential energy of a system of ideal
springs on the tree edges, where the force constant of each spring is
proportional to the reciprocal of its original length before each iteration.  The
$i$th iteration causes all springs to relax, minimizing $Q^{(i)}$.  Furthermore
as can be seen from \cref{eq:9} the forces of the $k$th Steiner point are
clearly in equilibrium.  Now write
%
\begin{align}
  \label{eq:10}
  L(S^{(i)}) &= Q^{(i)}(S^{(i)}) \ge Q^{(i)}(S^{(i+1)}) \\
             &= \sum_{j, k : j < k, (j,k) \in T, k \in S} \frac{{(
               |\vec x_k^{(i)} - \vec x_j^{(j)}| + |\vec x_k^{(i+1)} -
               \vec x_j^{(i+1)}| - |\vec x_k^{(i)} - \vec x_j^{(j)}|)}^2}{
               |\vec x_k^{(i)} - \vec x_j^{(i)}|} \\
             &= L(S^{(i)}) + 2 [ L(S^{(i+1)}) - L(S^{(i)})] \nonumber \\
             &+ \sum_{j, k : j < k, (j,k) \in T, k \in S} \frac{{(
               |\vec x_k^{(i+1)} -
               \vec x_j^{(i+1)}| - |\vec x_k^{(i)} - \vec x_j^{(j)}|)}^2}{
               |\vec x_k^{(i)} - \vec x_j^{(i)}|}
\end{align}
%
It is obvious that the last sum is non-negative, and thus
$L(S^{(i)}) \ge L(S^{(i+1)})$.  Smith furthermore goes into detail on how the
only stable fixed point is the optimum, and thus even if the iteration reaches
one such point, any small perturbation of $S$ in any $L$-decreasing direction
will cause the iteration to resume decreasing $L$ until it has been
minimized. The exact details of this part will not be covered here, but can be
found in~\cite[p.~147--148]{Smith1992}.

The equation system of each iteration can be solved using Gaussian elimination.
This is done by identifying an equation which only has at most one unknown on the right
hand side.  This corresponds to a Steiner point being connected to two or more
terminals, which is always guaranteed to exist.  Thus if we use a $\mathcal
O(N)$ algorithm for solving a single system and we actually have $d$ systems, we
can solve a single iteration in $\mathcal O(Nd)$.

According to Smith~\cite[p.~150]{Smith1992} the iteration has a good speed of
convergence. In most cases geometric convergence will occur. However some
degenerate configurations of terminals can cause sublinear convergence.

\NOTE[inline]{Should I perhaps touch on the speed of convergence more in-depth?}

\section{Questionable elements of Smith's article}
\label{sec:quest-elem-smiths}

There are a number of elements in the article and the given
implementation~\cite{Smith1992} which are somewhat questionable.  These I will
try to discuss here.

\subsection{Choice of error function}
\label{sec:choice-error-funct}

To detect when convergence has happened Smith proposes using a special
error function instead of the one often used where one stops when the
improvements from iteration to iteration falls below some $\epsilon$.  The
function proposed by Smith is
%
\begin{equation}
  \label{eq:1} E^2 = \sum_{
    \begin{array}{c} i \in S \\ (i,j) \in T \\ (i,k) \in T \\ j \ne k
    \end{array}} \text{pos} (2 (\vec x_j - \vec x_i) \cdot (\vec x_k - \vec x_i)
+ | \vec x_j - \vec x_i | \cdot | \vec x_k - \vec x_i |)
\end{equation}
%
Here $\text{pos}(x) = \max(x, 0)$.  This error function sums together all angles which
are smaller than $120^{\circ}$.  The error function is derived from calculating
the angle between edges in the following way:
%
\begin{align}
  \cos v & = \frac{\vec a \cdot \vec b}
    {|\vec a| \cdot |\vec b|} \label{eq:2}                 \\
  \intertext{If we then wish to find the cosine for $120^{\circ}$}
  \cos 120^{\circ}
         & = -\frac{1}{2} = \frac{\vec a \cdot \vec b}
    {|\vec a| \cdot |\vec b|} \Leftrightarrow \label{eq:3} \\
  0      & = \vec a \cdot \vec b + \frac{1}{2}
    (|\vec a| \cdot |\vec b|) \Leftrightarrow \label{eq:4} \\
  0      & = 2 (\vec a \cdot \vec b) + |\vec a| \cdot |\vec b| \label{eq:5}
\end{align}
%
Thus any angle less than $120^{\circ}$ gives a positive number when inserted in
the function, and any angle below will give a negative number (which is set to
zero by $\text{pos}(x)$).

Why this error function is supposed to be better is not explained very
thoroughly by Smith. He only refers to doing some experimentation which led him
to believe that this would be a good function.  The idea behind the function
seems to be that, as the final tree should only have angles of $120^{\circ}$ any
angle less than this must still need to be optimized.

There are however some problems with this choice of error function.  Consider
the configuration as shown in the first tree of \cref{fig:error-function}. Here
there are two pairs of terminals, very far apart.  These are connected by two
Steiner points as seen.  Running Smith's iteration the two Steiner points will
move together, and up towards the center of the rectangle of the four
terminals.  However suppose the two points reach each other before they reach
the center.  In this case the angles on the left and right will be more than
$120^{\circ}$ and thus not count towards the error.  Furthermore the edge
between the Steiner points become zero (or practically zero).  This means, that
even though the angles using this edge all are less than $120^{\circ}$ and would
count towards the error, the error they contribute with is zero, which is clear
by looking at the definition of the error.

This means that the algorithm will stop before it should with a tree which has
not been minimized.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{gfx/tikz/errorfunction}
  \caption[Here be dragons.]{Here be dragons.\label{fig:error-function}}
\end{figure}

Note however that the topology used in \cref{fig:error-function} is not the
one which gives the \gls{mst} for the four terminals.  The optimal topology
would have the top pair connected with one Steiner point, the two bottom
terminals connected with another Steiner point, and those two Steiner points
connected.  This topology when optimized will not face the same problem, as the
two Steiner points will not move towards each other.  Thus it is still unclear
whether this problem can occur with a topology which optimizes to a \gls{smt}.
Should this be the case then the algorithm cannot actually be considered an
exact algorithm, but only a approximation algorithm.

\subsection{The implementation}
\label{sec:implementation}

The implementation by Smith contains several unexplained variables and conditionals not
directly related to the algorithm as it is explained in the article by Smith.

\subsubsection{SCALE}
\label{sec:scale}

The variable \texttt{SCALE} is used by Smith when calculating the initial
placement of a Steiner point.  Steiner points are initially placed at the
centroid of its three neighbors.  However the point is not placed exactly at the
centroid, but is instead pertubed slightly.  To each coordinate of the centroid
is added $0.001 \cdot \texttt{SCALE} \cdot \texttt{RANDM()}$, where \texttt{RANDM()} is a
random number between $0$ and $1$.  \texttt{SCALE} is a number defined as shown
in \cref{fig:randm}.

\begin{figure}[htbp]
  \begin{c-code}
    SCALE = 0.0;
    for (i = 1; i <= NUMSITES; i++) {
      for (j = 0; j < DIMENSION; j++) {
        q = XX[i][j] - XX[1][j];
        if (q < 0.0) q = -q;
        if (q > SCALE) SCALE = q;
        printf(" %g", XX[i][j]);
      }
      printf("\n");
    }
    printf("SCALE = %g\n", SCALE);
  \end{c-code}
\caption[here be dragons]{here be dragons.\label{fig:randm}}
\end{figure}

As can be seen the variable finds the largest difference in coordinates between the first
terminal and all other terminals.  Why this extra variable should be a part of
the perturbation is however unclear.  The name of the variable might be a hint,
as it seems Smith wished to scale the perturbation in correspondence to the
points.  If this is the case however the code in \cref{fig:randm} seems to be
faulty as it should then not only compare the first point to all other, but
instead compare every point, to every other.

Thus it seems that the variable is not only not needed, but actually faulty.
This of course depends on what Smith is trying to achieve with \texttt{SCALE}.
But overall the variable seems unneeded, and thus the new implementation does
not use \texttt{SCALE}.

\subsubsection{tol}
\label{sec:tol}

\texttt{tol} is the parameter given to the optimize function and is described as
a small positive number.  In the code whenever the function is called the
variable has the value $0.0001*r/\text{NUMSITES}$ where $r$ is the current
error and and $\text{NUMSITES}$ is the number of terminals, $N$.  Having some
small positive number in the optimize function makes sense, as it is used to
avoid divisions with zero (by adding it to the edge lengths, some of which may
be zero).  However the specific choice which is both dependent on the error and
the number of terminals seems selected somewhat at random.  A perhaps more
obvious choice would be some small \textit{fixed} number, e.g.\ just $0.0001$.

\TODO[inline]{run different trees with this random factor, and with a fixed
  random factor and see if it makes a difference}
\NOTE[inline]{Current experimentation shows that a fixed tol can make the
  iteration move very slowly when small optimizations has to be made. This might
be an argument for letting it depend on the error and N. The exact numbers seem
to originate from experiments though.}

\subsubsection{Pruning}
\label{sec:pruning}

\TODO[inline]{Write about the way the main loop runs and how it prunes. It seems
to skip some trees it should not.}

\subsubsection{Loop condition when optimizing}
\label{sec:loop-condition-when-1}

In the main function when performing the optimization of a topology, the loop
condition seems to suffer from the same arbitrariness as both \texttt{tol} and
\texttt{SCALE}.  Depending on whether we are optimizing a regular topology, or a
topology with all $N-2$ Steiner points the condition is either $r > 0.005 \cdot
q$ or $r > q \cdot 0.0001$, $r$ being the error and $q$ the tree length.  The
problem here is that there is no direct relation between the error and the
length of the tree, and as such this loop condition seems very arbitrary.  While
it seems to work in practice, it seems that a better loop condition would be
when the error falls below some small number $\epsilon$.

\NOTE[inline]{Perhaps perform an experiment on this as well.}

\subsubsection{If clause when outputting tree}
\label{sec:if-clause-when}

This is most probably a bug.  When a full topology with $N-2$ Steiner points is
optimized its length is compared to the current upper bound, \texttt{STUB}. If
it is better than the upper bound, this is updated to the newly found tree
length.  However the tree is only outputted as a record if its length is smaller
than the previous upper bound times $0.99999$.  This means we risk situations
where the upper bound is updated, but the tree never outputted.  Most likely
this extra check should have been removed.  As we are dealing with floating
point arithmetic doing the multiplication might make sense in the initial check,
however the double check is faulty and may lead to wrong behavior. The faulty
snippet and its possible corrections can be found in \cref{fig:if-clause-snippet}.

\begin{figure}[htbp]
  \begin{c-code}
    /* The faulty snippet */
    if (q < STUB) {
      printf("\nnew record length %20.20g\n", q);
      for (i = 1; i <= k; i++) BESTVEC[i] = A[i];
      if (q < STUB*0.99999) output_tree();
      STUB = q;
    }

    /* First possible correction */
    if (q < STUB) {
      printf("\nnew record length %20.20g\n", q);
      for (i = 1; i <= k; i++) BESTVEC[i] = A[i];
      output_tree();
      STUB = q;
    }

    /* Second possible correction */
    if (q < STUB*0.99999) {
      printf("\nnew record length %20.20g\n", q);
      for (i = 1; i <= k; i++) BESTVEC[i] = A[i];
      output_tree();
      STUB = q;
    }
  \end{c-code}
  \caption[Here be dragons]{Here be dragons.\label{fig:if-clause-snippet}}
\end{figure}

\chapterbreak{}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
