{ \abnormalparskip{0pt}
\chapter{Smith's algorithm}
\label{cha:algorithm} }

% Short introduction to the chapter (max 1/2 page)

The following chapter will introduce the algorithm for finding \glspl{smt} in
$d$-space, as proposed by Smith~\cite{Smith1992}.

The chapter will mainly be concerned with the design of the algorithm, and the
proofs and theorems surrounding it.  It will however also delve into the actual
implementation done by Smith when this is relevant.

\section{Overview}
\label{sec:overview}

In general the algorithm follows the following form, proposed by Gilbert and
Pollak~\cite{Gilbert1968}.

\begin{enumerate}
\item Enumerate all Steiner topologies on $N$ terminals and $K$, $0 \le K \le
N-2$ Steiner points.
\item Optimize the coordinates of the Steiner points for each topology, to find
the shortest possible Euclidean embedding (tree) of that topology.
\item Select, and output, the shortest tree found.
\end{enumerate}

In other words this approach might be seen as an exhaustive search of the
solution space, and the approach described is simply to ``try all
possibilities''.

Smith however avoids necessarily having to enumerate and optimize all
topologies, by doing the following.  Firstly he only looks at \glspl{fst} as it
turns out we can regard any topology which is non-full, as a \gls{fst} where
some points overlap.

Secondly the algorithm builds the topologies, by a branching process where it
first builds the topology for $3$ terminals (and $1$ Steiner point).  From this
is then build all descendants having $4$ terminals (and $2$ Steiner points),
from each of these it builds the descendants having $5$ terminals (and $3$
Steiner points), etc.\ until it has built all descendants having $N$ terminals
(and $N-2$ Steiner points).  The idea behind this way of building the
\glspl{fst} is to allow a \gls{bnb} approach where we wish to prune all
descendants of a topology if we have a better upper bound than the length of the
current topology.  This process will be described more thoroughly in
\cref{sec:generation}.

The optimization of a topology is done by an iterative process which updates all
Steiner points of the topology every iteration, which Smith describes as an
iteration ``analogous to a Gauss-Seidel iteration''~\cite[p.~145]{Smith1992}.
The equations generated by each iteration are solved using Gaussian
elimination.  This iteration and how to solve it is described in
\cref{sec:optim-presp-topol}.

\section{Topologies}
\label{sec:topologies}

The first step of the algorithm is to generate topologies.  It is therefore
natural that we need some way of representing and generating these topologies.

The algorithm only considers \glspl{fst} where $K = N - 2$.  This simplification
is allowed, as we can simply regard any Steiner tree with $K \le N - 2$ as a
\gls{fst} where some edges have length zero and thus some points have
``merged''.

An incentive for only looking at \glspl{fst} can be found in~\cite{Gilbert1968}
which presents a table, here presented in \cref{tab:number-of-topologies},
containing the number of possible Steiner topologies for different numbers of
terminals and Steiner points.  As can clearly be seen the number of \glspl{fst}
(the diagonal) is a lot smaller than the total number of topologies.  Thus not
having to optimize the non-full topologies is clearly desirable.

\begin{table}[htbp]
  \centering
  \begin{tabular}{cccccc}
    \toprule
    $S$ & $N = 3$    & $N = 4$    & $N = 5$     & $N = 6$      & $N = 7$      \\
    \midrule
    0   & 3          & 12         & 60          & 360          & 2520         \\
    1   & \textbf{1} & 12         & 120         & 1200         & 12600        \\
    2   &            & \textbf{3} & 75          & 1350         & 22050        \\
    3   &            &            & \textbf{15} & 630          & 17640        \\
    4   &            &            &             & \textbf{105} & 6615         \\
    5   &            &            &             &              & \textbf{945} \\
    \bottomrule
  \end{tabular}
  \caption{The number of possible topologies for a Steiner tree with $N$
    terminals and $S$ Steiner points.\label{tab:number-of-topologies}}
\end{table}

Note however that even with this simplification the number of \glspl{fst} is
still exponential in $N$, which is clear from \cref{cor:number-of-fsts}.

\subsection{Representation}
\label{sec:representation}

It turns out that every \gls{fst} can be represented using a vector, in
particular we utilize the following theorem put forth by Smith~cite{Smith1992}

\begin{theorem}
  There is an 1--1 correspondence between \glspl{fst} with $N \ge 3$ terminals,
  and $(N-3)$-vectors $\vec{a}$, whose $i$th entry $a_i$ is an integer between
  $1 \le a_i \le 2 i + 1$.
\end{theorem}

Here terminals have indicies $1, 2, \ldots, N$, and Steiner points have indicies
$N+1, N+2, \ldots, 2N-2$.

The proof of this theorem is done constructively by induction on $N$.  It is
clear that the smallest \gls{fst}\footnote{This holds not only for \glspl{fst}
  but for any Steiner topology} we can construct, must have $N = 3$ as the
number of Steiner points is $N - 2 = 1$.  Thus we start with the initial null
vector $\vec{a} = ()$ corresponding to the unique \gls{fst} for the terminals
$1$, $2$ and $3$ connected through the respective edges 1, 2 and 3 and one
Steiner point $N+1$ as seen in \cref{fig:algorithm-topology-1}.  After this
first step, each entry of the topology vector is considered, one at a time,
where the $i$th entry of the topology vector describes the insertion of the
$(N+1+i)$th Steiner point on the edge $a_{i}$ and its connection to the
$(i+3)$th terminal.  Thus for the $i$th insertion we will have
$2i+1$\footnote{At the first iteration we clearly have 3 edges to insert on, and
  as each subsequent insertion generates two new edges we have, after $i$
  insertions, $3 + \underbrace{2 + \cdots + 2}_{2 i} = 2 i + 1$.} different
edges on which we can insert the Steiner point $N+1+i$ and connect it to the
regular point $i+3$.

\begin{figure}[htbp] \centering
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-1}
    \caption{The initial null vector.\label{fig:algorithm-topology-1}}
  \end{subfigure} ~
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-2}
    \caption{Connecting point 4 on edge 2.\label{fig:algorithm-topology-2}}
  \end{subfigure} ~
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-3}
    \caption{Connecting point 5 on edge 4.\label{fig:algorithm-topology-3}}
  \end{subfigure}
  \caption[Construction of FSTs]{Construction of the \glspl{fst} corresponding
to the vector $\vec{a} = (2, 4)$.\label{fig:algorithm-topologies}}
\end{figure}

Furthermore we get a corollary saying that the number of \glspl{fst} is
exponential in $N$

\begin{corollary}
\label{cor:number-of-fsts} The number of \glspl{fst} on $N$ terminals is
\[\prod_{i=0}^{N-3} 2i+1 = 1 \cdot 3 \cdot 5 \cdots (2N - 5)\] I.e.\ the number
of \glspl{fst} is exponential in $N$.
\end{corollary}

Which is clear as we must insert $N-2$ Steiner points, where the null vector is
the $0$th iteration.  Thus the last iteration must be $N-3$, and for each
iteration we have $2i+1$ different insertions.

Something which might not be completely obvious from the above proof is, that we
not only generate all topologies, but that we generate all topologies exactly
once.  This is important as duplicate topologies would risk significantly
increase the run time of the algorithm.

\FIXME[inline]{Prove the above statement.  I.e.\ that we cannot generate the same
  topology twice.}

The way Smith chooses to enumerate the edges is not explained outright, but only
in the form of a visual example~\cite[p.~143]{Smith1992}.  Exactly how one
enumerates the edges on a split is actually not so important, more so is it
important to keep it consistent.  The way Smith does it is as in
\cref{fig:algorithm-topologies}.  That is when inserting Steiner point $N+1+i$
on the $\text{edge}~a_{i}$ going from $\text{vertex}~a_i$ to $\text{vertex}~j$
with $N < j < N+i+1$, split it such that we get the following three
edges\footnote{An edge is represented as $x = (y, z)$, meaning the edge with
  index $x$ going from the vertex with index $y$ to the vertex with index
  $z$.}

\begin{itemize}
\item edge $a_{i} = (a_{i},N+1+i)$
\item edge $ 2i + 2 = (i+3,N+1+i)$
\item edge $ 2i + 3 = (j,N+1+i)$
\end{itemize}

\subsection{Generation}
\label{sec:generation}

Using the representation described in \cref{sec:representation} the problem of
generating all topologies can now be done as a backtracking problem generating
all $(N-3)$-topology vectors.  An example of how the generation works can be seen
in \cref{fig:topology-sprouting}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.8\textwidth]{gfx/tikz/topology-sprouting}
  \caption{Upon splitting an edge and connecting a new terminal to the topology
    the algorithm can perform the split on any of the existing edges.  Thus in
    this case the algorithm branch in to three different topologies.  In general
    the splitting result in the topology branching into as many branches as
    there are edges in the topology on which the split is perfomed.\label{fig:topology-sprouting}}
\end{figure}

To further speed up the generation of topologies, or rather to avoid generating
unnecessary topologies, Smith also utilizes the following theorem
%
\begin{theorem}
  For any set of $N$ distinct terminals in any Euclidean space, the length of
  the shortest tree, interconnecting $N-1$ points, with topology vector
  $a_1 \cdots a_{N-4}$ is no greater than the length of the shortest tree,
  interconnecting $N$ points, with topology vector $a_1 \cdots a_{N-3}$.
\end{theorem}
%
The above theorem is easily seen to be true, simply by considering removing the
edge $e$ connecting the last terminal $N$ to the rest of the tree.  This will
obviously shorten the tree, or if the edge was length zero it would remain the
same.  Furthermore upon optimizing the tree with the last point removed, we
would either get a further shortening, or it would remain the same.

The algorithm utilizes this to prune in the following way.  Imagine we have
found some upper bound for the \gls{smt}.  If we then optimize any generated
topology vector which does not yet include all the terminals, and it turns out
to have length greater than the upper bound, we can prune any topologies that we
would have generated from this vector, as the length of the larger topologies
cannot become any smaller than the length of the current, and thus cannot become
smaller than the length of the upper bound.

This way of pruning means the algorithm must generate and optimize topologies
depth-first to get a good upper bound as quickly as possible.  If it was instead
breadth-first, nothing would  be left to prune anything, as all the \glspl{fst}
would be the last topologies on which we optimize, and these are the only ones
that can give a new upper bound.

The actual implementation of the backtracking is not described by Smith in
article itself, but only by reading the code.  The details of the current
implementation will be discussed in \cref{cha:implementation}.  The
implementation does face some problems, one of which is the fact that the entire
topology is rebuilt from scratch every time we need to either add of remove a
point.

\section{Optimization of a prespecified topology}
\label{sec:optim-presp-topol}

When a topology has been found, we need a way of optimizing the positions of the
Steiner points.  The approach taken by Smith is in general to create an
iteration with $N-2$ equations, one for each Steiner point, and incrementally
optimize all of them every iteration.  This is done by solving the equations
using Gauss elimination.  The section will be concerned with describing the
iteration, the proof of correctness and convergence, the speed of convergence
and the error function for deciding convergence.

Apart from the the already used $N$ and $K$, we define the following symbols as
well
%
\begin{itemize}
\item $R$, the set of terminals.
\item $S$, the set of Steiner points.
\item $T$, the pre-specified topology.  Defined as all edges it contains in the
form $(j,k)$ where $\vec x_j$ and $\vec x_k$ are contained in either $S$ or $R$
and an edge exists in the topology from $x_j$ to $x_k$ or vice versa.
\item $L$, the length of the tree.  Defined as the sum of the euclidean lengths
of all edges in in the topology $T$.  See \cref{eq:length} for the exact
definition.
\end{itemize}

\NOTE[inline]{Smith is inconsistent with having the sets as containing either
the points of the indicies.  This could perhaps be fixed and clarified here.}

\subsection{Simple iteration}
\label{sec:simple-iteration}

To first get a sense of how we might go about optimizing the tree in a simple
manner we first look at a iteration which only optimizes one Steiner point at a
time.  This is a simpler approach to the one proposed by Smith, but will
probably also take longer time to converge.  The iteration is described by
\cref{alg:simple-iteration}.  The algorithm is intentionally a bit vague, as
even the simple iteration has several choices we can make.

\begin{algorithm}[htbp]
  \KwData{
    \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,label=-,leftmargin=10pt]
    \item A \gls{fst} containing the Steiner points $S$, terminals $R$, and edges $E$
    \item A small positive number $\epsilon$
    \end{itemize}
  }
  \KwResult{The optimized \gls{fst}}
  \ForEach{$s \in S$}{
    $\textit{error} \gets \infty$\;
    \While{$\textit{error} > \epsilon$}{
      optimize $s$ with respect to its neighbors in $T$\;
      recalculate \textit{error}\;
    }
  }
  \Return{$T$}
  \caption[Simple iteration]{Pseudo code describing the optimization strategy
    using the simple iteration.\label{alg:simple-iteration}}
\end{algorithm}

First of we should choose how a single Steiner point is optimized.  This is in
reality the problem of finding the Fermat-Torricelli point~\missingref{maybe a
  ref here?} for three points connected to the Steiner point we wish to
optimize.  There are several ways this can be done -- Either one can go about it
geometrically~\missingref{ref here!}, iteratively (using e.g.\ a descent
algorithm)~\missingref{ref here!} or analytically~\missingref{ref of the article
  from alexei here.}.  The geometrical approach cannot be taken for any
dimension above 2~\TODO{Check if this is correct, and add a reference}.  There
are several iterative approaches, but all have in common that several iterations
have to performed to place a single Steiner point.  Finally there exists the
analytical solution, which seems promising as it immediatly places a Steiner
point correctly according to the points it is connected to.  Of course some of
the neighbors may also be Steiner points, and thus when they are replaced the
Steiner point will have to be recalculated.

\TODO[inline]{Prove that the algorithm terminates.  And that it does so with an optimal
  solution.  This can probably be seen from the physical spring model, where we
  relax the springs}

The problem Smith poses for the simple iteration described here is that while it
quickly becomes optimal ``locally'' it may still converge slowly as it does not
regard the ``global'' structure.

\subsection{Smith's iteration}
\label{sec:smiths-iteration}

Now that we have looked at a simple way of optimizing the topology, by finding
the coordinates of one Steiner point at a time, we can progress to the iteration
proposed by Smith.  This iteration optimizes all points in every iteration.

To optimize the topology Smith proposes iteratively solving the $N-2$
equations\footnote{Note that in the second sum of \cref{eq:6} Smith has given
  the index $j:(j,k) \in T, j \in S$.  For the rewrite to make sense, this should
one have been $j:(j,k) \in T$.  Thus I believe this is just a typo and have
progressed as such.}
%
\begin{alignat}{2}
  \label{eq:6} \vec x_k^{(i+1)}
  &= \frac{\sum_{j : (j, k) \in T}
    \frac{\vec x_j^{(i+1)}}{|\vec x_k^{(i)} - \vec x_j^{(i)}|}}
    {\sum_{j: (j,k) \in T}
    \frac{1}{|\vec x_k^{(i)} - \vec x_j^{(i)}|}}, & \quad
    N+1 &\le k \le 2N-2
\intertext{which can be rewritten as}
\label{eq:9}
  \vec 0
  &= \sum_{j:(j,k) \in T}
    \frac{\vec x_k^{(i+1)} - \vec x_j^{(i+1)}}
    {|\vec x_k^{(i)} - \vec x_j^{(i)}|}, & \quad
     N+1 &\le k \le 2N-2
\end{alignat}
%
Here $\vec x_k \in S$ represents the $k$-th Steiner point, being a
$d$-vector.  Iteratively solving this system from the initial coordinates
$x^{(0)}_k$, the iteration will converge towards the unique optimum Steiner
point coordinates that minimize the total length, $L$\footnote{The tree length
  is as is normally the case calculated as the sum of all edge lengths.  In this
  case we use the regular Euclidean (L2) norm.  One could however think to
  possibly use other norms, e.g.\ the rectilinear L1.}, of the tree.  The
convergence happens in such a way that the sequence of tree lengths $L(S^{(i)})$
is monotonically decreasing.  Note that since the unknowns in the iteration are
vectors we actually have $d$ independent equation systems of $N-2$ equations, $d$ being the
dimension.

The proof that this iteration actually converges to an optimal solution is done
as following.  Firstly after each ($i$th) iteration, $S^{(i+1)}$ exactly
minimizes the following quadratic form $Q^{(i)}(S)$
%
\begin{equation}
  \label{eq:7}
  Q^{(i)}(S) = \sum_{(j,k) \in T, k \in S, j < k } \frac{|\vec x_k - \vec x_j|^2}{|\vec x^{(i)}_k - \vec x^{(i)}_j|}
\end{equation}
%
It is easily seen that $Q(S)$ is related to the length $L(S)$ as follows
%
\begin{equation}
  \label{eq:8}
  L(S^{(i)}) = Q^{(i)}(S^{(i)}), \quad \nabla L(S^{(i)}) \propto \nabla Q^{(i)}(S^{(i)})
\end{equation}

Secondly one can think of $Q$ as the potential energy of a system of ideal
springs on the tree edges, where the force constant of each spring is
proportional to the reciprocal of its original length before each iteration.  The
$i$th iteration causes all springs to relax, minimizing $Q^{(i)}$.  Furthermore
as can be seen from \cref{eq:9} the forces of the $k$th Steiner point are
clearly in equilibrium.  Now write
%
\begin{align}
  \label{eq:10}
  L(S^{(i)}) &= Q^{(i)}(S^{(i)}) \ge Q^{(i)}(S^{(i+1)}) \\
             &= \sum_{j, k : j < k, (j,k) \in T, k \in S} \frac{{(
               |\vec x_k^{(i)} - \vec x_j^{(j)}| + |\vec x_k^{(i+1)} -
               \vec x_j^{(i+1)}| - |\vec x_k^{(i)} - \vec x_j^{(j)}|)}^2}{
               |\vec x_k^{(i)} - \vec x_j^{(i)}|} \\
             &= L(S^{(i)}) + 2 [ L(S^{(i+1)}) - L(S^{(i)})] \nonumber \\
             &+ \sum_{j, k : j < k, (j,k) \in T, k \in S} \frac{{(
               |\vec x_k^{(i+1)} -
               \vec x_j^{(i+1)}| - |\vec x_k^{(i)} - \vec x_j^{(j)}|)}^2}{
               |\vec x_k^{(i)} - \vec x_j^{(i)}|}
\end{align}
%
It is obvious that the last sum is non-negative, and thus
$L(S^{(i)}) \ge L(S^{(i+1)})$.  Smiths furthermore goes into detail on how the
only stable fixed point is the optimum, and thus even if the iteration reaches
one such point, any small perturbation of $S$ in any $L$-decreasing direction
will cause the iteration to resume decreasing $L$ until it has been
minimized. The exact details of this part will not be covered here, but can be
found in~\cite[p.~147--148]{Smith1992}.

The equation system of each iteration can be solved using Gaussian elimination.
This is done by identifying an equation which only has at most one unknown on the right
hand side.  This corresponds to a Steiner point being connected to two or more
terminals, which is always guaranteed to exist.  Thus if we use a $\mathcal
O(N)$ algorithm for solving a single system and we actually have $d$ systems, we
can solve a single iteration in $\mathcal O(Nd)$.

\subsection{Convergence}
\label{sec:convergence}



\TODO[inline]{THIS SECTION IS NOT DONE!}

\subsection{Error function}
\label{sec:error-function}

\FIXME[inline]{Redo this section especially the part with the table!}

To detect when convergence has happened, Smith warns about using the obvious way
of testing whether one iterate differs substantially from the next and stop if
it does not~\cite[p.~151]{Smith1992}\NOTE{why does he warn about this? Should we
give an example where this could go wrong?}.  Instead Smith proposed using an
error-function which looks at all angles smaller than $120^{\circ}$, as an
\gls{mst}, if it is also a \gls{fst}, will only have angles of $120^{\circ}$.
The function Smith proposed is
%
\begin{equation}
  \label{eq:1} E^2 = \sum_{
    \begin{array}{c} i \in S \\ (i,j) \in T \\ (i,k) \in T \\ j \ne k
    \end{array}} \text{pos} (2 (\vec x_j - \vec x_i) \cdot (\vec x_k - \vec x_i)
+ | \vec x_j - \vec x_i | \cdot | \vec x_k - \vec x_i |)
\end{equation}
%
Which would be used to conclude that we have converged when $E \ll L$.  Here
$\text{pos}(x) = \max(x, 0)$.  The inner part of the function stems from the
calculation of the angle between two vectors.  Say we wish to find the cosine to
angle $v$ between the vectors $\vec a$ and $\vec b$.  This is defined as
%
\begin{align}
  \cos v & = \frac{\vec a \cdot \vec b}
    {|\vec a| \cdot |\vec b|} \label{eq:2}                 \\
  \intertext{If we then wish to find the cosine for $120^{\circ}$}
  \cos 120^{\circ}
         & = -\frac{1}{2} = \frac{\vec a \cdot \vec b}
    {|\vec a| \cdot |\vec b|} \Leftrightarrow \label{eq:3} \\
  0      & = \vec a \cdot \vec b + \frac{1}{2}
    (|\vec a| \cdot |\vec b|) \Leftrightarrow \label{eq:4} \\
  0      & = 2 (\vec a \cdot \vec b) + |\vec a| \cdot |\vec b| \label{eq:5}
\end{align}
%
Thus giving the inner part of Smith's error function, which uses the edges from
the Steiner points as the vectors between which we look at the angle.

How this error function relates to the length $L$ of the tree can be a bit hard
to figure out, but looking at \cref{eq:4} and the same for of the equation for
other angles might give some idea.  If we look at \cref{tab:error-functions} we
see that the angles less than $120^{\circ}$ has a smaller second term, comprised
of the lengths of the vectors.  This means that when we calculate the error
function for $120^{\circ}$ using the two vectors with a smaller angle, that they
will give a positive number instead of zero as would be the case if the angle
was indeed $120^{\circ}$.  In the same way those angles larger than
$120^{\circ}$ will give a negative number, and thus not count towards anything
in \cref{eq:1}.

\begin{table}[htbp] \centering
  \begin{tabular}{ccl}
    \toprule
    $v$           & $\cos v$       & ``error function''                    \\
    \midrule
    $0^{\circ}$   & $1$            & $\vec a \cdot \vec b -
                                     (|\vec a| \cdot |\vec b|)$            \\
    $90^{\circ}$  & $0$            & $\vec a \cdot \vec b$                 \\
    $120^{\circ}$ & $-\frac{1}{2}$ & $\vec a \cdot \vec b +
                                     \frac{1}{2}(|\vec a| \cdot |\vec b|)$ \\
    $180^{\circ}$ & $-1$           & $\vec a \cdot \vec b +
                                     (|\vec a| \cdot |\vec b|)$            \\
    \bottomrule
  \end{tabular}
  \caption[Angles and their corresponding ``error functions'']{Angles smaller
    $\mathit{120^{\circ}}$ clearly means that the error function becomes positive,
    whereas those greater than $\mathit{120^{\circ}}$ will become
    negative.\label{tab:error-functions}}
\end{table}

Note however that while it is now clear that the function is dependent on the
length of the edges, it is still somewhat unclear how it exactly relates to the
entire length of the tree, and why we are looking at $E \ll L$ instead of $E <
\epsilon$ where $0 < \epsilon \ll 1$.\NOTE{Can I leave it at this?}

Smith does not go in to detail with this, and I suspect it may be due to the
fact that he himself is unsure of the validity of the used error function.

\section{Questionable elements of Smiths article}
\label{sec:quest-elem-smiths}

There are a number of elements in the article and the given
implementation~\cite{Smith1992} which are somewhat questionable.  These I will
try to discuss here.

\subsection{Choice of error function}
\label{sec:choice-error-funct}
Where does it come from.  Convergence of some trees might not happen

\subsection{The implementation}
\label{sec:implementation}

\subsubsection{SCALE}
\label{sec:scale}

\subsubsection{tol}
\label{sec:tol}

\subsubsection{Pruning}
\label{sec:pruning}

\subsubsection{Loop condition when optimizing}
\label{sec:loop-condition-when-1}

\subsubsection{If clause when outputting tree}
\label{sec:if-clause-when}

\chapterbreak{}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
