{ \abnormalparskip{0pt}
\chapter{\citeauthor{smith1992}'s algorithm}
\label{cha:algorithm} }

% Short introduction to the chapter (max 1/2 page)

The following chapter will introduce the algorithm for finding \glspl{smt} in
$d$-space, as proposed by \textcite{smith1992}.

The chapter will mainly be concerned with the design of the algorithm, and the
proofs and theorems surrounding it. It will however also delve into the actual
implementation done by \citeauthor{smith1992} when this is relevant.

Finally the chapter discuss some more dubious parts of the implementation done
by \citeauthor{smith1992}.

Apart from the the already used $n$ and $k$, for clarity we define the following symbols
%
\begin{itemize}
\item $R$, the set of terminals.
\item $S$, the set of Steiner points.
\item $\mathcal{T} \equiv E(T)$, the pre-specified topology. Defined as all
  edges which the tree $T$ contains in the form $(i, j)$ where $\vec x_i$ and
  $\vec x_j$ are contained in either $S$ or $R$ and an edge exists in the
  topology from $x_i$ to $x_j$ or vice versa.\footnote{Notice that in
    \cref{cha:preliminaries} we mainly represented and edge between the points
    $a$ and $b$ as $ab$ as this is really simple. However this notation is not
    very good if e.g.\ one point is labelled $l+1$. Thus we adopt this extra
    notation as well, and may use these interchangebly.}
\item $L$, the length of the tree. Defined as the sum of the euclidean lengths
of all edges in the topology $\mathcal{T}$.
\end{itemize}

\section{Overview}
\label{sec:overview}

In general the algorithm follows the following form, proposed by \textcite{gilbert1968}.

\begin{enumerate}
\item Enumerate all Steiner topologies on $n$ terminals and $k$, $0 \le k \le
n-2$ Steiner points.
\item Optimize the coordinates of the Steiner points for each topology, to find
the shortest possible Euclidean embedding (tree) of that topology.
\item Select, and output, the shortest tree found.
\end{enumerate}

In other words this approach might be seen as an exhaustive search of the
solution space, and the approach described is simply to ``try all
possibilities''.

\citeauthor{smith1992} however avoids necessarily having to enumerate and optimize all
topologies, by doing the following: Firstly he only looks at \glspl{fst} as it
turns out we can regard any topology which is non-full, as a \gls{fst} where
some points overlap.

Secondly the algorithm builds the topologies, by a branching process where it
first builds the topology for $3$ terminals (and $1$ Steiner point). From this
is then build all descendants having $4$ terminals (and $2$ Steiner
points). Then again from each of these it builds the descendants having $5$
terminals (and $3$ Steiner points) and etc.\ until it has built all descendants
having $n$ terminals (and $n-2$ Steiner points). The idea behind this way of
building the \glspl{fst} is to allow a branch-and-bound approach where we wish
to prune all descendants of a topology if we have a better upper bound than the
length of the current topology. This process will be described more thoroughly
in \cref{sec:generation}.

The optimization of a topology is done by an iterative process which updates all
Steiner points of the topology every iteration, which \citeauthor{smith1992}
describes as an iteration ``analogous to a Gauss-Seidel
iteration''~\cite[p.~145]{smith1992}.  The equations generated by each iteration
are solved using Gaussian elimination. This iteration and how to solve it is
described in \cref{sec:optim-presp-topol}.

\section{Topologies}
\label{sec:topologies}

The first step of the algorithm is to generate topologies. It is therefore
natural that we need some way of representing and generating these topologies.

The algorithm only considers \glspl{fst}, where $k = n - 2$. This simplification
is allowed, as we can simply regard any Steiner tree with $k \le n - 2$ as a
\gls{fst} where some edges have length zero and thus some points have
``merged''.

An incentive for only looking at \glspl{fst} can be found in \textcite{gilbert1968}
which presents a table, here presented in \cref{tab:number-of-topologies},
containing the number of possible Steiner topologies for different numbers of
terminals and Steiner points. As can clearly be seen the number of \glspl{fst}
(the diagonal) is a lot smaller than the total number of topologies. Thus not
having to optimize the non-full topologies is clearly desirable.

\begin{table}[htbp]
  \centering
  \begin{tabular}{cccccc}
    \toprule
    $S$ & $N = 3$    & $N = 4$    & $N = 5$     & $N = 6$      & $N = 7$      \\
    \midrule
    0   & 3          & 12         & 60          & 360          & 2520         \\
    1   & \textbf{1} & 12         & 120         & 1200         & 12600        \\
    2   &            & \textbf{3} & 75          & 1350         & 22050        \\
    3   &            &            & \textbf{15} & 630          & 17640        \\
    4   &            &            &             & \textbf{105} & 6615         \\
    5   &            &            &             &              & \textbf{945} \\
    \bottomrule
  \end{tabular}
  \caption[Number of possible topologies]{The number of possible topologies for a Steiner tree with $N$
    terminals and $S$ Steiner points.\label{tab:number-of-topologies}}
\end{table}

Note however that even with this simplification the number of \glspl{fst} is
still exponential in $N$, which is clear from \cref{cor:number-of-fsts}.

\subsection{Representation}
\label{sec:representation}

It turns out that every \gls{fst} can be represented using a vector, in
particular we utilize the following theorem put forth by \textcite{smith1992}

\begin{theorem}
  There is an 1--1 correspondence between \glspl{fst} with $n \ge 3$ terminals,
  and $(n-3)$-vectors $\vec{a}$, whose $i$th entry $a_i$ is an integer between
  $1 \le a_i \le 2 i + 1$.
\end{theorem}

Here terminals have indicies $1, 2, \ldots, n$, and Steiner points have indicies
$n + 1, n + 2, \ldots, 2 n - 2$.

The proof of this theorem is done constructively by induction on $n$. It is
clear that the smallest \gls{fst}\footnote{This holds not only for \glspl{fst}
  but for any Steiner topology} we can construct, must have $n = 3$ as the
number of Steiner points is $n - 2 = 1$. Thus we start with the initial null
vector $\vec{a} = ()$ corresponding to the unique \gls{fst} for the terminals
$\{1, 2, 3\}$ connected through the respective edges $\{1, 2, 3\}$ and one
Steiner point $n+1$ as seen in \cref{fig:algorithm-topology-1}. After this
first step, each entry of the topology vector is considered, one at a time,
where the $i$th entry of the topology vector describes the insertion of the
$(n+1+i)$th Steiner point on the edge $a_{i}$ and its connection to the
$(i+3)$th terminal. Thus for the $i$th insertion we will have
$2i+1$\footnote{At the first iteration we clearly have $3$ edges to insert on, and
  as each subsequent insertion generates two new edges we have, after $i$
  insertions, $3 + \underbrace{2 + \cdots + 2}_{2 i} = 2 i + 1$.} different
edges on which we can insert the Steiner point $n + 1 + i$ and connect it to the
regular point $i+3$.

\begin{figure}[htbp] \centering
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-1}
    \caption{The initial null vector.\label{fig:algorithm-topology-1}}
  \end{subfigure}\hspace{1em}%
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-2}
    \caption{Connecting point 4 on edge 2.\label{fig:algorithm-topology-2}}
  \end{subfigure}\hspace{1em}%
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-3}
    \caption{Connecting point 5 on edge 4.\label{fig:algorithm-topology-3}}
  \end{subfigure}
  \caption[Construction of FSTs]{Construction of the \glspl{fst} corresponding
to the vector $\vec{a} = (2, 4)$.\label{fig:algorithm-topologies}}
\end{figure}

Furthermore we get a corollary saying that the number of \glspl{fst} is
exponential in $n$

\begin{corollary}
\label{cor:number-of-fsts}
The number of \glspl{fst} on $n$ terminals is
%
\[
  1 \cdot 3 \cdot 5 \cdots (2n - 7) \cdot (2n - 5) = \prod_{i=0}^{n-3} 2i+1
\]
%
I.e.\ the number of \glspl{fst} is exponential in $n$.
\end{corollary}

Which is clear as we must insert $n-2$ Steiner points, where the null vector is
the $0$th iteration. Thus the last iteration must be $n-3$, and for each
iteration we have $2i+1$ different insertions.

Something which might not be completely obvious from the above proof is, that we
not only generate all topologies, but that we generate all topologies exactly
once. This is important as duplicate topologies could significantly
increase the run time of the algorithm.

\FIXME[inline]{Prove the above statement. I.e.\ that we cannot generate the same
  topology twice. This seems to be possible using \textcite[p.~11--13]{brazil2015}.}

The way \citeauthor{smith1992} chooses to enumerate the edges is not explained outright, but only
in the form of a visual example~\cite[p.~143]{smith1992}. Exactly how one
enumerates the edges on a split is actually not so important, more so is it
important to keep it consistent. The way \citeauthor{smith1992} does it is as in
\cref{fig:algorithm-topologies}. That is when inserting Steiner point $n+1+i$
on the $\text{edge}~a_{i}$ going from $\text{vertex}~a_i$ to $\text{vertex}~j$
with $n < j < n+i+1$, split it such that we get the following three
edges\footnote{An edge is represented as $x = (y, z)$, meaning the edge with
  index $x$ going from the vertex with index $y$ to the vertex with index
  $z$.}

\begin{itemize}
\item edge $a_{i} = (a_{i},N+1+i)$
\item edge $ 2i + 2 = (i+3,N+1+i)$
\item edge $ 2i + 3 = (j,N+1+i)$
\end{itemize}

This is also the way the the new implementation enumerates the edge.

\subsection{Generation}
\label{sec:generation}

Using the representation described in \cref{sec:representation} the problem of
generating all topologies can now be done as a backtracking problem generating
all $(n-3)$-topology vectors. An example of how the generation works can be seen
in \cref{fig:topology-sprouting}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.8\textwidth]{gfx/tikz/topology-sprouting}
  \caption[Splitting of an edge]{Upon splitting an edge and connecting a new
    terminal to the topology the algorithm can perform the split on any of the
    existing edges. Thus in this case the algorithm branch in to three
    different topologies. In general the splitting result in the topology
    branching into as many branches as there are edges in the topology on which
    the split is perfomed.\label{fig:topology-sprouting}}
\end{figure}

To further speed up the generation of topologies, or rather to avoid generating
unnecessary topologies, \citeauthor{smith1992} also utilizes the following theorem
%
\begin{theorem}
  For any set of $n$ distinct terminals in any euclidean space, the length of
  the shortest tree, interconnecting $n-1$ points, with topology vector
  $a_1 \cdots a_{n-4}$ is no greater than the length of the shortest tree,
  interconnecting $N$ points, with topology vector $a_1 \cdots a_{n-3}$.
\end{theorem}
%
The above theorem is easily seen to be true, simply by considering removing the
edge $e$ connecting the last terminal $n$ to the rest of the tree. This will
obviously shorten the tree, or if the edge was length zero it would remain the
same. Furthermore upon optimizing the tree with the last point removed, we
would either get a further shortening, or it would remain the same.

The algorithm utilizes this to prune in the following way. Imagine we have
found some upper bound for the \gls{smt}. If we then optimize any generated
topology vector which does not yet include all the terminals, and it turns out
to have length greater than the upper bound, we can prune any topologies that we
would have generated from this vector, as the length of the larger topologies
cannot become any smaller than the length of the current, and thus cannot become
smaller than the length of the upper bound.

This way of pruning means the algorithm must generate and optimize topologies
depth-first to get a good upper bound as quickly as possible. If it was instead
breadth-first, nothing would  be left to prune anything, as all the \glspl{fst}
would be the last topologies on which we optimize, and these are the only ones
that can give a new upper bound.

The actual implementation of the backtracking is not described by the article
part of \textcite{smith1992}, but only by the source code in the appendix. The
details of the current implementation will be discussed in
\cref{cha:implementation}. The implementation does face some problems, one of
which is the fact that the entire topology is rebuilt from scratch every time we
need to either add of remove a point.

\section{Optimization of a prespecified topology}
\label{sec:optim-presp-topol}

When a topology has been found, we need a way of optimizing the positions of the
Steiner points. The approach taken by \citeauthor{smith1992} is in general to create an
iteration with $n-2$ equations, one for each Steiner point, and incrementally
optimize all of them every iteration. This is done by solving the equations
using Gauss elimination. The section will be concerned with describing the
iteration, the proof of correctness and convergence, the speed of convergence
and the error function for deciding convergence.

\subsection{Simple iteration}
\label{sec:simple-iteration}

To first get a sense of how we might go about optimizing the tree in a simple
manner we first look at a iteration which only optimizes one Steiner point at a
time. This is a simpler approach to the one proposed by \textcite{smith1992},
but according to \citeauthor{smith1992} will probably also take longer time to
converge. The iteration is described by \cref{alg:simple-iteration}. The
algorithm is intentionally a bit vague, as even the simple iteration has several
choices we can make.

\begin{algorithm}[htbp]
  \KwData{
    \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,label=-,leftmargin=10pt]
    \item A \gls{fst} containing the Steiner points $S$, terminals $R$, and edges $E$
    \item A small positive number $\epsilon$
    \end{itemize}
  }
  \KwResult{The optimized \gls{fst}}
  \ForEach{$s \in S$}{
    $\textit{error} \gets \infty$\;
    \While{$\textit{error} > \epsilon$}{
      optimize $s$ with respect to its neighbors in $T$\;
      recalculate \textit{error}\;
    }
  }
  \Return{$T$}
  \caption[Simple iteration]{Pseudo code describing the optimization strategy
    using the simple iteration.\label{alg:simple-iteration}}
\end{algorithm}

First of we should choose how a single Steiner point is optimized. As earlier
described this is the same as the Fermat-Torricelli problem, and thus we can go
about finding the Steiner point in all the same ways.

The problem \citeauthor{smith1992} poses for the simple iteration described here is that while it
quickly becomes optimal \textit{locally} it may still converge slowly as it does not
regard the \textit{global} structure.

As an iteration, using an analytical solution to the Fermat-Torricelli problem
described by \textcite{uteshev2014}, has actually been implemented the above
statement will be further discussed in \cref{cha:experiments} and \cref{cha:discussion}.

\subsection{\citeauthor{smith1992}'s iteration}
\label{sec:smiths-iteration}

Now that we have looked at a simple way of optimizing the topology, by finding
the coordinates of one Steiner point at a time, we can progress to the iteration
proposed by \textcite{smith1992}. This iteration optimizes all points in every iteration.

To optimize the topology \citeauthor{smith1992} proposes iteratively solving the $n-2$
equations\footnote{Note that in the second sum of \cref{eq:6} \textcite{smith1992} gives
  the index $j: jk \in T, j \in S$. For the rewrite to make sense, this should
one have been $j: jk \in T$. Thus I believe this is just a typo and have
progressed as such.}
%
\begin{alignat}{2}
  \vec x_l^{(i+1)}
  &= \frac{\sum_{j : (j,l) \in \mathcal{T}}
    \frac{\vec x_j^{(i+1)}}{|\vec x_l^{(i)} - \vec x_j^{(i)}|}}
    {\sum_{j: (j,l) \in \mathcal{T}}
    \frac{1}{|\vec x_l^{(i)} - \vec x_j^{(i)}|}}, & \quad
    n + 1 &\le l \le 2 n - 2 \label{eq:6}
\intertext{which can be rewritten as}
\vec 0
  &= \sum_{j: (j,l) \in \mathcal{T}}
    \frac{\vec x_l^{(i+1)} - \vec x_j^{(i+1)}}
    {|\vec x_l^{(i)} - \vec x_j^{(i)}|}, & \quad
     n + 1 &\le l \le 2 n - 2 \label{eq:9}
\end{alignat}
%
Here $\vec x_l \in S$ represents the $l$-th Steiner point, being a
$d$-vector. Iteratively solving this system from the initial coordinates
$x^{(0)}_l$, the iteration will converge towards the unique optimum Steiner
point coordinates that minimize the total length, $L$\footnote{The tree length
  is as is normally the case calculated as the sum of all edge lengths. In this
  case we use the regular Euclidean ($\mathcal{L}_2$) norm. One could however think to
  possibly use other norms, e.g.\ the rectilinear ($\mathcal{L}_1$) norm.}, of the tree. The
convergence happens in such a way that the sequence of tree lengths $L(S^{(i)})$
is monotonically decreasing. Note that since the unknowns in the iteration are
vectors we actually have $d$ independent equation systems of $n-2$ equations, $d$ being the
dimension.

The proof that this iteration actually converges to an optimal solution is done
as following. Firstly after each ($i$th) iteration, $S^{(i+1)}$ exactly
minimizes the following quadratic form $Q^{(i)}(S)$
%
\begin{equation}
  Q^{(i)}(S) = \sum_{(j,l) \in \mathcal{T}, l \in S, j < l }
  \frac{|\vec x_l - \vec x_j|^2}{|\vec x^{(i)}_l - \vec x^{(i)}_j|}
\end{equation}
%
It is easily seen that $Q(S)$ is related to the length $L(S)$ as follows
%
\begin{equation}
  L(S^{(i)}) = Q^{(i)}(S^{(i)}), \quad \nabla L(S^{(i)}) \propto \nabla Q^{(i)}(S^{(i)})
\end{equation}

Secondly using \textcite{gilbert1968}'s mechanical model one can think of $Q$ as
the potential energy of a system of ideal springs on the tree edges, where the
force constant of each spring is proportional to the reciprocal of its original
length before each iteration. The $i$th iteration causes all springs to relax,
minimizing $Q^{(i)}$. Furthermore as can be seen from \cref{eq:9} the forces of
the $l$th Steiner point are clearly in equilibrium. Now write
%
\begin{align}
  L(S^{(i)}) &= Q^{(i)}(S^{(i)}) \ge Q^{(i)}(S^{(i+1)}) \\
             &= \sum_{j, l : j < l, (j,l) \in \mathcal{T}, l \in S} \frac{{(
               |\vec x_l^{(i)} - \vec x_j^{(j)}| + |\vec x_l^{(i+1)} -
               \vec x_j^{(i+1)}| - |\vec x_l^{(i)} - \vec x_j^{(j)}|)}^2}{
               |\vec x_l^{(i)} - \vec x_j^{(i)}|} \\
             &= L(S^{(i)}) + 2 [ L(S^{(i+1)}) - L(S^{(i)})] \\
             &+ \sum_{j, l : j < l, (j,l) \in \mathcal{T}, l \in S} \frac{{(
               |\vec x_l^{(i+1)} -
               \vec x_j^{(i+1)}| - |\vec x_l^{(i)} - \vec x_j^{(j)}|)}^2}{
               |\vec x_l^{(i)} - \vec x_j^{(i)}|}
\end{align}
%
It is obvious that the last sum is non-negative, and thus
$L(S^{(i)}) \ge L(S^{(i+1)})$. \citeauthor{smith1992} furthermore goes into detail on how the
only stable fixed point is the optimum, and thus even if the iteration reaches
one such point, any small perturbation of $S$ in any $L$-decreasing direction
will cause the iteration to resume decreasing $L$ until it has been
minimized. The exact details of this part will not be covered here, but can be
found in~\cite[p.~147--148]{smith1992}.

The equation system of each iteration can be solved using Gaussian elimination.
This is done by identifying an equation which only has at most one unknown on the right
hand side. This corresponds to a Steiner point being connected to two or more
terminals, which is always guaranteed to exist. Thus if we use a $\mathcal
O(N)$ algorithm for solving a single system and we actually have $d$ systems, we
can solve a single iteration in $\mathcal{O}(n d)$.

According to \textcite[p.~150]{smith1992} the iteration has a good speed of
convergence. In most cases geometric convergence will occur. However some
degenerate configurations of terminals can cause sublinear convergence.

\section{Questionable elements of \citeauthor{smith1992}'s article}
\label{sec:quest-elem-smiths}

There are a number of elements in the article and the given
implementation~\cite{smith1992} which are somewhat questionable. These I will
try to discuss here.

\subsection{Choice of error function}
\label{sec:choice-error-funct}

To detect when convergence has happened \citeauthor{smith1992} proposes using a special
error function instead of the one often used where one stops when the
improvements from iteration to iteration falls below some $\epsilon$. The
function proposed by \citeauthor{smith1992} is
%
\begin{equation}
  E^2 = \sum_{
    \begin{array}{c} i \in S \\ (i,j) \in T \\ (i,l) \in T \\ j \ne l
    \end{array}} \text{pos} (2 (\vec x_j - \vec x_i) \cdot (\vec x_l - \vec x_i)
+ | \vec x_j - \vec x_i | \cdot | \vec x_l - \vec x_i |)
\end{equation}
%
Here $\text{pos}(x) = \max(x, 0)$. This error function sums together all angles which
are smaller than $120^{\circ}$. The error function is derived from calculating
the angle between edges in the following way:
%
\begin{align}
  \cos v & = \frac{\vec a \cdot \vec b}
    {|\vec a| \cdot |\vec b|}                 \\
  \intertext{If we then wish to find the cosine for $120^{\circ}$}
  \cos 120^{\circ}
         & = -\frac{1}{2} = \frac{\vec a \cdot \vec b}
    {|\vec a| \cdot |\vec b|} \Leftrightarrow \\
  0      & = \vec a \cdot \vec b + \frac{1}{2}
    (|\vec a| \cdot |\vec b|) \Leftrightarrow \\
  0      & = 2 (\vec a \cdot \vec b) + |\vec a| \cdot |\vec b|
\end{align}
%
Thus any angle less than $120^{\circ}$ gives a positive number when inserted in
the function, and any angle below will give a negative number (which is set to
zero by $\text{pos}(x)$).

Why this error function is supposed to be better is not explained very
thoroughly by \citeauthor{smith1992}. It is only referred to that doing some
experimentation which led him to believe that this would be a good function. The
idea behind the function seems to be that, as the final tree should only have
angles of $120^{\circ}$ any angle less than this must still need to be
optimized.

There are however some problems with this choice of error function. Consider
the configuration as shown in the first tree of \cref{fig:error-function}. Here
there are two pairs of terminals, very far apart. These are connected by two
Steiner points as seen. Running \citeauthor{smith1992}'s iteration the two Steiner points will
move together, and up towards the center of the rectangle of the four
terminals. However suppose the two points reach each other before they reach
the center. In this case the angles on the left and right will be more than
$120^{\circ}$ and thus not count towards the error. Furthermore the edge
between the Steiner points become zero (or practically zero). This means, that
even though the angles using this edge all are less than $120^{\circ}$ and would
count towards the error, the error they contribute with is zero, which is clear
by looking at the definition of the error.

This means that the algorithm will stop before it should with a tree which has
not been minimized.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{gfx/tikz/errorfunction}
  \caption[Possible problem with the error function]{The left part of the figure
    shows how the use of the current error function might result in a suboptimal
    tree. The right part of the figure shows the optimal tree. Notice that
    this topology does not result in the \gls{smt} for these
    terminals.\label{fig:error-function}}
\end{figure}

Note however that the topology used in \cref{fig:error-function} is not the
one which gives the \gls{smt} for the four terminals. The optimal topology
would have the top pair connected with one Steiner point, the two bottom
terminals connected with another Steiner point, and those two Steiner points
connected. This topology when optimized will not face the same problem, as the
two Steiner points will not move towards each other. Thus it is still unclear
whether this problem can occur with a topology which optimizes to a \gls{smt}.
Should this be the case then the algorithm cannot actually be considered an
exact algorithm, but only a approximation algorithm.

\subsection{The implementation}
\label{sec:implementation}

The implementation by \citeauthor{smith1992} contains several unexplained variables and conditionals not
directly related to the algorithm as it is explained in the article by \textcite{smith1992}.

\subsubsection{SCALE}
\label{sec:scale}

The variable \texttt{SCALE} is used by \citeauthor{smith1992} when calculating the initial
placement of a Steiner point. Steiner points are initially placed at the
centroid of its three neighbors. However the point is not placed exactly at the
centroid, but is instead pertubed slightly. To each coordinate of the centroid
is added $0.001 \cdot \texttt{SCALE} \cdot \texttt{RANDM()}$, where \texttt{RANDM()} is a
random number between $0$ and $1$. \texttt{SCALE} is a number defined as shown
in \cref{fig:randm}.

\begin{figure}[htbp]
\begin{c-code}
SCALE = 0.0;
for (i = 1; i <= NUMSITES; i++) {
  for (j = 0; j < DIMENSION; j++) {
    q = XX[i][j] - XX[1][j];
    if (q < 0.0) q = -q;
    if (q > SCALE) SCALE = q;
    printf(" %g", XX[i][j]);
  }
  printf("\n");
}
printf("SCALE = %g\n", SCALE);
\end{c-code}
  \captionof{listing}[Use of \texttt{SCALE} in \citeauthor{smith1992}'s implementation]{The implementation
    of \texttt{SCALE} in the original implementation. As can be seen it only
    compares the dimensions of the first terminal with the others, and not every
    terminal with every other terminal.\label{fig:randm}}
\end{figure}

As can be seen the variable finds the largest difference in coordinates between the first
terminal and all other terminals. Why this extra variable should be a part of
the perturbation is however unclear. The name of the variable might be a hint,
as it seems \citeauthor{smith1992} wished to scale the perturbation in correspondence to the
points. If this is the case however the code in \cref{fig:randm} seems to be
faulty as it should then not only compare the first point to all other, but
instead compare every point, to every other.

Thus it seems that the variable is not only not needed, but actually faulty.
This of course depends on what \citeauthor{smith1992} is trying to achieve with \texttt{SCALE}.
But overall the variable seems unneeded, and thus the new implementation does
not use \texttt{SCALE}.

\subsubsection{tol}
\label{sec:tol}

\texttt{tol} is the parameter given to the optimize function and is described as
a small positive number. In the code whenever the function is called the
variable has the value $0.0001*r/\text{NUMSITES}$ where $r$ is the current
error and and $\text{NUMSITES}$ is the number of terminals, $n$. Having some
small positive number in the optimize function makes sense, as it is used to
avoid divisions with zero (by adding it to the edge lengths, some of which may
be zero). However the specific choice which is both dependent on the error and
the number of terminals seems selected somewhat at random. A perhaps more
obvious choice would be some small \textit{fixed} number, e.g.\ just $0.0001$.

\TODO[inline]{run different trees with this random factor, and with a fixed
  random factor and see if it makes a difference}
\NOTE[inline]{Current experimentation shows that a fixed tol can make the
  iteration move very slowly when small optimizations has to be made. This might
be an argument for letting it depend on the error and N. The exact numbers seem
to originate from experiments though.}

\subsubsection{Pruning}
\label{sec:pruning}

\TODO[inline]{Write about the way the main loop runs and how it prunes. It seems
to skip some trees it should not.}

\subsubsection{Loop condition when optimizing}
\label{sec:loop-condition-when-1}

In the main function when performing the optimization of a topology, the loop
condition seems to suffer from the same arbitrariness as both \texttt{tol} and
\texttt{SCALE}. Depending on whether we are optimizing a regular topology, or a
topology with all $n-2$ Steiner points the condition is either $r > 0.005 \cdot
q$ or $r > q \cdot 0.0001$, $r$ being the error and $q$ the tree length. The
problem here is that there is no direct relation between the error and the
length of the tree, and as such this loop condition seems very arbitrary. While
it seems to work in practice, it seems that a better loop condition would be
when the error falls below some small number $\epsilon$.

\subsubsection{If clause when outputting tree}
\label{sec:if-clause-when}

This is most probably a bug. When a full topology with $N-2$ Steiner points is
optimized its length is compared to the current upper bound, \texttt{STUB}. If
it is better than the upper bound, this is updated to the newly found tree
length. However the tree is only outputted as a record if its length is smaller
than the previous upper bound times $0.99999$. This means we risk situations
where the upper bound is updated, but the tree never outputted. Most likely
this extra check should have been removed. As we are dealing with floating
point arithmetic doing the multiplication might make sense in the initial check,
however the double check is faulty and may lead to wrong behavior. The faulty
snippet and its possible corrections can be found in \cref{fig:if-clause-snippet}.

\begin{figure}[htbp]
\begin{c-code}
/* The faulty snippet */
if (q < STUB) {
  printf("\nnew record length %20.20g\n", q);
  for (i = 1; i <= k; i++) BESTVEC[i] = A[i];
  if (q < STUB*0.99999) output_tree();
  STUB = q;
}

/* First possible correction */
if (q < STUB) {
  printf("\nnew record length %20.20g\n", q);
  for (i = 1; i <= k; i++) BESTVEC[i] = A[i];
  output_tree();
  STUB = q;
}

/* Second possible correction */
if (q < STUB*0.99999) {
  printf("\nnew record length %20.20g\n", q);
  for (i = 1; i <= k; i++) BESTVEC[i] = A[i];
  output_tree();
  STUB = q;
}
\end{c-code}
  \captionof{listing}[Error in \citeauthor{smith1992}'s implementation when outputting trees]{The
    second conditional in the first code snippet may result in some \gls{smt}
    being set as upper bound, but outputted. This is fixed by both of the two
    next snippets.\label{fig:if-clause-snippet}}
\end{figure}

\chapterbreak{}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
