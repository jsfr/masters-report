{
\abnormalparskip{0pt}
\chapter{Smiths algorithm}
\label{cha:algorithm}
}

% Short introduction to the chapter (max 1/2 page)

The following chapter will introduce the main focus of this thesis, namely the
algorithm for finding \glspl{smt} in $d$-space proposed by
Smith~\cite{Smith1992}.

The chapter will mainly be concerned with the algorithmic design of the
algorithm, and the proofs and theorems surrounding it. It will however also
delve into more some parts of the implementation done by Smith when this is
relevant.

\section{Overview}
\label{sec:overview}

In general the algorithm follows the following form, proposed by Gilbert and
Pollak~\cite{Gilbert1968}.

\begin{enumerate}
\item Enumerate all Steiner topologies on $N$ regular points and $K$, $0 \le K
  \le N-2$ Steiner points.
\item Optimize the coordinates of the Steiner points for each topology, to find
  the shortest possible Euclidean embedding (tree) of that topology.
\item Select, and output, the shortest tree found.
\end{enumerate}

In other words this approach might be seen as an exhaustive search of the
solution space, and the approach described is simply to ``try all
possibilities''.

Smith however does some things a little differently to avoid having to optimize
every topology. Firstly he only looks at \glspl{fst} as it turns out we can
regard any topology which is non-full, as a \gls{fst} where some points overlap.

Secondly the algorithm not only finds the topologies for $N$ regular points, but
for $3, 4, \ldots, N$ regular points, all as \glspl{fst} having
$1, 2, \ldots, N-2$ Steiner points. The reason for this is to allow a \gls{bnb}
approach where we wish to prune all descendants of a topology if we have a
better upper bound than the length of the current topology.

The optimization of the algorithm is done by a iterative process which updates
all Steiner points of a topology every iteration, by Smith described as an
iteration analogous to a Gauss-Seidel iteration~\cite[p.~145]{Smith1992}. The
equations of each iteration are solved using Gauss-elimination.

\NOTE[inline]{Am I missing something in the overview?}

\section{Topologies}
\label{sec:topologies}

The first step of the algorithm is, as described in \Cref{sec:overview}, to
generate topologies. It is therefore natural that we need some way of
representing and generating these topologies.

The algorithm only considers \glspl{fst} where $K = N - 2$. This simplification
is allowed, as we can simply regard any Steiner tree with $K \le N - 2$ as a
\gls{fst} where some edges have length zero and thus some points have
``merged''.

Note however that even with this simplification the number of \glspl{fst} is
still exponential in $N$, which is clear from \Cref{cor:number-of-fsts}.

\subsection{Representation}
\label{sec:representation}

It turns out that every \gls{fst} can be represented using a vector, in
particular we utilize the following theorem

\begin{theorem}
There is an 1--1 correspondence between full Steiner topologies with $N \ge 3$
regular points, and $(N-3)$-vectors $\vec{a}$, whose $i$th entry $a_i$ is an
integer between $1 \le a_i \le 2 i + 1$.
\end{theorem}

The proof of this theorem is done constructively by induction on $N$. It is
clear that the smallest \gls{fst}, or any Steiner topology for that matter, we
can construct must have $N = 3$, as the number of Steiner points is $N - 2 = 1$.
Thus we start with the initial null vector $\vec{a} = ()$ corresponding to the
unique \gls{fst} for the points 1, 2 and 3 connected through the respective
edges 1, 2 and 3 and one Steiner point $N+1$ as seen in
\Cref{fig:algorithm-topology-1}. After this first step, each entry of the
topology vector is considered, one at a time, where the $i$th entry of the
topology vector describes the insertion of the $(N+1+i)$th Steiner point on the
edge $a_{i}$ and its connection to the $(i+3)$th regular point. Thus for the
$i$th insertion we will have $2i+1$\footnote{At the first iteration we clearly
  have 3 edges to insert on, and as each insertion generates two new edges we
  have $3 + \underbrace{2 + \cdots + 2}_{2 (i - 1)} = 2 i + 1$.} different edges
on which we can insert the Steiner point $N+1+i$ and connect it to the regular
point $i+3$.

\begin{figure}[htbp]
\centering
  \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-1}
    \caption{The initial null vector.\label{fig:algorithm-topology-1}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-2}
    \caption{Connecting point 4 on edge 2.\label{fig:algorithm-topology-2}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-3}
    \caption{Connecting point 5 on edge 4.\label{fig:algorithm-topology-3}}
  \end{subfigure}
  \caption[Construction of FSTs]{Construction of the \glspl{fst} corresponding
    to the vector $\vec{a} = (2, 4)$.\label{fig:algorithm-topologies}}
\end{figure}

Furthermore we get a corollary saying that the number of \glspl{fst} is
exponential in $N$

\begin{corollary}
\label{cor:number-of-fsts}
The number of \glspl{fst} on $N$ regular points is
\[\prod_{i=0}^{N-3} 2i+1 = 1 \cdot 3 \cdot 5 \cdots (2N -
5)\] I.e\ the number of \glspl{fst} is exponential in $N$.
\end{corollary}

Which is clear as we must insert $N-2$ Steiner points, where the null vector is
the $0$th iteration. Thus the last iteration must be $N-3$, and for each
iteration we have $2i+1$ different insertions.

The way Smith chooses to enumerate the edges is not explained outright, but only
in the form of a visual example~\cite{Smith1992}. However one must take care to
keep the enumeration consistent to avoid generating the same topologies more
than once. Thus we do the enumeration in the same way as Smith, and as in
\Cref{fig:algorithm-topologies}. That is when we insert Steiner point $N+1+i$ on
the edge $a_{i} = (a_{i}, j), N < j < N+i+1$, we split it such that we get the
following three edges
%
\begin{itemize}
\item edge $a_{i} = (a_{i},N+1+i)$
\item edge $ 2i + 2 = (i+3,N+1+i)$
\item edge $ 2i + 3 = (j,N+1+i)$
\end{itemize}

\subsection{Generation}
\label{sec:generation}

Using the representation described in \Cref{sec:representation} the problem of
generating all topologies can now be done as a backtracking problem generating
all $(N-3)$-topology vectors.

To further speed up the generation of topologies, or rather to avoid generating
unnecessary topologies, Smith also utilizes the following theorem

\begin{theorem}
For any set of $N$ distinct regular points in any Euclidean space, the length of
the shortest tree, interconnecting $N-1$ points, with topology vector $a_1
\cdots a_{N-4}$ is no greater than the length of the shortest tree,
interconnecting $N$ points, with topology vector $a_1 \cdots a_{N-3}$.
\end{theorem}

The above theorem is easily seen to be true, by simply considering removing the
edge $e$ connecting the last regular point $N$, to the rest of the tree. This
will obviously shorten the tree, or if the edge was length zero\footnote{This
  might happen if the points are either not in general position, or if the point
  removed lies exactly on top of a Steiner point. The last part seems to have
  been missed by Smith~\cite[p.~144]{Smith1992}.} it would remain the
same. Furthermore upon optimizing the tree with the last point removed we would
either get a further shortening, or it would remain the same.

The algorithm utilizes this to prune in the following way. Imagine we have found
some upper bound for the \gls{smt}. If we then optimize any generated topology
vector which does not yet include all the regular points, and it turns out to
have length greater than the upper bound, we can prune any topologies that we
would have generated from this vector, as the length of the larger topologies
cannot become any smaller than the length of the current, and thus cannot become
smaller than the length of the upper bound.

Thus the implementation of the algorithm generates and optimizes topologies
depth-first to ensure we get an upper bound as quickly as possible. If it did
breadth-first we would not be able to prune anything, as we would get all the
full topologies as the last to optimize.

The actual implementation of the backtracking is not described by Smith in
article itself, but only by reading the code. The details of the current
implementation will be discussed in\missingref{refer to implementation and the
  correct section.}. The implementation does face some problems, one of which is
the fact that the entire topology is rebuilt from scratch every time we need to
either add of remove a point.

\section{Optimization of a prespecified topology}
\label{sec:optim-presp-topol}

When a topology has been found, we need a way of optimizing the positions of the
Steiner points. The approach taken by Smith is in general to create an iteration
with $N-2$ equations, one for each Steiner point, and incrementally optimize all
of them every iteration. This is done by solving the equations using Gauss
elimination. The section will be concerned with describing the iteration, the
proof of correctness and convergence, the speed of convergence and the error
function for deciding convergence.

Apart from the the already used $N$ and $K$, we define the following symbols as
well
\begin{itemize}
\item $R$, the set of regular points.
\item $S$, the set of Steiner points.
\item $T$, the pre-specified topology. Defined as all edges it contains in the
  form $(j,k)$ where $\vec x_j$ and $\vec x_k$ are contained in either $S$ or
  $R$ and an edge exists in the topology from $x_j$ to $x_k$ or vice versa.
\item $L$, the length of the tree. Defined as the sum of the euclidean lengths
  of all edges in in the topology $T$. See \Cref{eq:length} for the exact
  definition.
\end{itemize}

\FIXME[inline]{Smith is inconsistent with having the sets as containing either
  the points of the indicies. This we should probably fix up just to clear the
  notation.}

\subsection{The iteration}
\label{sec:iteration}

To optimize the topology Smith proposes iteratively solving the $N-2$ equations
%
\begin{equation}
  \label{eq:6}
  \vec x_k^{(i+1)} = \left. \sum_{j : (j, k) \in T}
    \frac{\vec x_j^{(i+1)}}{|\vec x_j^{(i)} - \vec x_k^{(i)}|}
    \hspace{0.5em}\middle/\hspace{-0.5em} \sum_{j: (j,k)
      \in T, j \in S} \frac{1}{|\vec x_k^{(i)} - \vec x_j^{(i)}|} \right.
\end{equation}
%
Where $ N+1 \le k \le 2N-2$
%
\begin{equation}
L(\vec x_{N+1}, \ldots, \vec x_{2N-2}) =
\sum_{\begin{array}{c}
j,k: 1 \le j < k \le 2 N - 2 \\
(j,k) \in T
\end{array}}
| \vec x_{j} - \vec x_{k} |
\label{eq:length}
\end{equation}
%
\TODO[inline]{Write about the iteration and the proof of correctness}

\subsection{Convergence}
\label{sec:convergence}
\NOTE[inline]{Do I need to touch on this? If so it is about the speed and proof
  of convergence}

\subsection{Error function}
\label{sec:error-function}

To detect when convergence has happened, Smith warns about using the obvious way
of testing whether one iterate differs substantially from the next and stop if
it does not~\cite[p.~151]{Smith1992}\NOTE{why does he warn about this? Should we
give an example where this could go wrong?}. Instead Smith proposed using an
error-function which looks at all angles smaller than $120^{\circ}$, as an
\gls{mst}, if it is also a \gls{fst}, will only have angles of
$120^{\circ}$. The function Smith proposed is
%
\begin{equation}
  \label{eq:1}
  E^2 = \sum_{
    \begin{array}{c}
      i \in S \\
      (i,j) \in T \\
      (i,k) \in T \\
      j \ne k
    \end{array}}
  \text{pos} (2 (\vec x_j - \vec x_i) \cdot (\vec x_k - \vec x_i) +
  | \vec x_j - \vec x_i | \cdot | \vec x_k - \vec x_i |)
\end{equation}
%
Which would be used to conclude that we have converged when $E \ll L$. Here
$\text{pos}(x) = \max(x, 0)$. The inner part of the function stems from the
calculation of the angle between two vectors. Say we wish to find the cosine to
angle $v$ between the vectors $\vec a$ and $\vec b$. This is defined as
%
\begin{align}
  \cos v           & = \frac{\vec a \cdot \vec b}
                     {|\vec a| \cdot |\vec b|} \label{eq:2} \\
  \intertext{If we then wish to find the cosine for $120^{\circ}$}
  \cos 120^{\circ} & = -\frac{1}{2} = \frac{\vec a \cdot \vec b}
                     {|\vec a| \cdot |\vec b|} \Leftrightarrow \label{eq:3} \\
  0                & = \vec a \cdot \vec b +
                     \frac{1}{2} (|\vec a| \cdot |\vec b|)
                     \Leftrightarrow \label{eq:4} \\
  0                & = 2 (\vec a \cdot \vec b)
                     + |\vec a| \cdot |\vec b| \label{eq:5}
\end{align}
%
Thus giving the inner part of Smiths error function, which uses the edges from
the Steiner points as the vectors between which we look at the angle.

How this error function relates to the length $L$ of the tree can be a bit hard
to figure out, but looking at \Cref{eq:4} and the same for of the equation for
other angles might give some idea. If we look at \Cref{tab:error-functions} we
see that the angles less than $120^{\circ}$ has a smaller second term, comprised
of the lengths of the vectors. This means that when we calculate the error
function for $120^{\circ}$ using the two vectors with a smaller angle, that they
will give a positive number instead of zero as would be the case if the angle
was indeed $120^{\circ}$. In the same way those angles larger than $120^{\circ}$
will give a negative number, and thus not count towards anything in \Cref{eq:1}.

\begin{table}[htbp]
  \centering
  \begin{tabular}{ccl}
    \toprule
    $v$           & $\cos v$       & ``error function''                     \\
    \midrule
    $0^{\circ}$   & $1$            & $\vec a \cdot \vec b -
                                     (|\vec a| \cdot |\vec b|)$             \\
    $90^{\circ}$  & $0$            & $\vec a \cdot \vec b$                  \\
    $120^{\circ}$ & $-\frac{1}{2}$ & $\vec a \cdot \vec b +
                                     \frac{1}{2} (|\vec a| \cdot |\vec b|)$ \\
    $180^{\circ}$ & $-1$           & $\vec a \cdot \vec b +
                                     (|\vec a| \cdot |\vec b|)$             \\
    \bottomrule
  \end{tabular}
  \caption[Angles and their corresponding ``error functions'']{Angles smaller
    $\mathit{120^{\circ}}$ clearly means that the error function becomes
    positive, whereas those greater than $\mathit{120^{\circ}}$ will become
    negative.\label{tab:error-functions}}
\end{table}

Note however that while it is now clear that the function is dependent on the
length of the edges, it is still somewhat unclear how it exactly relates to the
entire length of the tree, and why we are looking at $E \ll L$ instead of
$E < \epsilon$ where $0 < \epsilon \ll 1$.\NOTE{Can I leave it at this?}

Smith does not go in to detail with this, and I suspect it may be due to the
fact that he himself is unsure of the validity of the used error function.

\chapterbreak{}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
