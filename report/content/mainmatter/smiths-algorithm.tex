{ \abnormalparskip{0pt}
\chapter{\citeauthor{smith1992}'s Algorithm}
\label{cha:algorithm} }

% Short introduction to the chapter (max 1/2 page)

This chapter will introduce the algorithm for finding \acp{smt} in
$d$-space, as proposed by \textcite{smith1992}.

The chapter will mainly be concerned with the design of the algorithm, and the
proofs and theorems surrounding it. It will however also delve into the actual
implementation done by \citeauthor{smith1992} when this is relevant.

Finally the chapter discuss some more dubious parts of the implementation done
by \citeauthor{smith1992}.

Apart from the already used $n$ and $k$, for clarity we define the following symbols
%
\begin{itemize}
\item $R$, the set of terminals.
\item $S$, the set of Steiner points.
\item $\mathcal{T} \equiv E(T)$, the pre-specified topology. Defined as all
  edges which the tree $T$ contains in the form $(i, j)$ where $p_i$ and
  $p_j$ are contained in either $S$ or $R$ and an edge exists in the
  topology from $p_i$ to $p_j$ or vice versa.\footnote{Notice that in
    \cref{cha:preliminaries} we mainly represented and edge between the points
    $p_a$ and $p_b$ as $ab$ as this is really simple. However this notation is not
    very good if e.g.\ one point is labeled $p_{l+1}$. Thus we adopt this extra
    notation as well, and may use these interchangeably.}
\item $L$, the length of the tree. Defined as the sum of the euclidean lengths
of all edges in the tree.
\end{itemize}

\section{Overview}
\label{sec:overview}

In general the algorithm follows the following form, proposed by \textcite{gilbert1968}.

\begin{enumerate}
\item Enumerate all Steiner topologies on $n$ terminals and $k$, $0 \le k \le
n-2$ Steiner points.
\item Optimize the coordinates of the Steiner points for each topology, to find
the shortest possible Euclidean embedding (tree) of that topology.
\item Select, and output, the shortest tree found.
\end{enumerate}

In other words this approach might be seen as an exhaustive search of the
solution space, and the approach described is simply to ``try all
possibilities''.

\citeauthor{smith1992} however avoids necessarily having to enumerate and optimize all
topologies, by doing the following: Firstly he only looks at \acp{fst} as it
turns out we can regard any topology which is non-full, as a \ac{fst} where
some points overlap.

Secondly the algorithm builds the topologies, by an enumeration process where it
first builds the topology for $3$ fixed terminals\footnote{By ``fixed
  terminals'' we mean that the order in which the terminals are added is fixed,
  i.e.\ even though we where we add the next terminal, the $i$th terminal added
  to the topology will always be the same.} (and $1$ Steiner point). From
this it then builds all descendants having $4$ fixed terminals (and $2$ Steiner
points). Then again from each of these it builds the descendants having $5$ fixed
terminals (and $3$ Steiner points), etc.\, until it has built all descendants
having $n$ fixed terminals (and $n-2$ Steiner points). The idea behind this way of
building the \acp{fst} is to allow a branch-and-bound approach where we wish to
prune all descendants of a topology if we have an upper bound with a length,
lower than the length we get when optimizing the tree of the underlying
topology.  This process will be described more thoroughly in
\cref{sec:generation}.

The optimization of a tree is done by an iterative process which updates all
Steiner points of the tree every iteration, which \citeauthor{smith1992}
describes as an iteration ``analogous to a Gauss-Seidel
iteration''~\cite[p.~145]{smith1992}. The equations, using the coefficients
generated by each iteration are solved using Gaussian elimination. This
iteration and how to solve it is described in \cref{sec:optim-presp-topol}.

\section{Topologies}
\label{sec:topologies}

The first step of the algorithm is to generate topologies. It is therefore
natural that we need some way of representing and generating these topologies.

The algorithm only considers \acp{fst}, where $k = n - 2$. This simplification
is allowed, as we can simply regard any Steiner tree with $k \le n - 2$ as a
\ac{fst} where some edges have length zero and thus some points have
``merged''.

An incentive for only looking at \acp{fst} can be found in \textcite{gilbert1968}
which presents a table, here presented in \cref{tab:number-of-topologies},
containing the number of possible Steiner topologies for different numbers of
terminals and Steiner points. As can clearly be seen the number of \acp{fst}
(the diagonal) is a lot smaller than the total number of topologies. Thus not
having to optimize the non-full topologies is clearly desirable.

\begin{table}[htbp]
  \centering
  \begin{tabular}{cccccc}
    \toprule
    $k \backslash n$ & 3    & 4    & 5     & 6      & 7      \\
    \hhline{~-----}
    0   & 3          & 12         & 60          & 360          & 2520         \\
    1   & \textbf{1} & 12         & 120         & 1200         & 12600        \\
    2   &            & \textbf{3} & 75          & 1350         & 22050        \\
    3   &            &            & \textbf{15} & 630          & 17640        \\
    4   &            &            &             & \textbf{105} & 6615         \\
    5   &            &            &             &              & \textbf{945} \\
    \bottomrule
  \end{tabular}
  \caption[Number of possible topologies]{The number of possible topologies for a Steiner tree with $n$
    terminals and $k$ Steiner points.\label{tab:number-of-topologies}}
\end{table}

Note however that even with this simplification the number of \acp{fst} is
still exponential in $N$, which is clear from \cref{cor:number-of-fsts}.

\subsection{Representation}
\label{sec:representation}

It turns out that every \ac{fst} can be represented using a vector, in
particular we utilize the following theorem put forth by \textcite{smith1992}

\begin{theorem}
  There is an 1--1 correspondence between \acp{fst} with $n \ge 3$ terminals,
  and $(n-3)$-vectors $\vec{a}$, whose $i$th entry $a_i$ is an integer between
  $1 \le a_i \le 2 i + 1$.
\end{theorem}

Here terminals have indicies $1, 2, \ldots, n$, and Steiner points have indicies
$n + 1, n + 2, \ldots, 2 n - 2$.

\begin{proof}
The proof of this theorem is done constructively by induction on $n$. It is
clear that the smallest \ac{fst}\footnote{This holds not only for \acp{fst}
  but for any Steiner topology} we can construct, must have $n = 3$ as the
number of Steiner points is $n - 2 = 1$. Thus we start with the initial null
vector $\vec{a} = ()$ corresponding to the unique \ac{fst} for the terminals
$\{p_1, p_2, p_3\}$ connected through the respective edges $\{e_1, e_2, e_3\}$ and one
Steiner point $p_{n+1}$ as seen in \cref{fig:algorithm-topology-1}. After this
first step, each entry of the topology vector is considered, one at a time,
where the $i$th entry of the topology vector describes the insertion of the
$(n+1+i)$th Steiner point on the edge $e_{a_{i}}$ and its connection to the
$(i+3)$th terminal. Thus for the $i$th insertion we will have
$2i+1$\footnote{At the first iteration we clearly have $3$ edges to insert on, and
  as each subsequent insertion generates two new edges we have, after $i$
  insertions, $3 + \underbrace{2 + \cdots + 2}_{2 i} = 2 i + 1$.} different
edges on which we can insert Steiner point $p_{n + 1 + i}$ and connect it to the
regular point $p_{i+3}$.
%
\begin{figure}[htbp] \centering
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-1}
    \caption{The initial null vector.\label{fig:algorithm-topology-1}}
  \end{subfigure}\hspace{1em}%
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-2}
    \caption{Connecting point $p_4$ on edge $e_2$.\label{fig:algorithm-topology-2}}
  \end{subfigure}\hspace{1em}%
  \begin{subfigure}[t]{0.267\textwidth}
    \includegraphics[width=\textwidth]{gfx/tikz/algorithm-topology-3}
    \caption{Connecting point $p_5$ on edge $e_4$.\label{fig:algorithm-topology-3}}
  \end{subfigure}
  \caption[Construction of FSTs]{Construction of the \acsp{fst} corresponding
to the vector $\vec{a} = (2, 4)$.\label{fig:algorithm-topologies}}
\end{figure}
\end{proof}

Furthermore we get a corollary saying that the number of \acp{fst} is
exponential in $n$

\begin{corollary}
\label{cor:number-of-fsts}
The number of \acp{fst} on $n$ terminals is
%
\[
  1 \cdot 3 \cdot 5 \cdots (2n - 7) \cdot (2n - 5) = \prod_{i=0}^{n-3} 2i+1
\]
%
I.e.\ the number of \acp{fst} is exponential in $n$.
\end{corollary}

Which is clear as we must insert $n-2$ Steiner points, where the null vector is
the $0$th iteration. Thus the last iteration must be $n-3$, and for each
iteration we have $2i+1$ different insertions.

Something which might not be completely obvious from the above proof is, that we
not only generate all topologies, but that we generate all topologies exactly
once. This is important as duplicate topologies could significantly
increase the run time of the algorithm. That we do not at any point construct
duplicate topologies can be shown as a proof by contradiction.

\begin{proof}
  That we have generated the same topology by two different procedures (topology
  vectors). Both topologies must have started from the same three terminals and
  single Steiner point (the smallest Steiner topology possible). We now begin
  splitting the topologies. At some point the two topologies must have split
  different edges, as if this was not the case, the two topologies would have
  the exact same procedure. Thus we consider the first step $i$ where two
  different edges are split. At this point we insert Steiner point
  $p_{n+i}$. Now find in the first topology a pair of terminals $p_1$ and $p_2$
  such that $p_{n+i}$ after the split lies on the path from $p_1$ to $p_2$, but
  does not lie on the path between the same two points in the second
  topology. This is always possible, and thus the two topologies are different,
  and since we cannot remove the Steiner point from the path in the first
  topology, or add it to the path in the second topology, these two topologies
  can never become the same again.
\end{proof}

\NOTE[inline]{If time add a figure of the paths.}

The way \citeauthor{smith1992} chooses to enumerate the edges is not explained
outright, but only in the form of a visual
example~\cite[p.~143]{smith1992}. Exactly how one enumerates the edges on a
split is actually not so important, more so is it important to keep it
consistent. The way \citeauthor{smith1992} does this, is as in
\cref{fig:algorithm-topologies}. That is when inserting Steiner point
$p_{n+1+i}$ on the edge $e_{a_i}$\footnote{Here $a_i$ is the $i$-th entry in the
  topology vector.} going from point $p_{a_i}$ to point $p_j$ with
$n < j < n+i+1$, split it such that we get the following three
edges\footnote{Remember that an edge is represented as $(i,j)$ meaning that it
  runs from point $p_i$ to point $p_j$.}: $e_{a_{i}} = (a_{i}, n+1+i)$,
$ e_{2i + 2} = (i+3,n+1+i)$ and $ e_{2i + 3} = (j,n+1+i)$. This is also the way
the new implementation enumerates the edges.

\subsection{Generation}
\label{sec:generation}

Using the representation just described the problem of generating all topologies
can now be done as a backtracking problem generating all $(n-3)$-topology
vectors. An example of how the generation works can be seen in
\cref{fig:topology-sprouting}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.8\textwidth]{gfx/tikz/topology-sprouting}
  \caption[Splitting of an edge]{Upon splitting an edge and connecting a new
    terminal to the topology the algorithm can perform the split on any of the
    existing edges. Thus in this case the algorithm branches into three
    different topologies. In general the splitting a topology results in the topology
    branching into as many new topologies as there are edges in the topology on which
    the split is perfomed.\label{fig:topology-sprouting}}
\end{figure}

To further speed up the generation of topologies, or rather to avoid generating
unnecessary topologies, \citeauthor{smith1992} also utilizes the following theorem
%
\begin{theorem}
  For any set of $n$ distinct terminals in any Euclidean space, the length of
  the shortest tree, interconnecting $n-1$ points, with topology vector
  $a_1 \cdots a_{n-4}$ is no greater than the length of the shortest tree,
  interconnecting $n$ points, with topology vector $a_1 \cdots a_{n-3}$.
\end{theorem}
%
\begin{proof}
  Consider deleting the edge incident to terminal $p_n$ in the \ac{rmt} on $n$
  terminals. Replace the Steiner point previously adjacent to $p_n$ and its
  edges with a straight edge, connecting the other two points adjacent to the
  Steiner point. This tree is not longer than before $p_n$ was removed, and
  furthermore it has the topology of the tree on the $n-1$ first terminals. This
  new tree currently has some length shorter than the previous, and per the
  definition of \ac{rmt}, the \ac{rmt} of the new tree must either be the length
  or the new tree or shorter. Thus we have proven that the \ac{rmt} of this
  topology cannot be greater than the previous topology.
\end{proof}

The algorithm utilizes this to prune in the following way. Imagine we have found
some upper bound for the \ac{smt} of all terminals. If we then optimize any tree
with a generated topology vector which does not yet include all the terminals,
and it turns out to have length greater than the upper bound, we can prune all
topologies that would be generated from this vector. This is possible as the
length of the larger trees with more terminals cannot become any smaller than
the length of the current tree, and thus cannot become smaller than the length
of the upper bound.

In general we need to generate topologies and optimize their respective trees
depth-first. The reason for this is two-fold: Firstly going breadth-first would
mean that we generate all \acp{fst} as the very last topologies to be
generated. Thus we would not get an upper bound before this stage, and would
therefore be unable to prune any significant branches, as all that is left to
optimize at this point are leaves\footnote{I.e.\ the \acp{fst}.}. We could of
course find some upper bound using a heuristic, e.g.\ a Minimum Spanning
Tree. This would possibly allow us to prune some branches, but would not change
that fact that we would not be able to benefit from possibly updating the upper
bound if we found a \ac{fst} with a smaller length. Thus going depth-first is
advantageous as it allows us to not only get an upper bound without the need of
a heuristic, but also allows us to update the upper bound and use this, should
we find a new better one.

Secondly is also the concern of memory usage of the actual implementation. If we
were to perform the algorithm breadth-first it would mean keeping track of
earlier topologies, which would drastically increase memory usage, in contrast
to depth-first we one only needs to remember the current way down.

The actual implementation of the backtracking is not described by the article
part of \textcite{smith1992}, but only by the source code in the appendix. The
details of the new implementation will be discussed in
\cref{cha:implementation}. The original implementation faces some problems,
one of which is the fact that the entire topology is rebuilt from scratch every
time we need to either add of remove a point. These will be discussed in
\cref{sec:implementation}.

In actuality the way in which we optimize the trees is a mix between the depth-
and breadth-first search, which could be considered a best-first approach. This
is described more thoroughly in \cref{sec:order-build-optim}.

\section{Optimization of a Prespecified Topology}
\label{sec:optim-presp-topol}

When a topology has been generated, we need some way of optimizing the positions of the
Steiner points. The approach taken by \citeauthor{smith1992} is in general to create an
iteration with $i-2$ equations\footnote{Where $i = 3, 4 \ldots n$ is the number of
terminals currently in the topology.}, one for each Steiner point, and incrementally
optimize all of them every iteration. This is done by solving the equations
using Gauss elimination. The section will be concerned with describing the
iteration, the proof of correctness and convergence, the speed of convergence
and the error function for deciding convergence.

\subsection{Simple Iteration}
\label{sec:simple-iteration}

To get a sense of how we might go about optimizing the tree in a simple
manner we first look at a iteration which only optimizes one Steiner point at a
time. This is a simpler approach to the one proposed by \textcite{smith1992},
but according to \citeauthor{smith1992} will probably also take longer time to
converge. The iteration is described by \cref{alg:simple-iteration}. The
algorithm is intentionally a bit vague, as even the simple iteration has several
choices we can make.

\begin{algorithm}[htbp]
  \KwData{
    \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,label=-,leftmargin=10pt]
    \item A \ac{fst} containing the Steiner points $S$, terminals $R$, and edges $E$
    \item A small positive number $\epsilon$
    \end{itemize}
  }
  \KwResult{The optimized \ac{fst}}
  \ForEach{$s \in S$}{
    $\textit{error}[s] \gets \infty$\;
  }
  $\textit{cont} \gets \texttt{true}$\;
  \While{$\textit{cont}$}{
    $\textit{cont} \gets \texttt{false}$\;
    \ForEach{$s \in S$}{
      \If{$\textit{error}[s] > \epsilon$}{
        optimize $s$ with respect to its neighbors in $T$\;
        recalculate \textit{error}[s]\;
        $\textit{cont} \gets \texttt{true}$\;
      }
    }
  }
  \Return{$T$}
  \caption[Simple iteration]{Pseudo code describing the optimization strategy
    using the simple iteration. The algorithm describes running a single
    optimization. In general one would have to run the algorithm several times,
    until the error of the entire tree falls below some threshold.\label{alg:simple-iteration}}
\end{algorithm}

First we should choose how a single Steiner point is optimized. As earlier
described this is the same as the Fermat-Torricelli problem, and thus we can go
about finding the Steiner point in all the same ways. Secondly one needs to
decide some way of calculating the error of the current Steiner point. This
could e.g.\ be done as in \textcite{smith1992}\footnote{Note that we would then
  not sum the error as is done by \citeauthor{smith1992}, but as we only need
  the error of the current Steiner point, and not the entire tree.}.

The problem \citeauthor{smith1992} poses for the simple iteration described here is that while it
quickly becomes optimal \textit{locally} it may still converge slowly as it does not
regard the \textit{global} structure. This statement is however not qualified
any further in the article.

As an iteration, using an analytical solution to the Fermat-Torricelli problem
described by \textcite{uteshev2014}, has actually been implemented the above
statement will be further discussed in \cref{cha:experiments} and \cref{cha:discussion}.

\subsection{\citeauthor{smith1992}'s Iteration}
\label{sec:smiths-iteration}

Now that we have looked at a simple way of optimizing the topology, by finding
the coordinates of one Steiner point at a time, we can progress to the iteration
proposed by \textcite{smith1992}.

To optimize the topology \citeauthor{smith1992} proposes the following

\begin{theorem}
  As the $i$th step in an iterative process, $i= 0, 1, \ldots$, solve the $n-2$
  equations\footnote{Note that in the second sum of \cref{eq:6}
    \textcite{smith1992} gives the index $j: jk \in T, j \in S$. For the rewrite
    to make sense, this should one have been $j: jk \in T$. Thus I believe this
    is just a typo and have progressed as such.}\footnote{Note that since the
    unknowns in the iteration are vectors we actually have $d$ independent
    equation systems of $n-2$ equations.}
%
\begin{alignat}{2}
  p_l^{(i+1)}
  &= \frac{\sum_{j : (j,l) \in \mathcal{T}}
    \frac{p_j^{(i+1)}}{|p_l^{(i)} - p_j^{(i)}|}}
  {\sum_{j: (j,l) \in \mathcal{T}} \frac{1}{|p_l^{(i)} - p_j^{(i)}|}},
  & \quad n + 1
  &\le l \le 2 n - 2 \label{eq:6}
  \intertext{which can be rewritten as follows, %
    to show that forces on $p_l$ is in equilibrium}
  \vec 0
  &= \sum_{j: (j,l) \in \mathcal{T}} \frac{p_l^{(i+1)} - p_j^{(i+1)}}
  {|p_l^{(i)} - p_j^{(i)}|},
  & \quad n + 1
  &\le l \le 2 n - 2 \label{eq:9}
\end{alignat}
%
for the unknowns $p_l^{(i+1)}$ where $p_l \in S$ represents the $\mathit{i}$th
Steiner point, being a $\mathit{d}$-vector. Then from all initial choices of
Steiner point coordinates $p_l^{(0)}$, except for a set of measure zero in
$\mathbb{R}^{(n-2)d}$, the iteration converges to the unique optimum Steiner
point coordinates\footnote{Note that if we have an edge of length zero, the
  iteration is not actually defined in the way we have written it, as we get a
  division by zero. \textcite[p.~147--148]{smith1992} argues as to why this is
  not a problem, and how to resolve such a situation. This argument will be
  discussed later in this section. Thus the proof of convergence, for now,
  assumes this situation does not happen.} that minimize the total tree length
$L$. This convergence happens in such a way that the sequence of tree lengths
$L(S^{(i)})$ is monotonically decreasing.
\end{theorem}

Here $L$ is defined as
%
\begin{equation}
  L(S) = \sum_{j,l : (j,l) \in \mathcal{T}} | p_l - p_j |
\end{equation}
%
Which is simply the Euclidean norm. \textcite{smith1992} also presents a proof
of the convergence of the iteration. A presentation and discussion of this can
be found in \cref{cha:proof-convergence}.

\subsubsection{Solving the Equations}
\label{sec:solving-equations}

The equation system of each iteration can be solved using Gaussian elimination
as described by \textcite[p.~148--149]{smith1992}. Roughly the Gaussian
elimination described and implemented by \citeauthor{smith1992}\footnote{and
  also the new implementation.} is done by identifying an equation which only
has at most one unknown on the right hand side. This corresponds to a Steiner
point being connected to two or more terminals, which is always guaranteed to
exist. Working from this point we can then eliminate a variable by substitution
in the other equations, which will result in at least one equation having no
unknowns on the right hand side. In the end we just do backward substitution.
Doing the elimination from this specific point, instead of just some random
point means that we can a perform all elimination in $\mathcal O(N)$. Since we
$d$ linear systems we can then solve them all in $\mathcal{O}(n d)$. The details
can be found in~\cite[p.~148--149]{smith1992}.

\subsubsection{Speed of Convergence}
\label{sec:speed-convergence}

According to \textcite[p.~150]{smith1992} the iteration has a good speed of
convergence. In most cases geometric convergence will occur. However some
degenerate configurations of terminals can cause sublinear convergence.

\section{Questionable Elements of \citeauthor{smith1992}'s Article}
\label{sec:quest-elem-smiths}

There are a number of elements in the article and the given
implementation~\cite{smith1992} which are somewhat questionable. These I will
try to discuss here.

\subsection{Choice of Error Function}
\label{sec:choice-error-funct}

To detect when convergence has happened \citeauthor{smith1992} proposes using a special
error function instead of the one often used where one stops when the
improvements from iteration to iteration falls below some $\epsilon$. The
function proposed by \citeauthor{smith1992} is
%
\begin{equation}
  E^2 = \sum_{
    \begin{array}{c} i \in S \\ (i,j) \in \mathcal{T} \\ (i,l) \in \mathcal{T} \\ j \ne l
    \end{array}} \text{pos} (2 (\vec x_j - \vec x_i) \cdot (\vec x_l - \vec x_i)
+ | \vec x_j - \vec x_i | \cdot | \vec x_l - \vec x_i |)
\end{equation}
%
Here $\text{pos}(x) = \max(x, 0)$. This error function sums together all angles which
are smaller than $120^{\circ}$. The error function is derived from calculating
the angle between edges in the following way:
%
\begin{align}
  \cos v & = \frac{\vec a \cdot \vec b}
    {|\vec a| \cdot |\vec b|}                 \\
  \intertext{If we then wish to find the cosine for $120^{\circ}$}
  \cos 120^{\circ}
         & = -\frac{1}{2} = \frac{\vec a \cdot \vec b}
    {|\vec a| \cdot |\vec b|} \Leftrightarrow \\
  0      & = \vec a \cdot \vec b + \frac{1}{2}
    (|\vec a| \cdot |\vec b|) \Leftrightarrow \\
  0      & = 2 (\vec a \cdot \vec b) + |\vec a| \cdot |\vec b|
\end{align}
%
Thus any angle less than $120^{\circ}$ gives a positive number when inserted in
the function, and any angle below will give a negative number (which is set to
zero by $\text{pos}(x)$).

Why this error function is supposed to be better is not explained very
thoroughly by \citeauthor{smith1992}. It is only referred to that doing some
experimentation which led him to believe that this would be a good function. The
idea behind the function seems to be that, as the final tree should only have
angles of $120^{\circ}$ any angle less than this must still need to be
optimized.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{gfx/tikz/errorfunction}
  \caption[Possible problem with the error function, part 1]{The left part of the figure
    shows how the use of the current error function might result in a suboptimal
    tree. The right part of the figure shows the optimal tree. Notice that
    this topology does not result in the \ac{smt} for these
    terminals.\label{fig:error-function}}
\end{figure}

There are however some problems with this choice of error function. Consider the
configuration as shown in the first tree of \cref{fig:error-function}. Here
there are two pairs of terminals, very far apart. These are connected by two
Steiner points as seen. Running \citeauthor{smith1992}'s iteration the two
Steiner points will move together, and up towards the center of the rectangle of
the four terminals. However suppose the two points reach each other before they
reach the center. In this case the angles on the left and right will be more
than $120^{\circ}$ and thus not add to the error. Furthermore the edge between
the Steiner points become zero (or practically zero). This means, that even
though the angles using this edge all are less than $120^{\circ}$ and add to the
error, the error they contribute with is zero---which is clear by looking at the
definition of the error.

This means that the algorithm will stop before it should with a tree which has
not been minimized.

Note however that the topology used in \cref{fig:error-function} is not the one
which gives the \ac{smt} for the four terminals. The optimal topology would have
the top pair connected with one Steiner point, the two bottom terminals
connected with another Steiner point, and those two Steiner points
connected. This topology when optimized will not face the same problem, as the
two Steiner points will not move towards each other as illustrated in
\cref{fig:error-function2}. Thus it is still unclear whether this problem can
occur with a topology which optimizes to a \ac{smt}.  Should this be the case
then we can for sure say that the algorithm is only a approximation algorithm,
and not an exact algorithm\footnote{Some of the other problems in the article
  also casts doubt on whether the algorithm is actually exact. E.g.\ the earlier
described uncertainty about the convergence of the iteration function.}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{gfx/tikz/errorfunction2}
  \caption[Possible problem with the error function, part 2]{The left part of the figure
    shows the connecting and optimizing of the optimal topology for the given
    terminals (the first Steiner point is just placed arbitrarily). Notice how
    the Steiner points do not pull together, but instead apart, leading to the
    optimal tree. Thus this topology does not suffer the problem with the error
    function described in \cref{fig:error-function}.\label{fig:error-function2}}
\end{figure}

\subsection{The Implementation}
\label{sec:implementation}

The implementation by \citeauthor{smith1992} contains several unexplained variables and conditionals not
directly related to the algorithm as it is explained in the article by \textcite{smith1992}.

\subsubsection{SCALE}
\label{sec:scale}

The variable \texttt{SCALE} is used by \citeauthor{smith1992} when calculating the initial
placement of a Steiner point. Steiner points are initially placed at the
centroid of its three neighbors. However the point is not placed exactly at the
centroid, but is instead pertubed slightly. To each coordinate of the centroid
is added $0.001 \cdot \texttt{SCALE} \cdot \texttt{RANDM()}$, where \texttt{RANDM()} is a
random number between $0$ and $1$. \texttt{SCALE} is a number defined as shown
in \cref{fig:randm}.

\begin{figure}[htbp]
\begin{c-code}
SCALE = 0.0;
for (i = 1; i <= NUMSITES; i++) {
  for (j = 0; j < DIMENSION; j++) {
    q = XX[i][j] - XX[1][j];
    if (q < 0.0) q = -q;
    if (q > SCALE) SCALE = q;
    printf(" %g", XX[i][j]);
  }
  printf("\n");
}
printf("SCALE = %g\n", SCALE);
\end{c-code}
  \captionof{listing}[Use of \texttt{SCALE} in \citeauthor{smith1992}'s implementation]{The implementation
    of \texttt{SCALE} in the original implementation. As can be seen it only
    compares the dimensions of the first terminal with the others, and not every
    terminal with every other terminal.\label{fig:randm}}
\end{figure}

As can be seen the variable finds the largest difference in coordinates between the first
terminal and all other terminals. Why this extra variable should be a part of
the perturbation is however unclear. The name of the variable might be a hint,
as it seems \citeauthor{smith1992} wished to scale the perturbation in correspondence to the
points. If this is the case however the code in \cref{fig:randm} seems to be
faulty as it should then not only compare the first point to all other, but
instead compare every point, to every other.

Thus it seems that the variable is not only not needed, but actually faulty.
This of course depends on what \citeauthor{smith1992} is trying to achieve with \texttt{SCALE}.
But overall the variable seems unneeded, and thus the new implementation does
not use \texttt{SCALE}.

\subsubsection{tol}
\label{sec:tol}

\texttt{tol} is the parameter given to the optimize function and is described as
a small positive number. In the code whenever the function is called the
variable has the value $0.0001*r/\text{NUMSITES}$ where $r$ is the current
error and and $\text{NUMSITES}$ is the number of terminals, $n$. Having some
small positive number in the optimize-function makes sense, as it is used to
avoid divisions with zero (by adding it to the edge lengths, some of which may
be zero). However the specific choice which is both dependent on the error and
the number of terminals seems selected somewhat at random. A perhaps more
obvious choice would be some small \textit{fixed} number, e.g.\ just
$0.0001$.

\subsubsection{Loop Condition When Optimizing}
\label{sec:loop-condition-when-1}

In the main function when performing the optimization of a topology, the loop
condition seems to suffer from the same arbitrariness as both \texttt{tol} and
\texttt{SCALE}. Depending on whether we are optimizing a topology without all
terminals, or a topology with all terminals and $n-2$ Steiner points the
condition is either $r > 0.005 \cdot q$ or $r > q \cdot 0.0001$\footnote{The
  difference in the condition stems from wishing higher precision when
  optimizing a tree of a topology with all terminals.}, $r$ being the error and
$q$ the tree length. The problem here is that there seems to be no direct relation
between the error and the length of the tree, and as such this loop condition
seems very arbitrary. The loop condition however seems to work in practice.

\subsubsection{If-Clause When Outputting Trees}
\label{sec:if-clause-when}

\begin{figure}[htbp]
\begin{c-code}
/* The faulty snippet */
if (q < STUB) {
  printf("\nnew record length %20.20g\n", q);
  for (i = 1; i <= k; i++) BESTVEC[i] = A[i];
  if (q < STUB*0.99999) output_tree();
  STUB = q;
}

/* First possible correction */
if (q < STUB) {
  printf("\nnew record length %20.20g\n", q);
  for (i = 1; i <= k; i++) BESTVEC[i] = A[i];
  output_tree();
  STUB = q;
}

/* Second possible correction */
if (q < STUB*0.99999) {
  printf("\nnew record length %20.20g\n", q);
  for (i = 1; i <= k; i++) BESTVEC[i] = A[i];
  output_tree();
  STUB = q;
}
\end{c-code}
  \captionof{listing}[Error in \citeauthor{smith1992}'s implementation when outputting trees]{The
    second conditional in the first code snippet may result in some \ac{smt}
    being set as upper bound, but outputted. This is fixed by both of the two
    next snippets.\label{fig:if-clause-snippet}}
\end{figure}

This is most probably a bug\footnote{As I consider this a bug, it has also been
  fixed in the version of the original source code I have used for
  testing.}. When a full topology with $n-2$ Steiner points is optimized its
length is compared to the current upper bound, \texttt{STUB}. If it is better
than the upper bound, this is updated to the newly found tree length. However
the tree is only outputted as a record if its length is smaller than the
previous upper bound times $0.99999$. This means we risk situations where the
upper bound is updated, but the tree never outputted. Most likely this extra
check should have been removed. As we are dealing with floating point arithmetic
doing the multiplication might make sense in the initial check, however the
double check is faulty and may lead to wrong behavior. The faulty snippet and
its possible corrections can be found in \cref{fig:if-clause-snippet}.

\subsubsection{Order of Building and Optimizing}
\label{sec:order-build-optim}

When going over the implementation done by \citeauthor{smith1992} a thing that
stroke me as odd initially was the way in which \citeauthor{smith1992} keeps
track of which topology vectors has been tried, and how he chooses which to try
next.

In the following description, $\ast$ is used to represent all possible values,
so e.g.\ $A = [1, \ast]$ would be the same as writing
$A = [1, 1], A = [1, 2], A = [1,3], A = [1, 4]$. The implementation does as
follows:
%
\begin{enumerate}
\item Start with the empty topology vector, meaning we have a topology with $3$
  terminals, $1$ Steiner point and $3$ edges.
\item Build and optimize all topology vectors of length $1$, i.e.\ $A = [\ast]$.
\item When finished with optimizing a topology store its length in a stack
  ordered such that the topology vector with the shortest tree length is on top.
\item Now select the top topology vector on the stack, and using this a prefix,
  build and optimize all topology vectors of length $2$. So say we have chosen $A = [1]$,
  then build $A = [1, \ast]$. Again put the
  topology vectors on a stack in order of their increasing tree length.
\item Continue building the topology vector in this way, until we have a
  topology vector which connects all terminals.
\item At this point go back a level, and choose the next topology vector on top
  of the stack, i.e.\ the second best. So say e.g.\ we have $5$ terminals in all
  and have just optimized $A = [1, \ast]$. Then if $A = [2]$ was the shortest tree
  after $A = [1]$ we would build and optimize $A = [2, \ast]$ next.
\item Continue building and optimizing the vectors in this way, putting them on,
  and popping them of the stack until the stack is empty.
\end{enumerate}
%
Another way to look at the approach is to see the topology as a tree
structure. What we then do, is that we optimize every child of the node at which
we are currently at, and select the one with the shortest tree. At this node we
then again optimize all its children and select the shortest tree. When going
backwards we then just select the second best, third best and so on.

This approach in which we actually optimize a breadth and then select which
vector to continue the depth-first search on can be described as a best-first
search. It may seem a bit elaborate in contrast to performing a simple
depth-first search in which we e.g.\ always kept as far to the left as possible
in the tree so we would optimize a topology of $6$ terminals as
%
\begin{align}
  A & = [1]     \\
  A & = [1,1]   \\
  A & = [1,1,1] \\
  A & = [1,1,2] \\
    & \vdots    \\
  A & = [1,2]   \\
  A & = [1,2,1] \\
    & \vdots    \\
  A & = [2]     \\
    & \vdots
\end{align}
%
Best-first search however turns out to be much more efficient than simple
depth-first. The new implementation did not use best-first initially, but
instead just depth-first. This caused a significant slow-down when compared with
the original implementation\footnote{No formal tests were made as the original
  implementation was several magnitudes faster, and thus it was obvious that
  something needed to be changed.}. Because of this, the new implementation now
uses the best-first approach.

The reason that the best-first search is more effective, lies in the fact that
we often will be able to prune more topologies than with the depth-first
search. When using the best-first we in general get better upper bounds much
faster, and thus we become able to prune more topologies and do so
faster. Imagine e.g.\ that we have $n = 6$. In simple depth-first we would
simply start to plow through the topologies descending from $A = [1]$. However
it may very well be that $A = [3]$ shows a lot more promise, and if we had
optimized this we could have pruned both $A = [1]$ and maybe also $A = [2]$.
This is exactly what best-first tries to do by always going the route which
currently shows the most promise.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
